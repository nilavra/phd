# Methods: LongSAL - the Longitudinal Study

To investigate the research questions and hypotheses discussed in Section \@ref(sec-rq), we conducted the LongSAL study.
The following sections discuss the study design, apparatus and procedure.

## Study Design and Participants {#sec-method-exp-design}

LongSAL (Longitudinal Search as Learning study) is a remote, exploratory, longitudinal study that was conducted between January and June 2022 (Spring semester) at the School of Information, University of Texas at Austin (UT Austin).

Participants were recruited from the student pool enrolled in the required undergraduate core-course: *Ethical Foundations for Informatics* [@fleischmann2022i303].
18 participants originally signed up for the study; 10 participants fully completed all the phases of the study, and the remaining 8 dropped off at different points during the semester.
Students enrolled in the course had to submit a research paper of 2,000-2,500 words as the final project for the course.
There were has four checkpoints spread across the semester to submit the drafts in progress: (i) paper proposal, (ii) outline, (iii) rough draft, and (iv) final paper.
Writing the research paper required choosing an informatics ethical dilemma, and applying three ethical perspectives covered in the course to explore potential solutions to the selected dilemma.
This involved searching and navigating information online, finding at least 20 relevant external sources, combining ideas, and weaving a narration around the information found in the selected sources.


The study design was informed by running a pilot study during Summer
2021 semester, in partnership with two courses at UT Austin School of
Information: *Information in Cyberspace*, and *Academic Success in the
Digital University*. More details of the pilot study are presented in
Appendix \@ref(ch-pilot-study). 
<!-- The final project overview from the *Information in Cyberspace* course is presented in Figure [1.1](#fig-final-project-description){reference-type="ref" reference="fig-final-project-description"}.  -->



## Apparatus

### YASBIL Browsing Logger

The YASBIL browsing logger [@bhattacharya2021yasbil] was utilised
for this study. YASBIL (Yet Another Search Behaviour and Interaction
Logger)^[https://github.com/LongSAL/yasbil] is a two-component logging solution for ethically recording a
user's browsing activity for Interactive IR user studies. It was
developed by the author in early Spring 2021, and was employed in the
pilot study for data collection and testing. YASBIL comprises a Firefox
browser extension and a WordPress plugin. The browser extension logs
browsing activity in the participants' machines. The WordPress plugin
collects the logged data into the researcher's data server. YASBIL
captures participant's behavioural data, such as webpage visits, time
spent on pages, identification of popular search engines and their
SERPs, 
<!-- identifying rank of result clicked on SERPs,  -->
tracking mouse
clicks and scrolls, and the order and sequences of these events. The
logging works on any webpage, without the need to own or have knowledge
about the HTML structure of the webpage. To protect the privacy of
participants, the logger software can be switched on or off by the participant.
Participants received regular reminders to turn YASBIL on only when they were searching for information related to the course.

<!-- and participants will be instructed (and encouraged) to switch on the logger only when they are performing search activities related to the experiment.  -->
YASBIL offers ethical data transparency and security
for participants, by enabling them to view and obtain copies of the
logged data, as well as securely upload the data to the researcher's
server over an HTTPS connection. Although developed using the
cross-browser WebExtension API ^[https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Build-a-cross-browser-extension], YASBIL
currently works in the Firefox Web Browser. So participants were
instructed to install Firefox and YASBIL on their machines when they
volunteered to participate in the study.




<!-- ### Qualtrics Survey Software -->

<!-- ### Zoom Video-conferencing Software -->




<!-- ## Search Task Template {#sec-method-search-task-template} -->



## Procedure {#sec-method-procedure}

<!----------- fig:study-proc ----------->

```{r study-proc, fig.scap='(ref:scap-study-proc)', fig.cap = '(ref:cap-study-proc)',  echo=FALSE, out.width='100%', fig.align='center'}
nbShowFig("figs/study-proc")
```
<!-- each text reference needs to be their own line / paragraph ! -->
(ref:cap-study-proc) Longitudinal study procedure with long caption containing **bold** and *italic* text, which is not possible from within `R code`.

(ref:scap-study-proc) Longitudinal study procedure.

<!----------- fig:study-proc (leave a blank line before this) ----------->

The longitudinal study consisted of six data collection components, as illustrated in Figure \@ref(fig:study-proc).
They comprise three asynchronous **questionnaires** (`QSNR1`, `QSNR2`, `QSNR3`),
two **remote synchronous study phases** over Zoom video conferencing software (`PHASE1`, `PHASE3`),
and a set of four asynchronous **longitudinal tracking phases** (`PHASE2a`, `PHASE2b`, `PHASE2c`, `PHASE2d`).
These phases are discussed in detail in the following sections.
<!-- Reference it like Figure \@ref(fig:study-proc) -->


### `QSNR0`: Recruitment Questionnaire {#sec-method-qsnr0}

Participants were recruited for the study via the recruitment questionnaire (QSNR0). The description of the study and the link to the questionnaire was posted in the Canvas Learning Management System used for the I303 course.


### `QSNR1`: Entry Questionnaire {#sec-method-qsnr1}

After recruitment, participants completed the entry questionnaire (QSNR1).
The purpose of QSNR1 was to capture their individual-differences, or
moderating variables, at the beginning of the semester.
Details of the data captured in SUR1 are
described below, with references to sections in the Appendix, where the
full-text of the questionnaire can be found.


#### Consent Form (Appendix \@ref(app-qsnr-consent-form))

The first page of QSNR1 was online consent form for participating in the study. Participants
were able to proceed with the study once they provided informed consent.



#### Demographics (Appendix \@ref(app-qsnr-demographics))

Questions about demographic information of the participant pool.

<!-- 1. **Search and IT proficiency:** (Appendix \@ref(app-qsnr-search-it-proficiency))
   Captures previous search experience, and proficiency in navigating the web.
   Some items are
    adapted from the *Digital Health Literacy Instrument (DHLI)* by
    [@van2017development], and the *Search Self-Efficacy scale (SSE)* by
    [@brennan2016factor]. -->

<!-- 2. **Course Load and other engagements:** (Appendix \@ref(app-qsnr-course-load))
   To determine how busy the participant
    will be in the semester, and how much time they plan to allocate for
    the course with which the study is integrated. This will help to
    establish the learner's context. -->

<!-- 3. **Note-taking Strategies:** (Appendix \@ref(app-qsnr-note-taking))
    Captures styles and
    strategies used by participants to take notes. Adapted from
    *Listening and Note Taking Survey* by
    [@note-taking-survey-penn-state], and *Note Taking Strategies
    Inventory* by [@note-taking-strategies-umass]. -->


#### Motivation (Appendix \@ref(app-qsnr-imi))

Adapted from the *Intrinsic Motivation Inventory (IMI)* by
[@ryan1982control], which is a multidimensional measurement device
intended to assess participants' subjective experience related to a
target activity (the assignments for the course they are taking).
The instrument assesses participants' interest/enjoyment, perceived
competence, effort/importance, pressure/tension, perceived choice,
and value/usefulness, while performing a given activity, thus
yielding six subscale scores. 
<!-- Three items in the value/usefulness
subscale were completed with contextual information when the
study site is finalized.  -->
The pressure/tension and the perceived
choice components were not included in the entry questionnaire QSNR1, and were
present in the mid-term (QSNR2) and exit (QSNR3) questionnaires.


#### Self-regulation (Appendix \@ref(app-qsnr-srq))

Adapted from the *Self-Regulation Questionnaire (SRQ)* by
[@brown1999self], which assess seven self-regulatory processes
through self-report: receiving relevant information, evaluating the
information and comparing it to norms, triggering change, searching
for options, formulating a plan, implementing the plan, and
assessing the plan's effectiveness (Section \@ref(sec-bg-learn-self-regulation)).

#### Metacognition (Appendix \@ref(app-qsnr-mai))

Adapted from the *Metacognivite Awareness Inventory (MAI)*,
originally proposed by [@schraw1994assessing] as a 52-item true /
false questionnaire, and later revised by [@terlecki2018call] to use
five-point Likert scales. The instrument measures two components of
cognition through self-report: knowledge about cognition, and
regulation of cognition (Section \@ref(sec-bg-learn-metacognition)).

After completing QSNR1 offline, participants were instructed to prepare
for the initial synchronous phase, `PHASE1`, by
installing Firefox web browser and the YASBIL extension on their machines.
<!-- *(ii)* get a quick introduction to concept maps (by watching a short video),
*(ii)* familiarizing themselves with the *Sero!* learning platform
(for creating and assessing concept maps).  -->
This was a one-time step.
If a
participant could not find the time for this step, they were informed
that an extra 5-10 minutes would be taken in the beginning of `PHASE1` to
complete this step.

The entry questionnaire and the software installation took about
10-15 minutes to complete. Participants were compensated with USD 5
for their time for completing this step. The questionnaire was published to the I-303 course students in
the first week of the Spring 2022 semester.
<!-- , and was closed after 18 participants participants have been recruited. -->



### `PHASE1`: Initial Phase {#sec-method-phase1}

The `PHASE1` of the data collection took place in the beginning of the semester.
The data-collection took place over a Zoom video call combined with YASBIL browsing logger installed in the participants' machines.
Participants were asked to share their screen for the whole duration of the phase.
Their screens and audio were recorded for the entire duration.
They had the freedom to turn off their video.
The total time for `PHASE1` was expected to not exceed 1.5 hours (90 minutes).
Participants were compensated with USD 25 for this phase.
The different components of `PHASE1` are described below.

#### Training Search Task

Participants performed a training search task to familiarize themselves with how to operate the YASBIL browser extension to log their browsing activity. The training task took around 2-5 minutes.


#### `PHASE1-FINANCE` and `PHASE1-UBUNTU`: Two Actual Search Tasks

Participants performed two search tasks: `PHASE1-FINANCE`, and `PHASE1-UBUNTU`. 
The `PHASE1-FINANCE` task was repeated at the end of the semester as `PHASE3-FINANCE` task.
The `PHASE1-UBUNTU` task was not repeated, and instead the `PHASE3-BIAS` task too its place.
This helps to answer the research question RQ2 (Section \@ref(sec-rq)).
The order of the two search tasks were randomized.


The repeated search task `FINANCE` was on the topic of financial literacy, a topic that we posit can be considered as universally important to college students, and part of lifelong learning. 
The prompts for the `PHASE1-FINANCE` and `PHASE3-FINANCE` tasks are presented in Figure \@ref(fig:search-task-repeated).
The non-repeated search task were on topics that were taught in the I303 course: 
Ubuntu ethics (for `PHASE1`)
and
Algorithmic Bias (for `PHASE3`).
The prompts for these tasks are present in Figure \@ref(fig:search-task-new).

<!-- Each search task is expected to take around 20 minutes. -->

To answer RQ3 (effect of externalization and articulation in learning), each participant performed one of the search tasks while thinking aloud (Concurrent-Think Aloud or *CTA condition*), and performed the other search task in silence (*silent condition*). 
The choice of the search task for each of the conditions was randomized and balanced.

 

Each search task began with a pre-task questionnaire (Appendix \@ref(app-phase1-pretask)), which asked participants to self-rate their pre-search knowledge-level and interest on the topic. 
Then participants turned on the YASBIL browsing logger and started searching. 
The deliverable for each search task was a written summary (artefact). 
After participants are satisfied with the quality of the deliverable, they turned off YASBIL browsing logger, and proceeded to the post-task questionnaire.

The post-task questionnaire (Appendix \@ref(app-phase1-posttask)) asked participants to self-rate their post-search topic knowledge, search experience, interest and motivation, and overall perceptions.
The pre-task and post-task questionnaires are adapted from [@collins2016assessing; @crescenzi2020adaptation]. 

After the two search tasks were completed, participants answered questions about whether they preferred the think-aloud condition or the silent condition, and why (Appendix \@ref(app-phase1-cta-v-silent)).


#### `PHASE1-SHEG`: Website Reliability Assessment

To assess participants' (mis)information evaluation capabilities, they performed a website reliability assessment created by the Stanford History Education Group (SHEG) [@sheg2021website-reliability].

> *This task presents students with the website of the American College of Pediatricians (ACPeds.org) and asks them whether it is a trustworthy source to learn about children’s health. Despite the site’s professional title and appearance, the American College of Pediatricians is not the nation’s major professional organization of pediatricians. That designation belongs to the similarly named American Academy of Pediatrics.
> 
> This exercise is an open web search in which students are free to stay on the American College of Pediatricians site or leave it to search for information about the group. Successful students will look beyond the surface features of the site and detect its agenda from its new releases or other focus issues. A faster route, however, is to leave the site almost immediately to search for reliable information about the true agenda of this organization.*
> 
> `r nbQuoteAuthor('--- @sheg2021website-reliability')`

The prompt for the `PHASE1-SHEG` task is present in Appendix \@ref(app-phase1-sheg)).


#### Memory Span Test

`PHASE1` concluded with the assessment of the participant's working memory capacity (WMC) using a memory span task [@francis2004coglab].
<!-- Memory span assessment is kept in the synchronous session because it is a timed task, and needs to conducted in a controlled (experimenter observed) condition. -->
The task has 25 trials. On each trial participants saw a list of items presented one at a time in random order and were asked to recall the items in the same order in which they were presented. If they got a list correct, the list length increased by 1 for that type of material. If they got a list incorrect, the list length decreased by 1.

The type of material participants were asked to recall were: digits, letters that sound dissimilar, letters that sound similar, short words, and long words. The outcome score was the list length of the last list that participants could correctly recall.



### PHASE2A - PHASE2D: Longitudinal Tracking Phase {#sec-method-phase2}

### QSNR2: Mid-Term Questionnaire {#sec-method-qsnr2}

### PHASE3: Final Phase {#sec-method-phase3}

#### `PHASE3-SHEG`: Webpage Comparison Assessment


To assess participants' (mis)information evaluation capabilities, they performed a Website Comparison assessment created by the Stanford History Education Group (SHEG) [@sheg2021webpage-comparison]. 
This task asked participants to compare two websites and select one that they would use to begin research on a topic. One of the pages is a Wikipedia article. The other has ".edu" in its URL, but the page reveals that the content is a student-written blog post created as part of a university course. Many students have been taught that Wikipedia pages are completely unreliable and should be avoided. Many have also been taught that sites with a .edu domain are trustworthy. This assessment gauges their ability to think in more nuanced ways about these kinds of sites. 
<!-- The website reliability assessment is expected to take 10 minutes. -->

### QSNR3: Exit Questionnaire {#sec-method-qsnr3}


<!-- 


### SES2: Longitudinal Tracking

The longitudinal tracking \[SES2\] will be conducted asynchronously over
the duration of the semester, to understand the change (or lack thereof)
of participants' search behaviour and knowledge gain over time. Whenever
participants will work on different parts of their final project
research paper (which will be termed as SES2a, SES2b, ...etc.), as
described in Figure
 [1.1](#fig-final-project-description){reference-type="ref"
reference="fig-final-project-description"}, they will use Firefox web
browser, and will log their browsing activity using the YASBIL browsing
logger. To protect their privacy, participants will be instructed to
turn YASBIL on only when they are searching for information related to
the course. In addition to the submitted working-draft of their research
paper, participants will submit a cumulative concept map with each
document submission. The cumulative concept map will help to track
participants' evolution of knowledge about the final project topic(s)
over the course of the semester. Participants will receive reminder
emails before the deadline of each assignment, to remind them to use
Firefox, turn YASBIL on, and incrementally update the concept map.
Participants will receive USD 5 per each assignment for which they log
data, up to a maximum of USD 20 for four assignments.

### SUR2: Mid-Term Survey {#sec-method-sur2}

The mid-term survey \[SUR2\] will take place around the mid point of the
semester (Week 8-9). The purpose is to track whether any of the
participants' individual difference measures (e.g. motivation,
metacognition, course load etc.) changed during the first half of the
semester. This survey will essentially be a replica of the Entry Survey
\[SUR1\] (Section [1.4.1](#sec-method-sur1){reference-type="ref"
reference="sec-method-sur1"}), with two modifications. First, the
consent form and the demographics sections will be absent. Second,
Intrinsic Motivation Inventory (IMI) will include the 'pressure/tension'
and the 'perceived choice' subscales, as these scales are more
meaningful after an activity has taken place [@ryan1982control]. The IMI
will also be reworded to reflect the mid-point of the semester (Appendix
[\[app-midterm-survey\]](#app-midterm-survey){reference-type="ref"
reference="app-midterm-survey"}). Participants will be compensated with
USD 5 for their time for completing this step.

### SES3: Final Session

The Final Session \[SES3\] will be similar in structure to the Initial
Session (SES1), and will take place at the end of the semester, after
all the course related tasks are completed by the student. The purpose
of the session is to record the 'evolved' search behaviour, and final
knowledge state. Participants will perform two search tasks, one website
reliability assessment task, and take the memory span test once again.
In the end, there will be a short semi-structured interview.

Of the two search tasks, the topic of one will be repeated from SES1
(financial literacy, Figure
[1.4](#fig-search-task-repeated){reference-type="ref"
reference="fig-search-task-repeated"}), while the topic of the other
will come from the course material. In both search tasks, participants
will be given the option of ***not searching*** if they feel confident
enough to answer the search task questions from their prior knowledge.
The deliverables for each search-task will be a written summary
(artefact) and a concept map (Figure
[1.2](#fig-search-task-template){reference-type="ref"
reference="fig-search-task-template"}).

Following the two search task, participants will perform another website
comparison/reliability task created by the
[@sheg2021webpage-comparison], which will assess their evolved
information evaluation skills. Then they will retake the memory span
test [@francis2004coglab].

A semi-structured interview will be conducted in the end, where
participants will reflect on their overall searching and learning
experience. Certain 'interesting' handpicked sessions from their
submitted logs may be identified and questions about them can also be
asked to participants. A list of the interview questions asked in the
Pilot Study (Appendix
[\[ch-pilot-study\]](#ch-pilot-study){reference-type="ref"
reference="ch-pilot-study"}) are presented in Appendix
[\[app-post-task-interview\]](#app-post-task-interview){reference-type="ref"
reference="app-post-task-interview"}, which can be reused.

Similar to SES1, participants will be asked to share their screen for
the whole duration of the session, except for the interview, whence they
can stop screen-sharing. Their screens and audio will be recorded for
the same. They may choose to turn off their video. The total time for
SES3 is expected to not exceed 1.5 hours (90 minutes). Participants will
be compensated with USD 25 for this session, and will be asked to
complete the Exit Survey \[SUR3\] as soon as convenient.

### SUR3: Exit Survey

The exit survey \[SUR3\] will take place after the Final Session
\[SES3\]. The purpose is to record the final state of the participants'
individual difference measures (e.g. motivation, metacognition, course
load etc.), and whether they changed during the second half of the
semester. This survey will essentially be a replica of the mid-term
survey \[SUR2\] (Section [1.4.4](#sec-method-sur2){reference-type="ref"
reference="sec-method-sur2"}), with the Intrinsic Motivation Inventory
(IMI) reworded to reflect the end-point of the semester (Appendix
[\[app-final-survey\]](#app-final-survey){reference-type="ref"
reference="app-final-survey"}). Participants will be compensated with
USD 5 for their time for completing this step. If participants do not
take the survey within three days (say) after appearing for SES3, they
will be sent reminder emails.

Participants will be compensated with a bonus payment of USD 15, if they
complete all the parts of the study without missing any component.

## Measurement and Variables

### Independent / Explanatory: Search Interaction and Process Measures

The independent variables will be the search process behavioural
measures. Information searching behaviour will be operationalized using
a battery of search process measures, based on the three-stages of user
interaction discussed in Section
[\[sec-bg-search-3\-stage\]](#sec-bg-search-3-stage){reference-type="ref"
reference="sec-bg-search-3-stage"}. These measures include query
reformulation types and measures
(Table [\[tab-res-Q\-QRT-txnmy\]](#tab-res-Q-QRT-txnmy){reference-type="ref"
reference="tab-res-Q-QRT-txnmy"}), SERP examination measures, content
page examination measures, and overall search session measures (Table
[\[tab-search-behaviours\]](#tab-search-behaviours){reference-type="ref"
reference="tab-search-behaviours"}). A non-exhaustive list of such
search process measures which have been used in prior literature is
presented in Appendices
[\[sec-app-vars-qry\]](#sec-app-vars-qry){reference-type="ref"
reference="sec-app-vars-qry"} through
[\[sec-app-vars-overall-search\]](#sec-app-vars-overall-search){reference-type="ref"
reference="sec-app-vars-overall-search"}.

### Dependant / Outcome: Learning Measures

Learning (knowledge gain) will constitute the dependant variables, or
factor variables, when dichotomized via median split. Learning outcomes
are planned to be assessed by: *(i)* analysis of concept maps
[@halttunen2005assessing] (Appendix
[\[sec-app-vars-concept-maps\]](#sec-app-vars-concept-maps){reference-type="ref"
reference="sec-app-vars-concept-maps"}); *(ii)* analysis of written
summaries / knowledge artefacts [@wilson2013comparison]; and *(iii)*
instructor awarded scores and grades received by students in the course,
which will be obtained via FERPA release. Time, resources and
feasibility permitting, other possible ways of assessing learning can be
by using *(iv)* Online Research and Comprehension Assessment (ORCA)
[@leu2015new Table 3], [@kanniainen2021assessing Appendix A]; and *(v)*
information-use from websites in written artefacts
[@vakkari2019modeling; @vakkari2020usefulness].

### Moderator: Individual Differences

The variables of individual differences that are hypothesized to
moderate learning are *(i)* motivation, scored using Intrinsic
Motivation Inventory (IMI) [@ryan1982control]; *(ii)* self-regulation,
scored using Self-Regulation Questionnaire (SRQ) [@brown1999self];
*(iii)* metacognition, scored using revised Metacognivite Awareness
Inventory (MAI) [@schraw1994assessing; @terlecki2018call] *(iv)* memory
span, scored using memory span test [@francis2004coglab]; *(v)* search
proficiency, scored using Digital Health Literacy Instrument (DHLI)
[@van2017development] and Search Self-Efficacy Scale (SSE)
[@brennan2016factor]; *(vi)* information evaluation capabilities
(mastery / emerging / beginner), scored according to rubrics provided by
[@sheg2021webpage-comparison] assessments (an example grading rubric is
present in Appendix
[\[sec-app-pilot-ses3\]](#sec-app-pilot-ses3){reference-type="ref"
reference="sec-app-pilot-ses3"} Task 3).

## Data Analysis Plans

Exploratory data analysis (such as time series plotting) and descriptive
statistics will be used to identify if changes in search process /
interaction measures can be visually observed over the course of time.
Inferential statistics (difference between groups) will be employed to
test if there are significant differences in the learning measures
between student groups who learn more versus learn less. Pattern Mining
and clustering approaches may also be used to identify clusters or
patterns in the search process (time-series) data, and see if these
clusters correlate with high and low learning. An example of measuring
such changes in variables can be found in [@133 Section 3.2]. Advanced
search interactions such as parallel browsing behaviour (multi-tabbed
and multi-windowed browsing) may also be analysed
[@huang2010parallel; @labaj2012modeling].

## Anticipated Limitations

There are foreseeable limitations to this proposed longitudinal study.
First, there may not be enough participants who sign up for the study.
The remedy for this is choosing a course with a large number of
students, and using appealing messaging in the recruitment material
(e.g., an attractive video message was used to recruit participants in
the Pilot Study[^2]). Second, participants may drop off due to various
reasons during the study. This can be tackled by regularly communicating
with the participants, keeping them engaged with affectionate, caring
and encouraging messaging, and letting them know that their
participation is valued highly by the researchers. Third, participants
may not show any changes in their search behaviours, of the changes may
be random. As one anonymous reviewer put it "the smart kids will show up
smart and with good search skills and they will leave smart with good
search skills and they will not change their inherent behavior over the
time. Similarly, the dumb kids will show up dumb, do whatever keeps them
dumb and end up dumb." Alternatively, there may be are not changes over
time, but rather strategies that are consistently followed by students
who will learn more, and students who will learn less. There is no easy
fix to this, and if this happens, it will be treated as a finding from
the study. A possible rescue from this situation would then be to use
the (semi) qualitative data -- the semi structured interview at the end,
the concurrent think alouds, and others -- to derive interesting
findings.

## Proposed Dissertation Timeline

![image](figs/timeline-data-collection.pdf){width="0.9\\linewidth"}

1.  **November 2021:** prepare and submit IRB proposal

2.  **December 2021:** defend proposal, and if changes to the study
    protocol are suggested by the committee, submit them as an IRB
    amendment

3.  **January - May 2022:** conduct longitudinal study; a week-by-week
    schedule is in Table
    [\[tab-timeline-data-collection\]](#tab-timeline-data-collection){reference-type="ref"
    reference="tab-timeline-data-collection"}

4.  **Summer and Fall 2022:** two backup semesters for data collection,
    if something goes wrong in Spring 2022, especially due to COVID-19
    pandemic situations.

5.  **August 2022 - February 2023:** analyze data and write dissertation

6.  **April 2023:** Dissertation defense

7.  **May 2023:** revise and complete dissertation

[^1]: <https://serolearn.com>

[^2]: <https://youtu.be/RkBUZ4At8Qg> -->
