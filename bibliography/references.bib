

@article{garcia2023motivational,
  title={Motivational, Identity-Based, and Self-Regulatory Factors Associated with Academic Achievement of US Collegiate Student-Athletes: A Meta-Analytic Investigation},
  author={Garc{\'\i}a, Agust{\'\i}n J and Fong, Carlton J and Regalado, Yvette M},
  journal={Educational Psychology Review},
  volume={35},
  number={1},
  pages={14},
  year={2023},
  publisher={Springer}
}

@misc{hartel2022exploratory,
 author = {Hartel, Jenna},
 year = {2022},
 title = {Exploratory Research | Hear! Here! (Ideas for Doctoral Students) Series (#1)},
 url = {https://www.youtube.com/watch?v=URz4CAaJqnE},
 urldate = {2023-03-07}
}

@book{stebbins2001exploratory,
  title={Exploratory research in the social sciences},
  author={Stebbins, Robert A},
  volume={48},
  year={2001},
  publisher={Sage}
}

@article{vargha2000critique,
  title={A critique and improvement of the CL common language effect size statistics of McGraw and Wong},
  author={Vargha, Andr{\'a}s and Delaney, Harold D},
  journal={Journal of Educational and Behavioral Statistics},
  volume={25},
  number={2},
  pages={101--132},
  year={2000},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{mcgraw1992common,
  title={A common language effect size statistic.},
  author={McGraw, Kenneth O and Wong, Seok P},
  journal={Psychological bulletin},
  volume={111},
  number={2},
  pages={361},
  year={1992},
  publisher={American Psychological Association}
}

@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year={1947},
  publisher={JSTOR}
}

@inproceedings{karimi2011domain,
  title={Domain expert topic familiarity and search behavior},
  author={Karimi, Sarvnaz and Scholer, Falk and Clark, Adam and Kharazmi, Sadegh},
  booktitle={Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval},
  pages={1135--1136},
  year={2011}
}

@book{li2008relationships,
  title={Relationships among work tasks, search tasks, and interactive information searching behavior},
  author={Li, Yuelin},
  year={2008},
  publisher={Rutgers The State University of New Jersey, School of Graduate Studies}
}

@inproceedings{qu2010effect,
  title={The effect of task type and topic familiarity on information search behaviors},
  author={Qu, Peng and Liu, Chang and Lai, Maosheng},
  booktitle={Proceedings of the third symposium on Information interaction in context},
  pages={371--376},
  year={2010}
}

@article{rha2016exploring,
  title={Exploring the relationships between search intentions and query reformulations},
  author={Rha, Eun Youp and Mitsui, Matthew and Belkin, Nicholas J and Shah, Chirag},
  journal={Proceedings of the Association for Information Science and Technology},
  volume={53},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Wiley Online Library}
}

@article{zhou2019metacognitive,
  title={Metacognitive scaffolding for online information search in K-12 and higher education settings: a systematic review},
  author={Zhou, Mingming and Lam, Kelly Ka Lai},
  journal={Educational technology research and development},
  volume={67},
  number={6},
  pages={1353--1384},
  year={2019},
  publisher={Springer}
}

@article{reisouglu2020analysis,
  title={An analysis of the online information searching strategies and metacognitive skills exhibited by university students during argumentation activities},
  author={Reiso{\u{g}}lu, {\.I}lknur and Toksoy, Seyhan Ery{\i}lmaz and Erenler, S{\"u}meyye},
  journal={Library \& Information Science Research},
  volume={42},
  number={3},
  pages={101019},
  year={2020},
  publisher={Elsevier}
}

@article{winne2022metacognition,
  title={Metacognition and self-regulated learning},
  author={Winne, P and Azevedo, Roger},
  journal={The Cambridge handbook of the learning sciences},
  pages={93--113},
  year={2022},
  publisher={Cambridge University Press}
}

@article{williamson2015selfregulated,
  title = {Self-Regulated Learning: An Overview of Metacognition, Motivation and Behaviour},
  shorttitle = {Self-Regulated Learning},
  author = {Williamson, Genevieve},
  year = {2015},
  journal = {Journal of Initial Teacher Inquiry},
  volume = {I},
  publisher = {{University of Canterbury, College of Education, Health and Human Development}},
  doi = {10.26021/851},
  collaborator = {University Of Canterbury and University Of Canterbury},
  copyright = {Creative Commons Attribution 4.0 International License., Creative Commons Attribution 4.0 International},
  keywords = {behaviour,metacognition,motivation,self-regulated learning}
}


@article{urgo2022learning,
  title={Learning assessments in search-as-learning: A survey of prior work and opportunities for future research},
  author={Urgo, Kelsey and Arguello, Jaime},
  journal={Information Processing \& Management},
  volume={59},
  number={2},
  pages={102821},
  year={2022},
  publisher={Elsevier}
}

@article{krejtz2015gaze,
  title={Gaze transition entropy},
  author={Krejtz, Krzysztof and Duchowski, Andrew and Szmidt, Tomasz and Krejtz, Izabela and Gonz{\'a}lez Perilli, Fernando and Pires, Ana and Vilaro, Anna and Villalobos, Natalia},
  journal={ACM Transactions on Applied Perception (TAP)},
  volume={13},
  number={1},
  pages={1--20},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@article{vallat2018pingouin,
  title={Pingouin: statistics in Python.},
  author={Vallat, Raphael},
  journal={J. Open Source Softw.},
  volume={3},
  number={31},
  pages={1026},
  year={2018}
}

@article{koriat1997monitoring,
  title={Monitoring one's own knowledge during study: A cue-utilization approach to judgments of learning.},
  author={Koriat, Asher},
  journal={Journal of experimental psychology: General},
  volume={126},
  number={4},
  pages={349},
  year={1997},
  publisher={American Psychological Association}
}

@article{lanza2016latent,
  title={Latent class analysis for developmental research},
  author={Lanza, Stephanie T and Cooper, Brittany R},
  journal={Child Development Perspectives},
  volume={10},
  number={1},
  pages={59--64},
  year={2016},
  publisher={Wiley Online Library}
}

@book{collins2009latent,
  title={Latent class and latent transition analysis: With applications in the social, behavioral, and health sciences},
  author={Collins, Linda M and Lanza, Stephanie T},
  volume={718},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{pastor2007latent,
  title={A latent profile analysis of college students’ achievement goal orientation},
  author={Pastor, Dena A and Barron, Kenneth E and Miller, BJ and Davis, Susan L},
  journal={Contemporary educational psychology},
  volume={32},
  number={1},
  pages={8--47},
  year={2007},
  publisher={Elsevier}
}

@article{zyberaj2022latent,
  title={Latent transition analysis in organizational psychology: A simplified “how to” guide by using an applied example},
  author={Zyberaj, Jetmir and Baka{\c{c}}, Cafer and Seibel, Sebastian},
  journal={Frontiers in Psychology},
  pages={6713},
  year={2022},
  publisher={Frontiers}
}

@article{ryoo2018longitudinal,
  title={Longitudinal model building using latent transition analysis: an example using school bullying data},
  author={Ryoo, Ji Hoon and Wang, Cixin and Swearer, Susan M and Hull, Michael and Shi, Dingjing},
  journal={Frontiers in psychology},
  volume={9},
  pages={675},
  year={2018},
  publisher={Frontiers Media SA}
}

@article{johnson2021latent,
  title={Latent profile transition analyses and growth mixture models: A very non-technical guide for researchers in child and adolescent development},
  author={Johnson, Sara K},
  journal={New Directions for Child and Adolescent Development},
  volume={2021},
  number={175},
  pages={111--139},
  year={2021},
  publisher={Wiley Online Library}
}

@article{fleischmann2022i303,
    author = {Fleischmann, Ken and Verma, Nitin and Gursoy, Ayse and Bautista, John Robert and Day, Jaxsen},
    title = {I 303 : Ethical Foundations for Informatics [Syllabus]},
    howpublished = {\url{https://www.ischool.utexas.edu/ischool-course-offerings?courseID=439}},
    month = {},
    journal={School of Information, University of Texas at Austin},
    year = {2022},
    note = {(Accessed on 2023-01-19)}
}


@article{taramigkou2018leveraging,
  title={Leveraging exploratory search with personality traits and interactional context},
  author={Taramigkou, Maria and Apostolou, Dimitris and Mentzas, Gregoris},
  journal={Information Processing \& Management},
  volume={54},
  number={4},
  pages={609--629},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{hassan2014struggling,
  title={Struggling or exploring? Disambiguating long search sessions},
  author={Hassan, Ahmed and White, Ryen W and Dumais, Susan T and Wang, Yi-Min},
  booktitle={Proceedings of the 7th ACM international conference on Web search and data mining},
  pages={53--62},
  year={2014}
}

@inproceedings{ibanez2022comparison,
  title={A comparison of dataset search behaviour of internal versus search engine referred sessions},
  author={Ib{\'a}{\~n}ez, Luis-Daniel and Simperl, Elena},
  booktitle={Acm sigir conference on human information interaction and retrieval},
  pages={158--168},
  year={2022}
}

@inproceedings{lam2007session,
  title={Session viewer: Visual exploratory analysis of web session logs},
  author={Lam, Heidi and Russell, Daniel and Tang, Diane and Munzner, Tamara},
  booktitle={2007 IEEE Symposium on Visual Analytics Science and Technology},
  pages={147--154},
  year={2007},
  organization={IEEE}
}

@phdthesis{hendahewa2016implicit,
  title = {Implicit Search Feature Based Approach to Assist Users in Exploratory Search Tasks},
  author = {Hendahewa, Chathra Hasini},
  year = {2016},
  school = {Rutgers, The State University of New Jersey}
}


@inproceedings{krejtz2014entropy,
  title={Entropy-based statistical analysis of eye movement transitions},
  author={Krejtz, Krzysztof and Szmidt, Tomasz and Duchowski, Andrew T and Krejtz, Izabela},
  booktitle={Proceedings of the Symposium on Eye Tracking Research and Applications},
  pages={159--166},
  year={2014}
}


@article{he2016beyond,
  title={Beyond actions: Exploring the discovery of tactics from user logs},
  author={He, Jiyin and Qvarfordt, Pernilla and Halvey, Martin and Golovchinsky, Gene},
  journal={Information Processing \& Management},
  volume={52},
  number={6},
  pages={1200--1226},
  year={2016},
  publisher={Elsevier}
}


@misc{note-taking-survey-penn-state,
  author={{Penn State Learning}},
  title = {Listening and Note Taking Survey | Penn State Learning},
  howpublished = "\url{https://pennstatelearning.psu.edu/listening-and-note-taking-survey}",
  note = "[Online; accessed 2021-10-09]",
  year = {2021},
  publisher={Pennsylvania State University},
}

@inproceedings{bhattacharya2021longitudinal,
  title={A Longitudinal Study to Understand Learning During Search},
  author={Bhattacharya, Nilavra},
  booktitle={Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
  pages={363--366},
  year={2021}
}

@article{newlondon1996pedagogy,
  title={A pedagogy of multiliteracies: Designing social futures},
  author={{New London Group}},
  journal={Harvard educational review},
  volume={66},
  number={1},
  pages={60--92},
  year={1996},
  publisher={Harvard}
}

@article{hansen2016editorial,
  title = {Editorial: Recent Advances on Searching as Learning: An Introduction to the Special Issue},
  shorttitle = {Editorial {{Recent}} Advances on Searching as Learning},
  author = {Hansen, Preben and Rieh, Soo Young},
  year = {2016},
  month = feb,
  journal = {Journal of Information Science},
  volume = {42},
  number = {1},
  pages = {3--6},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/0165551515614473},
  langid = {english},
  keywords = {nb-star},
  annotation = {00000}
}




@article{eickhoff2017introduction,
  title={Introduction to the special issue on search as learning},
  author={Eickhoff, Carsten and Gwizdka, Jacek and Hauff, Claudia and He, Jiyin},
  journal={Information Retrieval Journal},
  volume={20},
  number={5},
  pages={399--402},
  year={2017},
  publisher={Springer}
}

@inproceedings{gwizdka2016search,
  title={Search as learning (SAL) workshop 2016},
  author={Gwizdka, Jacek and Hansen, Preben and Hauff, Claudia and He, Jiyin and Kando, Noriko},
  booktitle={Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval},
  pages={1249--1250},
  year={2016}
}

@inproceedings{freund2014searching,
  title={Searching as learning (SAL) workshop 2014},
  author={Freund, Luanne and He, Jiyin and Gwizdka, Jacek and Kando, Noriko and Hansen, Preben and Rieh, Soo Young},
  booktitle={Proceedings of the 5th information interaction in context symposium},
  pages={7--7},
  year={2014}
}

@inproceedings{agosti2014evaluation,
  title={Evaluation methodologies in information retrieval dagstuhl seminar 13441},
  author={Agosti, Maristella and Fuhr, Norbert and Toms, Elaine and Vakkari, Pertti},
  booktitle={ACM SIGIR Forum},
  volume={48},
  number={1},
  pages={36--41},
  year={2014},
  organization={ACM New York, NY, USA}
}

@article{freund2013searching,
  title={From searching to learning},
  author={Freund, Luanne and Gwizdka, Jacek and Hansen, Preben and Kando, Noriko and Rieh, Soo Young},
  journal={Evaluation methodologies in information retrieval. Dagstuhl reports},
  volume={13441},
  pages={102--105},
  year={2013}
}

@inproceedings{collins2017search,
  title={Search as learning (dagstuhl seminar 17092)},
  author={Collins-Thompson, Kevyn and Hansen, Preben and Hauff, Claudia},
  booktitle={Dagstuhl reports},
  volume={7},
  number={2},
  year={2017},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{allan2012frontiers,
  title={Frontiers, challenges, and opportunities for information retrieval: Report from SWIRL 2012 the second strategic workshop on information retrieval in Lorne},
  author={Allan, James and Croft, Bruce and Moffat, Alistair and Sanderson, Mark},
  booktitle={ACM SIGIR Forum},
  volume={46},
  number={1},
  pages={2--32},
  year={2012},
  organization={ACM New York, NY, USA}
}

@book{karapanos2021advances,
  title = {Advances in {{Longitudinal HCI Research}}},
  editor = {Karapanos, Evangelos and Gerken, Jens and Kjeldskov, Jesper and Skov, Mikael B.},
  year = {2021},
  series = {Human\textendash{{Computer Interaction Series}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-67322-2},
  isbn = {978-3-030-67321-5 978-3-030-67322-2},
  langid = {english}
}




@inproceedings{bateman2012search,
  ids = {bateman2012search},
  title = {The Search Dashboard: How Reflection and Comparison Impact Search Behavior},
  shorttitle = {The Search Dashboard},
  booktitle = {Proceedings of the 2012 {{ACM}} Annual Conference on {{Human Factors}} in {{Computing Systems}} - {{CHI}} '12},
  author = {Bateman, Scott and Teevan, Jaime and White, Ryen},
  year = {2012},
  pages = {1785},
  publisher = {{ACM Press}},
  address = {{Austin, Texas, USA}},
  doi = {10.1145/2207676.2208311},
  isbn = {978-1-4503-1015-4},
  langid = {english}
}




@inproceedings{huang2011no,
  title={No clicks, no problem: using cursor movements to understand and improve search},
  author={Huang, Jeff and White, Ryen and Dumais, Susan},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={1225--1234},
  year={2011}
}

@inproceedings{downey2007models,
  title={Models of Searching and Browsing: Languages, Studies, and Application.},
  author={Downey, Doug and Dumais, Susan T and Horvitz, Eric},
  booktitle={IJCAI},
  volume={7},
  pages={2740--2747},
  year={2007}
}

@misc{koeman2020hciux,
author = {Lisa Koeman},
title = {HCI/UX research: what methods do we use? – Lisa Koeman – blog},
howpublished = {\url{https://lisakoeman.nl/blog/hci-ux-research-what-methods-do-we-use/}},
month = {},
year = {2020},
note = {(Accessed on 11/08/2020)}
}

@book{kuhlthau2004seeking,
  title={Seeking meaning: A process approach to library and information services},
  author={Kuhlthau, Carol Collier},
  volume={2},
  year={2004},
  publisher={Libraries Unlimited Westport, CT}
}

@article{kelly2006measuringb,
  title = {Measuring Online Information Seeking Context, {{Part}} 2: Findings and Discussion},
  shorttitle = {Measuring Online Information Seeking Context, {{Part}} 2},
  author = {Kelly, Diane},
  year = {2006},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {57},
  number = {14},
  pages = {1862--1874},
  issn = {1532-2890},
  doi = {10.1002/asi.20484},
  copyright = {Copyright \textcopyright{} 2006 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  keywords = {-tablet,longitudinal,nb-star},
  annotation = {\-eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20484}
}

@article{kelly2006measuringa,
  title = {Measuring Online Information Seeking Context, {{Part}} 1: Background and Method},
  shorttitle = {Measuring Online Information Seeking Context, {{Part}} 1},
  author = {Kelly, Diane},
  year = {2006},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {57},
  number = {13},
  pages = {1729--1739},
  issn = {1532-2890},
  doi = {10.1002/asi.20483},
  copyright = {Copyright \textcopyright{} 2006 Wiley Periodicals, Inc., A Wiley Company},
  langid = {english},
  keywords = {-tablet,longitudinal,nb-star},
  annotation = {\-eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20483}
}




@article{pennanen2003students,
  title = {Students' Conceptual Structure, Search Process, and Outcome While Preparing a Research Proposal: A Longitudinal Case Study},
  author = {Pennanen, Mikko and Vakkari, Pertti},
  year = {2003},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {54},
  number = {8},
  pages = {759--770},
  publisher = {{Wiley Online Library}},
  keywords = {longitudinal,vakkari}
}

@incollection{vakkari2000cognition,
  title = {Cognition and Changes of Search Terms and Tactics during Task Performance: A Longitudinal Case Study},
  booktitle = {Content-Based Multimedia Information Access-Volume 1},
  author = {Vakkari, Pertti},
  year = {2000},
  pages = {894--907},
  keywords = {longitudinal,vakkari}
}

@article{vakkari2001changes,
  title = {Changes in Search Tactics and Relevance Judgements When Preparing a Research Proposal a Summary of the Findings of a Longitudinal Study},
  author = {Vakkari, Pertti},
  year = {2001},
  journal = {Information retrieval},
  volume = {4},
  number = {3},
  pages = {295--310},
  publisher = {{Springer}},
  keywords = {longitudinal,vakkari}
}

@article{vakkari2001theory,
  title = {A Theory of the Task-based Information Retrieval Process: A Summary and Generalisation of a Longitudinal Study},
  shorttitle = {A Theory of the Task-based Information Retrieval Process},
  author = {Vakkari, Pertti},
  year = {2001},
  month = feb,
  journal = {Journal of Documentation},
  volume = {57},
  number = {1},
  pages = {44--60},
  issn = {0022-0418},
  doi = {10.1108/EUM0000000007075},
  langid = {english},
  keywords = {longitudinal,vakkari}
}





@article{weber2018can,
  title = {Can Digital Information Literacy among Undergraduates Be Improved? Evidence from an Experimental Study},
  shorttitle = {Can Digital Information Literacy among Undergraduates Be Improved?},
  author = {Weber, Hannes and Hillmert, Steffen and Rott, Karin Julia},
  year = {2018},
  month = nov,
  journal = {Teaching in Higher Education},
  volume = {23},
  number = {8},
  pages = {909--926},
  issn = {1356-2517, 1470-1294},
  doi = {10.1080/13562517.2018.1449740},
  langid = {english}
}

@article{weber2019informationseeking,
  ids = {weber2019informationseekingc},
  title = {Information-Seeking Behaviour and Academic Success in Higher Education: Which Search Strategies Matter for Grade Differences among University Students and How Does This Relevance Differ by Field of Study?},
  shorttitle = {Information-Seeking Behaviour and Academic Success in Higher Education},
  author = {Weber, Hannes and Becker, Dominik and Hillmert, Steffen},
  year = {2019},
  month = apr,
  journal = {Higher Education},
  volume = {77},
  number = {4},
  pages = {657--678},
  issn = {0018-1560, 1573-174X},
  doi = {10.1007/s10734-018-0296-4},
  langid = {english},
  keywords = {star}
}





@inproceedings{azzopardi2014modelling,
  title={Modelling interaction with economic models of search},
  author={Azzopardi, Leif},
  booktitle={Proceedings of the 37th international ACM SIGIR conference on Research \& development in information retrieval},
  pages={3--12},
  year={2014}
}

@inproceedings{villa2013relevance,
  title={Is relevance hard work? Evaluating the effort of making relevant assessments},
  author={Villa, Robert and Halvey, Martin},
  booktitle={Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},
  pages={765--768},
  year={2013}
}

@article{rieh2012amount,
  title={Amount of invested mental effort (AIME) in online searching},
  author={Rieh, Soo Young and Kim, Yong-Mi and Markey, Karen},
  journal={Information Processing \& Management},
  volume={48},
  number={6},
  pages={1136--1150},
  year={2012},
  publisher={Elsevier}
}

@inproceedings{smith2008user,
  title={User adaptation: good results from poor systems},
  author={Smith, Catherine L and Kantor, Paul B},
  booktitle={Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={147--154},
  year={2008}
}

@article{bailey2011amount,
  title={Is amount of effort a better predictor of search success than use of specific search tactics?},
  author={Bailey, Earl and Kelly, Diane},
  journal={Proceedings of the American Society for Information Science and Technology},
  volume={48},
  number={1},
  pages={1--10},
  year={2011},
  publisher={Wiley Online Library}
}

@article{miller1956magical,
  title={The magical number seven, plus or minus two: Some limits on our capacity for processing information.},
  author={Miller, George A},
  journal={Psychological review},
  volume={63},
  number={2},
  pages={81},
  year={1956},
  publisher={American Psychological Association}
}

@article{diamond2013executive,
  title={Executive functions},
  author={Diamond, Adele},
  journal={Annual review of psychology},
  volume={64},
  pages={135--168},
  year={2013},
  publisher={Annual Reviews}
}

@article{arguello2019effects,
  title = {The Effects of Working Memory, Perceptual Speed, and Inhibition in Aggregated Search},
  author = {Arguello, Jaime and Choi, Bogeum},
  year = {2019},
  month = may,
  journal = {ACM Transactions on Information Systems},
  volume = {37},
  number = {3},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {1046-8188},
  doi = {10.1145/3322128},
  articleno = {36},
  issue-date = {July 2019},
  keywords = {aggregated search,Cognitive abilities,inhibition,perceptual speed,search behaviors,user engagement,working memory,workload}
}

@inproceedings{cole2020more,
  title = {More than Words: The Impact of Memory on How Undergraduates with Dyslexia Interact with Information},
  booktitle = {Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
  author = {Cole, Lynne and MacFarlane, Andrew and Makri, Stephann},
  year = {2020},
  series = {{{CHIIR}} '20},
  pages = {353--357},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3343413.3378005},
  isbn = {978-1-4503-6892-6},
  keywords = {dyslexia,human information behaviour,memory}
}

@inproceedings{gwizdka2013effects,
  ids = {10.1145/2500342.2500358},
  title = {Effects of Working Memory Capacity on Users' Search Effort},
  booktitle = {Proceedings of the {{International Conference}} on {{Multimedia}}, {{Interaction}}, {{Design}} and {{Innovation}}},
  author = {Gwizdka, Jacek},
  year = {2013},
  series = {{{MIDI}} '13},
  pages = {11:1--11:8},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2500342.2500358},
  articleno = {11},
  copyright = {All rights reserved},
  isbn = {978-1-4503-2303-1},
  keywords = {-tablet,CLIS-project,cognitive abilities,cognitive load,Cognitive load,mypub,search user interface,working memory,Working memory,z-IX-lab},
  annotation = {Cited by 0000}
}

@inproceedings{gwizdka2017can,
  ids = {10.1145/3020165.3022148,gwizdka2017can},
  title = {I {{Can}} and {{So I Search More}}: Effects {{Of Memory Span On Search Behavior}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Conference Human Information Interaction}} and {{Retrieval}}},
  author = {Gwizdka, Jacek},
  year = {2017},
  series = {{{CHIIR}} '17},
  pages = {341--344},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3020165.3022148},
  isbn = {978-1-4503-4677-1},
  keywords = {CLIS2-project,eye-tracking,Eye-tracking,mental effort,mypub,web search,working memory span.,z-IX-lab}
}



@inproceedings{zhang2011predicting,
  title = {Predicting {{Users}}' {{Domain Knowledge}} from {{Search Behaviors}}},
  booktitle = {Proceedings of the 34th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Zhang, Xiangmin and Cole, Michael and Belkin, Nicholas},
  year = {2011},
  series = {{{SIGIR}} '11},
  pages = {1225--1226},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2009916.2010131},
  isbn = {978-1-4503-0757-4},
  annotation = {00008}
}



@article{wildemuth2004effects,
  title = {The Effects of Domain Knowledge on Search Tactic Formulation},
  author = {Wildemuth, Barbara M.},
  year = {2004},
  month = feb,
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {55},
  number = {3},
  pages = {246--258},
  issn = {1532-2882, 1532-2890},
  doi = {10.1002/asi.10367},
  langid = {english}
}




@article{vakkari2019modeling,
  ids = {vakkari2019modelinga},
  title = {Modeling the Usefulness of Search Results as Measured by Information Use},
  author = {Vakkari, Pertti and V{\"o}lske, Michael and Potthast, Martin and Hagen, Matthias and Stein, Benno},
  year = {2019},
  month = may,
  journal = {Information Processing \& Management},
  volume = {56},
  number = {3},
  pages = {879--894},
  issn = {03064573},
  doi = {10.1016/j.ipm.2019.02.001},
  langid = {english},
  keywords = {nb-star}
}

@inproceedings{vakkari2020usefulness,
  ids = {vakkari2020UsefulnessSearchResult},
  title = {The {{Usefulness}} of {{Search Results}}: A {{Systematization}} of {{Types}} and {{Predictors}}},
  shorttitle = {The {{Usefulness}} of {{Search Results}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Vakkari, Pertti},
  year = {2020},
  month = mar,
  pages = {243--252},
  publisher = {{ACM}},
  address = {{Vancouver BC Canada}},
  doi = {10.1145/3343413.3377955},
  isbn = {978-1-4503-6892-6},
  langid = {english},
  keywords = {-tablet,document usefulness,nb-star,predictors of usefulness,search results}
}



@inproceedings{labaj2012modeling,
  title = {Modeling Parallel Web Browsing Behavior for Web-Based Educational Systems},
  booktitle = {2012 {{IEEE}} 10th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  author = {Labaj, Martin and Bielikova, Maria},
  year = {2012},
  month = nov,
  pages = {229--234},
  publisher = {{IEEE}},
  address = {{Star Lesn , Slovakia}},
  doi = {10.1109/ICETA.2012.6418330},
  isbn = {978-1-4673-5122-5 978-1-4673-5120-1 978-1-4673-5121-8},
  keywords = {nb-star,users-browsing-model}
}



@inproceedings{huang2010parallel,
  title = {Parallel Browsing Behavior on the Web},
  booktitle = {Proceedings of the 21st {{ACM}} Conference on {{Hypertext}} and Hypermedia - {{HT}} '10},
  author = {Huang, Jeff and White, Ryen},
  year = {2010},
  pages = {13},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/1810617.1810622},
  isbn = {978-1-4503-0041-4},
  langid = {english},
  keywords = {nb-star,users-browsing-model}
}




@inproceedings{white2009characterizing,
  title = {Characterizing the Influence of Domain Expertise on Web Search Behavior},
  booktitle = {Proceedings of the {{Second ACM International Conference}} on {{Web Search}} and {{Data Mining}} - {{WSDM}} '09},
  author = {White, Ryen and Dumais, Susan and Teevan, Jaime},
  year = {2009},
  pages = {132},
  publisher = {{ACM Press}},
  address = {{Barcelona, Spain}},
  doi = {10.1145/1498759.1498819},
  isbn = {978-1-60558-390-7},
  langid = {english},
  organization = {ACM},
}







@article{wineburg2017lateral,
  title={Lateral reading: Reading less and learning more when evaluating digital information},
  author={Wineburg, Sam and McGrew, Sarah},
  year={2017},
  publisher={Stanford History Education Group Working Paper}
}

@article{mcgrew2021skipping,
  title={Skipping the Source and Checking the Contents: An in-Depth Look at Students’ Approaches to Web Evaluation},
  author={McGrew, Sarah},
  journal={Computers in the Schools},
  volume={38},
  number={2},
  pages={75--97},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{breakstone2018we,
  title={Why we need a new approach to teaching digital literacy},
  author={Breakstone, Joel and McGrew, Sarah and Smith, Mark and Ortega, Teresa and Wineburg, Sam},
  journal={Phi Delta Kappan},
  volume={99},
  number={6},
  pages={27--32},
  year={2018},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{wineburg2016students,
  title={Why students can’t Google their way to the truth},
  author={Wineburg, Sam and McGrew, Sarah},
  journal={Education Week},
  volume={36},
  number={11},
  pages={22--28},
  year={2016}
}

@article{mcgrew2017challenge,
  title={The Challenge That's Bigger than Fake News: Civic Reasoning in a Social Media Environment.},
  author={McGrew, Sarah and Ortega, Teresa and Breakstone, Joel and Wineburg, Sam},
  journal={American educator},
  volume={41},
  number={3},
  pages={4},
  year={2017},
  publisher={ERIC}
}

@article{mihailidis2013media,
  title={Media literacy as a core competency for engaged citizenship in participatory democracy},
  author={Mihailidis, Paul and Thevenin, Benjamin},
  journal={American Behavioral Scientist},
  volume={57},
  number={11},
  pages={1611--1622},
  year={2013},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{kahne2012digital,
  title={Digital media literacy education and online civic and political participation},
  author={Kahne, Joseph and Lee, Nam-Jin and Feezell, Jessica Timpany},
  journal={International Journal of Communication},
  volume={6},
  pages={24},
  year={2012}
}

@article{mcgrew2020learning,
  title={Learning to evaluate: An intervention in civic online reasoning},
  author={McGrew, Sarah},
  journal={Computers \& Education},
  volume={145},
  pages={103711},
  year={2020},
  publisher={Elsevier}
}

@article{mcgrew2018can,
  title={Can students evaluate online sources? Learning from assessments of civic online reasoning},
  author={McGrew, Sarah and Breakstone, Joel and Ortega, Teresa and Smith, Mark and Wineburg, Sam},
  journal={Theory \& Research in Social Education},
  volume={46},
  number={2},
  pages={165--193},
  year={2018},
  publisher={Taylor \& Francis}
}


@article{collins2021reimagining,
  title = {Reimagining {{Digital Literacy Education}} to {{Save Ourselves}}},
  author = {Collins, Cory},
  year = {2021},
  journal = {Learning for Justice},
  number = {Fall 2021},
  url = {https://www.learningforjustice.org/magazine/fall-2021/reimagining-digital-literacy-education-to-save-ourselves},
  urldate = {2021-10-18},
  chapter = {Article},
  langid = {english}
}




@inproceedings{mcgrew2021click,
  title = {Click {{Restraint}}: Teaching {{Students}} to {{Analyze Search Results}}},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Computer}}-{{Supported Collaborative Learning}}-{{CSCL}} 2021},
  author = {McGrew, Sarah and Glass, Alex C},
  year = {2021},
  organization = {{International Society of the Learning Sciences}}
}




@article{breakstone2021students,
  title = {Students' {{Civic Online Reasoning}}: A {{National Portrait}}},
  shorttitle = {Students' {{Civic Online Reasoning}}},
  author = {Breakstone, Joel and Smith, Mark and Wineburg, Sam and Rapaport, Amie and Carle, Jill and Garland, Marshall and Saavedra, Anna},
  year = {2021},
  month = may,
  journal = {Educational Researcher},
  doi = {10.3102/0013189X211017495},
  langid = {english}
}




@article{lei2015effect,
  title={Effect of metacognitive strategies and verbal-imagery cognitive style on biology-based video search and learning performance},
  author={Lei, Pei-Lan and Sun, Chuen-Tsai and Lin, Sunny SJ and Huang, Tsung-Kuan},
  journal={Computers \& Education},
  volume={87},
  pages={326--339},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{roy2021note,
  title={Note the Highlight: Incorporating Active Reading Tools in a Search as Learning Environment},
  author={Roy, Nirmal and Torre, Manuel Valle and Gadiraju, Ujwal and Maxwell, David and Hauff, Claudia},
  booktitle={Proceedings of the 2021 Conference on Human Information Interaction and Retrieval},
  pages={229--238},
  year={2021}
}


@article{dicerbo2014impacts,
  title={Impacts of the digital ocean on education},
  author={DiCerbo, Kristen E and Behrens, John T},
  journal={London: Pearson},
  volume={1},
  year={2014}
}

@misc{pea2014learning,
  title={The learning analytics workgroup: A report on building the field of learning analytics for personalized learning at scale},
  author={Pea, Roy and Jacks, D},
  year={2014},
  publisher={Stanford, CA: Stanford University},
  howpublished = "\url{https://ed.stanford.edu/sites/default/files/law-report-complete-09-02-2014.pdf}",
  note = "[Online; accessed 2021-10-09]",
}

@article{cope2013new,
  title = {Towards a {{New Learning}}: The {{{\emph{Scholar}}}} {{Social Knowledge Workspace}}, in {{Theory}} and {{Practice}}},
  shorttitle = {Towards a {{New Learning}}},
  author = {Cope, Bill and Kalantzis, Mary},
  year = {2013},
  month = dec,
  journal = {E-Learning and Digital Media},
  volume = {10},
  number = {4},
  pages = {332--356},
  issn = {2042-7530, 2042-7530},
  doi = {10.2304/elea.2013.10.4.332},
  langid = {english},
  keywords = {nb-star}
}





@incollection{amina2017active,
  title={Active knowledge making: Epistemic dimensions of e-learning},
  author={Amina, Tabassum},
  booktitle={e-Learning Ecologies},
  pages={65--87},
  year={2017},
  publisher={Routledge}
}



@article{wilson2013comparison,
  title = {A Comparison of Techniques for Measuring Sensemaking and Learning within Participant-Generated Summaries},
  author = {Wilson, Mathew J and Wilson, Max L},
  year = {2013},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {64},
  number = {2},
  pages = {291--306},
  publisher = {{Wiley Online Library}},
  keywords = {learning-measure,nb-star}
}




@book{novak1984learning,
  title={Learning How to Learn}, 
  DOI={10.1017/CBO9781139173469},
  author={Novak, Joseph D. and Gowin, D. Bob},
  year={1984},
  publisher={Cambridge University Press}
}

@incollection{dervin2010sensemaking,
  title={Sense-making},
  author={Dervin, B. and Naumer, C.M.},
  year={2010},
  publisher={Taylor and Francis},
  booktitle = {Encyclopedia of library and information sciences (3rd ed.)},
  editor = {Bates, M.J. and Maac M.N.},
  pages = {4696–-4707},
}

@article{marton1976qualitative-b,
  title={On qualitative differences in learning—ii Outcome as a function of the learner's conception of the task},
  author={Marton, Ference and S{\"a}alj{\"o}, Roger},
  journal={british Journal of educational Psychology},
  volume={46},
  number={2},
  pages={115--127},
  year={1976},
  publisher={Wiley Online Library}
}

@article{marton1976qualitative-a,
  title={On qualitative differences in learning: I—Outcome and process},
  author={Marton, Ference and S{\"a}lj{\"o}, Roger},
  journal={British journal of educational psychology},
  volume={46},
  number={1},
  pages={4--11},
  year={1976},
  publisher={Wiley Online Library}
}


@article{piaget1936origins,
  title={Origins of intelligence in children.},
  author={Piaget, Jean},
  year={1936},
  publisher={Routledge \& Kegan Paul}
}

@article{wittrock1989generative,
  title={Generative processes of comprehension},
  author={Wittrock, Merlin C},
  journal={Educational psychologist},
  volume={24},
  number={4},
  pages={345--376},
  year={1989},
  publisher={Taylor \& Francis}
}

@article{grabowski1996generative,
  title={Generative learning: Past, present, and future},
  author={Grabowski, Barbara L},
  journal={Handbook of research for educational communications and technology},
  pages={897--918},
  year={1996},
  publisher={Simon \& Schuster Macmillan New York}
}

@incollection{rumelhart1981accretion,
  title={Accretion, tuning and restructuring: Three modes of learning},
  author={Rumelhart, David E and Norman, Donald A},
  year={1981},
  booktitle = {Semantic factors in cognition},
  editor = {Cotton, J.W. and Klatzky R.},
  pages = {37--90},
}

@incollection{rumelhart1977representation,
  title={The representation of knowledge in memory},
  author={Rumelhart, David E and Ortony, Andrew},
  year={1977},
  publisher={Hillsdale, NJ: Erlbaum},
  booktitle = {Schooling and the acquisition of knowledge},
  editor = {Anderson, R.C. and Spiro R.J. and Montague W.E.},
  pages = {99--135},
}

@book{ausubel1968educational,
  title={Educational psychology: A cognitive view},
  author={Ausubel, David Paul and Novak, Joseph Donald and Hanesian, Helen and others},
  volume={6},
  year={1968},
  publisher={Holt, Rinehart and Winston New York}
}

@article{novak2002meaningful,
  title={Meaningful learning: The essential factor for conceptual change in limited or inappropriate propositional hierarchies leading to empowerment of learners},
  author={Novak, Joseph D},
  journal={Science education},
  volume={86},
  number={4},
  pages={548--571},
  year={2002},
  publisher={Wiley Online Library}
}

@book{ausubel2012acquisition,
  title={The acquisition and retention of knowledge: A cognitive view},
  author={Ausubel, David Paul},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{zhang2014towards,
  title = {Towards a Comprehensive Model of the Cognitive Process and Mechanisms of Individual Sensemaking},
  author = {Zhang, Pengyi and Soergel, Dagobert},
  year = {2014},
  month = sep,
  journal = {Journal of the Association for Information Science and Technology},
  volume = {65},
  number = {9},
  pages = {1733--1756},
  issn = {2330-1643},
  doi = {10.1002/asi.23125},
  copyright = {\textcopyright{} 2014 ASIS\&T},
  langid = {english}
}







@book{sawyer2005cambridge,
  title = {The {{Cambridge}} Handbook of the Learning Sciences},
  author = {Sawyer, R Keith},
  year = {2005},
  publisher = {{Cambridge University Press}}
}




@inproceedings{egusa2010usingb,
  title = {Using a {{Concept Map}} to {{Evaluate Exploratory Search}}},
  booktitle = {Proceedings of the {{Third Symposium}} on {{Information Interaction}} in {{Context}}},
  author = {Egusa, Yuka and Saito, Hitomi and Takaku, Masao and Terai, Hitoshi and Miwa, Makiko and Kando, Noriko},
  year = {2010},
  series = {{{IIiX}} '10},
  pages = {175--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1840784.1840810},
  isbn = {978-1-4503-0247-0},
  keywords = {-tablet,concept map,Exploratory search,tab browsing,task models,user experiments,User studies},
  annotation = {00014}
}

@inproceedings{egusa2014howd,
  title = {How {{Concept Maps Change}} If a {{User Does Search}} or {{Not}}?},
  booktitle = {Proceedings of the 5th {{Information Interaction}} in {{Context Symposium}}},
  author = {Egusa, Yuka and Takaku, Masao and Saito, Hitomi},
  year = {2014},
  series = {{{IIiX}} '14},
  pages = {68--75},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2637002.2637012},
  isbn = {978-1-4503-2976-7},
  keywords = {-tablet,concept map,Exploratory search,task models,user experiments,User studies},
  annotation = {00001}
}

@inproceedings{egusa2014howe,
  title = {How to Evaluate Searching as Learning},
  booktitle = {Searching as {{Learning Workshop}} ({{IIiX}} 2014 Workshop)},
  author = {Egusa, Yuka and Takaku, Masao and Hitomi Saito},
  year = {2014},
  address = {{Regensburg, Germany}},
  url = {http://www.diigubc.ca/IIIXSAL/program.html},
  keywords = {-tablet},
  annotation = {00000}
}

@inproceedings{egusa2017evaluating,
  title = {Evaluating {{Complex Interactive Searches Using Concept Maps}}.},
  booktitle = {{{SCST}}@ {{CHIIR}}},
  author = {Egusa, Yuka and Takaku, Masao and Saito, Hitomi},
  year = {2017},
  pages = {15--17}
}




@article{saracevic1975relevance,
  title = {Relevance: A Review of and a Framework for the Thinking on the Notion in Information Science},
  author = {Saracevic, Tefko},
  year = {1975},
  journal = {Journal of the American Society for information science},
  volume = {26},
  number = {6},
  pages = {321--343},
  publisher = {{Wiley Online Library}},
  keywords = {nb-star}
}

@article{saracevic2007relevance,
  title = {Relevance: A Review of the Literature and a Framework for Thinking on the Notion in Information Science. {{Part II}}: Nature and Manifestations of Relevance},
  author = {Saracevic, Tefko},
  year = {2007},
  journal = {Journal of the American Society for Information Science and Technology},
  volume = {58},
  number = {13},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20682},
  pages = {1915--1933},
  doi = {10.1002/asi.20682},
  keywords = {nb-star}
}

@article{saracevic2007relevancea,
  title = {Relevance: A Review of the Literature and a Framework for Thinking on the Notion in Information Science. {{Part III}}: Behavior and Effects of Relevance},
  author = {Saracevic, Tefko},
  year = {2007},
  journal = {Journal of the American Society for information Science and Technology},
  volume = {58},
  number = {13},
  pages = {2126--2144},
  publisher = {{Wiley Online Library}},
  keywords = {nb-star}
}







@article{rieh2016searching,
  title = {Towards Searching as a Learning Process: A Review of Current Perspectives and Future Directions},
  shorttitle = {Towards Searching as a Learning Process},
  author = {Rieh, Soo Young and {Collins-Thompson}, Kevyn and Hansen, Preben and Lee, Hye-Jung},
  year = {2016},
  month = feb,
  journal = {Journal of Information Science},
  volume = {42},
  number = {1},
  pages = {19--34},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/0165551515615841},
  langid = {english},
  annotation = {00000}
}


@article{halttunen2005assessing,
  title = {Assessing Learning Outcomes in Two Information Retrieval Learning Environments},
  author = {Halttunen, Kai and Jarvelin, Kalervo},
  year = {2005},
  month = jul,
  journal = {Information Processing \& Management},
  volume = {41},
  number = {4},
  pages = {949--972},
  issn = {03064573},
  doi = {10.1016/j.ipm.2004.02.004},
  langid = {english},
  keywords = {concept map,concept-map,nb-star}
}




@book{francis2004coglab,
  title = {{{CogLab}} on a {{CD}}},
  publisher = {{Wadsworth Publishing Company}},
  author = {Francis, Greg and MacKewn, Angie and Goldthwaite, Danalee},
  year = {2004}
}

@misc{sheg2021website-reliability,
  title = {Website {{Reliability}} | {{Civic Online Reasoning}}},
  author = {{Stanford History Education Group (SHEG)}},
  year = {2021},
  howpublished = "\url{https://cor.stanford.edu/curriculum/assessments/website-reliability}",
  note = "[Online; accessed 2021-10-09]"
}


@misc{sheg2021webpage-comparison,
  title = {Webpage Comparison | {{Civic Online Reasoning}}},
  author = {{Stanford History Education Group (SHEG)}},
  year = {2021},
  howpublished = "\url{https://cor.stanford.edu/curriculum/assessments/webpage-comparison}",
  note = "[Online; accessed 2021-10-09]"
}


@inproceedings{brennan2016factor,
  title={Factor analysis of a search self-efficacy scale},
  author={Brennan, Kathy and Kelly, Diane and Zhang, Yinglong},
  booktitle={Proceedings of the 2016 ACM on conference on human information interaction and retrieval},
  pages={241--244},
  year={2016}
}


@misc{url-cross-browser-extn,
author = {{Mozilla Developer Network}},
title = {Building a cross-browser extension - Mozilla | MDN},
howpublished = {\url{https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Build-a-cross-browser-extension}},
month = {},
year = {2021},
note = {(Accessed on 03/08/2021)}
}

@inproceedings{bhattacharya2021yasbil,
  title={YASBIL: Yet Another Search Behaviour (and) Interaction Logger},
  author={Bhattacharya, Nilavra and Gwizdka, Jacek},
  booktitle={Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2585--2589},
  year={2021}
}

@phdthesis{crescenzi2020adaptation,
  title = {Adaptation in {{Information Search}} and {{Decision}}-{{Making}} under {{Time Pressure}}},
  author = {Crescenzi, Anita Marie Caywood},
  year = {2020},
  doi = {10.17615/YT6K-AC37},
  copyright = {In Copyright - Educational Use Permitted},
  langid = {english},
  school = {The University of North Carolina at Chapel Hill University Libraries}
}


@inproceedings{collins2016assessing,
  title={Assessing learning outcomes in web search: A comparison of tasks and query strategies},
  author={Collins-Thompson, Kevyn and Rieh, Soo Young and Haynes, Carl C and Syed, Rohail},
  booktitle={Proceedings of the 2016 ACM on conference on human information interaction and retrieval},
  pages={163--172},
  year={2016}
}





@misc{ngss-sensemaking,
  author={{Next Generation Science Standards}},
  title = {Task Annotation Project in Science | Sense-making},
  howpublished = "\url{https://www.nextgenscience.org/sites/default/files/TAPS\%20Sense-making.pdf}",
  note = "[Online; accessed 2021-10-09]",
  year = {2021}
}



@incollection{ko2021seeking,
  title = {Seeking Information},
  booktitle = {Foundations of {{Information}}},
  author = {Ko, Amy J.},
  year = {2021},
  url = {https://faculty.washington.edu/ajko/books/foundations-of-information/#/seeking},
  urldate = {2021-10-11},
  langid = {english}
}


@misc{note-taking-strategies-umass,
  author={{UMass Amherst Student Success}},
  title = {Note Taking Strategies Inventory | Success@UMass},
  howpublished = "\url{https://www.umass.edu/studentsuccess/sites/default/files/inline-files/note-taking-strategies-0.pdf}",
  note = "[Online; accessed 2021-10-09]",
  year = {2021}
}


@book{novak2010learninga,
  title = {Learning, Creating, and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations},
  shorttitle = {Learning, Creating, and Using Knowledge},
  author = {Novak, Joseph Donald},
  year = {2010},
  edition = {2nd ed},
  publisher = {{Routledge}},
  address = {{New York, NY}},
  isbn = {978-0-415-99184-1 978-0-415-99185-8 978-0-203-86200-1},
  langid = {english},
  lccn = {LB1060 .N677 2010},
  keywords = {concept-map,star},
  annotation = {OCLC: ocn277196025}
}




@book{moon2011applieda,
  title = {Applied {{Concept Mapping}}: Capturing, {{Analyzing}}, and {{Organizing Knowledge}}},
  shorttitle = {Applied {{Concept Mapping}}},
  editor = {Moon, Brian and Hoffman, Robert R. and Novak, Joseph and Canas, Alberto},
  year = {2011},
  month = feb,
  edition = {Zeroth},
  publisher = {{CRC Press}},
  doi = {10.1201/b10716},
  isbn = {978-0-429-24635-7},
  langid = {english},
  keywords = {concept-map}
}




@misc{guyan2013improving,
  title = {Improving {{Learner Motivation}} for {{eLearning}}},
  author = {Guyan, Matt},
  year = {2013},
  journal = {Learning Snippets},
  url = {https://learningsnippets.wordpress.com/2013/10/30/improving-learner-motivation-for-elearning/},
  urldate = {2021-10-08},
  langid = {english}
}



@article{ryan2000intrinsic,
  title={Intrinsic and extrinsic motivations: Classic definitions and new directions},
  author={Ryan, Richard M and Deci, Edward L},
  journal={Contemporary educational psychology},
  volume={25},
  number={1},
  pages={54--67},
  year={2000},
  publisher={Elsevier}
}

@article{ryan1982control,
  title={Control and information in the intrapersonal sphere: An extension of cognitive evaluation theory.},
  author={Ryan, Richard M},
  journal={Journal of personality and social psychology},
  volume={43},
  number={3},
  pages={450},
  year={1982},
  publisher={American Psychological Association}
}


@article{ryan2000self,
  title={Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being.},
  author={Ryan, Richard M and Deci, Edward L},
  journal={American psychologist},
  volume={55},
  number={1},
  pages={68},
  year={2000},
  publisher={American Psychological Association}
}

@book{deci2013intrinsic,
  title={Intrinsic motivation and self-determination in human behavior},
  author={Deci, Edward L and Ryan, Richard M},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{tahamtan2019effect,
  title={The Effect of Motivation on Web Search Behaviors of Health Consumers},
  author={Tahamtan, Iman},
  booktitle={Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages={401--404},
  year={2019}
}

@book{ryan2017self,
  title={Self-determination theory: Basic psychological needs in motivation, development, and wellness},
  author={Ryan, Richard M and Deci, Edward L},
  year={2017},
  publisher={Guilford Publications}
}

@misc{cherry2020what,
  title = {What {{Is Motivation}}?},
  author = {Cherry, Kendra},
  year = {2020},
  journal = {Verywell Mind},
  url = {https://www.verywellmind.com/what-is-motivation-2795378},
  urldate = {2021-10-08},
  chapter = {Verywell},
  langid = {english}
}



@article{kanfer1970self-a,
  title={Self-regulation: Research, issues, and speculations},
  author={Kanfer, Frederick H},
  journal={Behavior modification in clinical psychology},
  volume={74},
  pages={178--220},
  year={1970},
  publisher={New York}
}
@article{kanfer1970self-b,
  title={Self-monitoring: Methodological limitations and clinical applications.},
  author={Kanfer, Frederick H},
  year={1970},
  publisher={American Psychological Association}
}

@misc{brown1998self,
  title={Self-regulation and the addictive behaviours},
  author={Brown, JM},
  year={1998},
  publisher={New York: Plenum Press}
}

@incollection{brown1999self,
  title={The self-regulation questionnaire},
  author={Brown, Janice M and Miller, William R and Lawendowski, Lauren A},
  booktitle = {Innovations in clinical practice: A sourcebook},
  publisher = {Professional Resource Press/Professional Resource Exchange},
  editor = {VandeCreek L. and Jackson T.L.},
  pages = {281--292},
  volume = {17},
  address = {Sarasota, FL},
  year={1999}
}

@article{miller1991self,
  title={Self-regulation as a conceptual basis for the prevention and treatment of addictive behaviours},
  author={Miller, William R and Brown, Janice M},
  journal={Self-control and the addictive behaviours},
  pages={3--79},
  editor = {Heather N. and Miller W. R. and Greeley J.},
  publisher={Sydney: Maxwell Macmillan Publishing Australia},
  year={1991}
}

@misc{mannion2020metacognition,
  title = {Metacognition, Self-Regulation and Self-Regulated Learning: What's the Difference?},
  shorttitle = {Metacognition, Self-Regulation and Self-Regulated Learning},
  author = {Mannion, James},
  year = {2020},
  journal = {impact.chartered.college},
  url = {https://impact.chartered.college/article/metacognition-self-regulation-regulated-learning-difference/},
  urldate = {2021-10-07},
  langid = {american}
}




@misc{terlecki2020revising,
  title = {Revising the {{Metacognitive Awareness Inventory}} ({{MAI}}) to Be {{More User}}-{{Friendly}}},
  author = {Terlecki, Melissa},
  year = {2020},
  month = jan,
  journal = {Improve with Metacognition},
  url = {https://www.improvewithmetacognition.com/revising-the-metacognitive-awareness-inventory/},
  urldate = {2021-10-08},
  langid = {american}
}



@misc{viu2021mai,
  title = {Metacognitive Awareness Inventory (MAI)},
  author = {{Vancouver Island University}},
  year = {2021},
  url = {https://alearningjourneyweb.files.wordpress.com/2017/03/self-directed-vs-self-regulated-chart.pdf},
}

@incollection{blanken2017metacognition,
  title={Metacognition: Cognitive Dimensions of e-Learning},
  author={Blanken-Webb, Jane},
  booktitle={e-Learning Ecologies},
  pages={163--182},
  year={2017},
  publisher={Routledge}
}

@article{schraw1994assessing,
  title = {Assessing {{Metacognitive Awareness}}},
  author = {Schraw, Gregory and Dennison, Rayne Sperling},
  year = {1994},
  month = oct,
  journal = {Contemporary Educational Psychology},
  volume = {19},
  number = {4},
  pages = {460--475},
  issn = {0361476X},
  doi = {10.1006/ceps.1994.1033},
  langid = {english}
}



@article{jossberger2010challenge,
  title={The challenge of self-directed and self-regulated learning in vocational education: A theoretical analysis and synthesis of requirements},
  author={Jossberger, Helen and Brand-Gruwel, Saskia and Boshuizen, Henny and Van de Wiel, Margje},
  journal={Journal of vocational education and training},
  volume={62},
  number={4},
  pages={415--440},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{saks2014distinguishing,
  title = {Distinguishing {{Self}}-Directed and {{Self}}-Regulated {{Learning}} and {{Measuring}} Them in the {{E}}-Learning {{Context}}},
  author = {Saks, Katrin and Leijen, {\"A}li},
  year = {2014},
  month = feb,
  journal = {Procedia - Social and Behavioral Sciences},
  volume = {112},
  pages = {190--198},
  issn = {18770428},
  doi = {10.1016/j.sbspro.2014.01.1155},
  langid = {english}
}



@article{zimmerman1989social,
  title={A social cognitive view of self-regulated academic learning.},
  author={Zimmerman, Barry J},
  journal={Journal of educational psychology},
  volume={81},
  number={3},
  pages={329},
  year={1989},
  publisher={American Psychological Association}
}

@book{knowles1975self,
  title={Self-directed learning: a guide for learners and teachers.},
  author={Knowles, Malcolm S},
  year={1975},
  publisher={New York: Association press}
}

@misc{teresa2017selfdirected,
  title = {Self-{{Directed Learning}} vs {{Self Regulated Learning}} Chart},
  author = {Teresa},
  year = {2017},
  url = {https://alearningjourneyweb.files.wordpress.com/2017/03/self-directed-vs-self-regulated-chart.pdf}
}




@article{loyens2008selfdirected,
  title = {Self-{{Directed Learning}} in {{Problem}}-{{Based Learning}} and Its {{Relationships}} with {{Self}}-{{Regulated Learning}}},
  author = {Loyens, Sofie M. M. and Magda, Joshua and Rikers, Remy M. J. P.},
  year = {2008},
  month = dec,
  journal = {Educational Psychology Review},
  volume = {20},
  number = {4},
  pages = {411--427},
  issn = {1040-726X, 1573-336X},
  doi = {10.1007/s10648-008-9082-7},
  langid = {english}
}




@article{zlatkin2021students,
  title = {Students' Online Information Use and Learning Progress in Higher Education \textendash{} {{A}} Critical Literature Review},
  author = {{Zlatkin-Troitschanskaia}, Olga and Hartig, Johannes and Goldhammer, Frank and Krstev, Jan},
  year = {2021},
  month = aug,
  journal = {Studies in Higher Education},
  pages = {1--26},
  issn = {0307-5079, 1470-174X},
  doi = {10.1080/03075079.2021.1953336},
  langid = {english},
  keywords = {nb-star}
}




@book{cope2017elearningc,
  title = {E-{{Learning Ecologies}}: Principles for {{New Learning}} and {{Assessment}}},
  shorttitle = {E-{{Learning Ecologies}}},
  author = {Cope, Bill and Kalantzis, Mary},
  year = {2017},
  month = feb,
  publisher = {{Taylor \& Francis}},
  googlebooks = {eUQlDwAAQBAJ},
  isbn = {978-1-317-27336-3},
  langid = {english}
}



@book{kalantzis2012newa,
  title = {New {{Learning}}: Elements of a {{Science}} of {{Education}}},
  shorttitle = {New {{Learning}}},
  author = {Kalantzis, Mary and Cope, Bill},
  year = {2012},
  month = jun,
  publisher = {{Cambridge University Press}},
  googlebooks = {N3z3\-0XdVYsC},
  isbn = {978-1-107-64428-1},
  langid = {english}
}


@book{ambrose2010howa,
  title = {How {{Learning Works}}: Seven {{Research}}-{{Based Principles}} for {{Smart Teaching}}},
  shorttitle = {How {{Learning Works}}},
  author = {Ambrose, Susan A. and Bridges, Michael W. and DiPietro, Michele and Lovett, Marsha C. and Norman, Marie K.},
  year = {2010},
  month = apr,
  publisher = {{John Wiley \& Sons}},
  googlebooks = {UZE6fBn81\-EC},
  isbn = {978-0-470-61760-1},
  langid = {english}
}


@article{vakkari2016searching,
  title = {Searching as Learning: A Systematization Based on Literature},
  shorttitle = {Searching as Learning},
  author = {Vakkari, Pertti},
  year = {2016},
  month = feb,
  journal = {Journal of Information Science},
  volume = {42},
  number = {1},
  pages = {7--18},
  issn = {0165-5515, 1741-6485},
  doi = {10.1177/0165551515615833},
  langid = {english},
  keywords = {nb-star}
}


@article{van2017development,
  title={Development of the digital health literacy instrument: measuring a broad spectrum of health 1.0 and health 2.0 skills},
  author={Van Der Vaart, Rosalie and Drossaert, Constance},
  journal={Journal of medical Internet research},
  volume={19},
  number={1},
  pages={e27},
  year={2017},
  publisher={JMIR Publications Inc., Toronto, Canada}
}



@article{tsigilis2003temporal,
  title={Temporal stability of the intrinsic motivation inventory},
  author={Tsigilis, Nikolaos and Theodosiou, Argiris},
  journal={Perceptual and motor skills},
  volume={97},
  number={1},
  pages={271--280},
  year={2003},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{kanniainen2021assessing,
  title = {Assessing Reading and Online Research Comprehension: Do Difficulties in Attention and Executive Function Matter?},
  shorttitle = {Assessing Reading and Online Research Comprehension},
  author = {Kanniainen, Laura and Kiili, Carita and Tolvanen, Asko and Aro, Mikko and Anmarkrud, {\O}istein and Lepp{\"a}nen, Paavo H.T.},
  year = {2021},
  month = apr,
  journal = {Learning and Individual Differences},
  volume = {87},
  pages = {101985},
  issn = {10416080},
  doi = {10.1016/j.lindif.2021.101985},
  langid = {english}
}




@article{leu2015new,
  title = {The {{New Literacies}} of {{Online Research}} and {{Comprehension}}: Rethinking the {{Reading Achievement Gap}}},
  shorttitle = {The {{New Literacies}} of {{Online Research}} and {{Comprehension}}},
  author = {Leu, Donald J. and Forzani, Elena and Rhoads, Chris and Maykel, Cheryl and Kennedy, Clint and Timbrell, Nicole},
  year = {2015},
  month = jan,
  journal = {Reading Research Quarterly},
  volume = {50},
  number = {1},
  pages = {37--59},
  issn = {00340553},
  doi = {10.1002/rrq.85},
  langid = {english}
}



@article{terlecki2018call,
  title = {A {{Call}} for {{Metacognitive Intervention}}: Improvements {{Due}} to {{Curricular Programming}} in {{Leadership}}},
  shorttitle = {A {{Call}} for {{Metacognitive Intervention}}},
  author = {Terlecki, Melissa and McMahon, Anne},
  year = {2018},
  month = oct,
  journal = {Journal of Leadership Education},
  volume = {17},
  number = {4},
  pages = {130--145},
  issn = {15529045},
  doi = {10.12806/V17/I4/R8},
  keywords = {metacognition}
}



@book{marchionini1997information,
  title = {Information {{Seeking}} in {{Electronic Environments}}},
  author = {Marchionini, Gary},
  year = {1997},
  month = mar,
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-58674-0},
  language = {en}
}

@inproceedings{pirolli1996scatter,
author = {Pirolli, Peter and Schank, Patricia and Hearst, Marti and Diehl, Christine},
title = {Scatter/Gather Browsing Communicates the Topic Structure of a Very Large Text Collection},
year = {1996},
booktitle = {Conference on Human Factors in Computing Systems (CHI'96)}
}

@book{council2000how,
  title = {How People Learn: {{Brain}}, Mind, Experience, and School: {{Expanded}} Edition},
  author={{National Research Council}},
  year = {2000},
  publisher = {{The National Academies Press}},
  address = {{Washington, DC}},
  doi = {10.17226/9853},
  isbn = {978-0-309-07036-2}
}

@book{sousa2017brain,
  title={How the Brain Learns, Fifth Edition},
  author={Sousa, David A},
  year={2017},
  publisher={Corwin Press},
  isbn={978-1506346304}
}

@article{kelly2009evaluation,
  title={Evaluation challenges and directions for information-seeking support systems},
  author={D. {Kelly} and S. {Dumais} and J. O. {Pedersen}},
  journal={IEEE Computer},
  volume={42},
  number={3},
  year={2009}
}

@inproceedings{hoeber2019study,
  title={A Study of Academic Search Scenarios and Information Seeking Behaviour},
  author={Hoeber, Orland and Patel, Dolinkumar and Storie, Dale},
  booktitle={Conference on Human Information Interaction and Retrieval (CHIIR)},
  pages={231--235},
  year={2019}
}

@inproceedings{iwilds2020,
author = {Bhattacharya, Nilavra and Gwizdka, Jacek},
title = {Visualizing and Quantifying Vocabulary Learning During Search},
year = {2020},
booktitle = {Proceedings of the CIKM 2020 Workshops, October 19-20, 2020,
Galway, Ireland}
}

@book{webb1999unobtrusive,
  title={Unobtrusive measures},
  author={Webb, Eugene J and Campbell, Donald T and Schwartz, Richard D and Sechrest, Lee},
  volume={2},
  year={1999},
  publisher={Sage Publications}
}

@misc{Namedent58:online,
author = {Wikipedia},
title = {Named-entity recognition - Wikipedia},
howpublished = {\url{https://en.wikipedia.org/wiki/Named-entity-recognition}},
month = {},
year = {2020},
note = {(Accessed on 09/04/2020)}
}

@misc{Effectsi42:online,
author = {Wikipedia},
title = {Effect size - Wikipedia},
howpublished = {\url{https://en.wikipedia.org/wiki/Effect-size}},
month = {},
year = {2020},
note = {(Accessed on 09/04/2020)}
}

@article{griffin2006observing,
  title={Observing the what and when of language production for different age groups by monitoring speakers’ eye movements},
  author={Griffin, Zenzi M and Spieler, Daniel H},
  journal={Brain and Language},
  volume={99},
  number={3},
  pages={272--288},
  year={2006},
  publisher={Elsevier}
}

@article{underwood2003driving,
  title={Driving experience, attentional focusing, and the recall of recently inspected events},
  author={Underwood, Geoffrey and Chapman, Peter and Berger, Zoe and Crundall, David},
  journal={Transportation Research Part F: Traffic Psychology and Behaviour},
  volume={6},
  number={4},
  pages={289--304},
  year={2003},
  publisher={Elsevier}
}

@article{yoo2008unconscious,
  title={Unconscious processing of web advertising: Effects on implicit memory, attitude toward the brand, and consideration set},
  author={Yoo, Chan Yun},
  journal={Journal of interactive marketing},
  volume={22},
  number={2},
  pages={2--18},
  year={2008},
  publisher={Elsevier}
}

@misc{WhatEyeT54:online,
author = {Genco, Steve},
title = {What Eye-Tracking Can and Can’t Tell You About Attention - NMSBA},
howpublished = {\url{https://www.nmsba.com/buying-neuromarketing/neuromarketing-techniques/what-eye-tracking-can-and-cant-tell-you-about-attention}},
month = {},
year = {n.d.},
note = {(Accessed on 09/04/2020)}
}

@article{hyrskykari2008gaze,
  title={Gaze path stimulation in retrospective think-aloud},
  author={Hyrskykari, Aulikki and Ovaska, Saila and Majaranta, P{\"a}vi and R{\"a}ih{\"a}, Kari-Jouko and Lehtinen, Merja},
  year={2008}
}

@misc{Differen62:online,
author = {Wikipedia},
title = {Differential privacy},
howpublished = {\url{https://en.wikipedia.org/wiki/Differential-privacy}},
month = {},
year = {2020},
note = {(Accessed on 09/04/2020)}
}

@misc{Neuroeth2:online,
author = {Dana-Foundation},
title = {Neuroethics: Mapping the Field | Dana Foundation},
howpublished = {\url{https://dana.org/article/neuroethics-mapping-the-field/}},
month = {},
year = {2002},
note = {(Accessed on 09/04/2020)}
}

@misc{Eyetrack99:online,
author = {Tobii},
title = {Eye tracking in gaming, how does it work? – Tobii Eye Tracking Support},
howpublished = {\url{https://help.tobii.com/hc/en-us/articles/115003295025-Eye-tracking-in-gaming-how-does-it-work-}},
month = {},
year = {2017},
note = {(Accessed on 09/04/2020)}
}

@misc{Eyetrack1:online,
author = {Horti, Samuel},
title = {Eye tracking for gamers: seeing is believing | TechRadar},
howpublished = {\url{https://www.techradar.com/news/eye-tracking-for-gamers-seeing-is-believing}},
month = {},
year = {2018},
note = {(Accessed on 09/04/2020)}
}

@misc{Leagueof99:online,
author = {Maggiora, Ignacio della},
title = {League of Legends Eye Tracking BETA: Live Now | Official Tobii Eye Tracking Blog},
howpublished = {\url{https://blog.tobii.com/beta-testers-hear-our-call-4f74685acc02}},
month = {},
year = {2020},
note = {(Accessed on 09/04/2020)}
}

@misc{WindowsI62:online,
author = {Tobii},
title = {Windows Interaction features for Alienware – Tobii Eye Tracking Support},
howpublished = {\url{https://help.tobii.com/hc/en-us/articles/360003139514}},
month = {},
year = {2018},
note = {(Accessed on 09/04/2020)}
}

@misc{url-vice-et,
  author={Bar-Zeev, Avi},
  title = {The Eyes Are the Prize: Eye-Tracking Technology Is Advertising's Holy Grail. VR and AR Eye-Tracking Technology Will Usher in a Privacy Dystopia.},
  year = {2019},
  howpublished = "\url{https://www.vice.com/en-ca/article/bj9ygv/the-eyes-are-the-prize-eye-tracking-technology-is-advertisings-holy-grail}",
  note = "[Online; accessed 2020-09-02]"
}


@inproceedings{berget2019experimental,
  title={Experimental methods in IIR: The tension between rigour and ethics in studies involving users with dyslexia},
  author={Berget, Gerd and MacFarlane, Andrew},
  booktitle={Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages={93--101},
  year={2019}
}

@article{flicker2004ethical,
  title={Ethical dilemmas in research on Internet communities},
  author={Flicker, Sarah and Haans, Dave and Skinner, Harvey},
  journal={Qualitative health research},
  volume={14},
  number={1},
  pages={124--134},
  year={2004},
  publisher={Sage Publications}
}

@article{walther2002research,
  title={Research ethics in Internet-enabled research: Human subjects issues and methodological myopia},
  author={Walther, Joseph B},
  journal={Ethics and information technology},
  volume={4},
  number={3},
  pages={205--216},
  year={2002},
  publisher={Springer}
}

@misc{Deductiv2:online,
author = {Michigan, University of},
title = {Deductive Disclosure Risk},
howpublished = {\url{https://www.icpsr.umich.edu/web/pages/DSDR/disclosure.html}},
month = {},
year = {2020},
note = {(Accessed on 09/04/2020)}
}

@inproceedings{lin2020we,
  title={We Could, but Should We? Ethical Considerations for Providing Access to GeoCities and Other Historical Digital Collections},
  author={Lin, Jimmy and Milligan, Ian and Oard, Douglas W and Ruest, Nick and Shilton, Katie},
  booktitle={Proceedings of the 2020 Conference on Human Information Interaction and Retrieval},
  pages={135--144},
  year={2020}
}

@book{iachello2007end,
  title={End-user privacy in human-computer interaction},
  author={Iachello, Giovanni and Hong, Jason},
  volume={1},
  year={2007},
  publisher={Now Publishers Inc}
}

@article{kelly2009methods,
  title={Methods for evaluating interactive information retrieval systems with users},
  author={Kelly, Diane},
  journal={Foundations and trends in Information Retrieval},
  volume={3},
  number={1—2},
  pages={1--224},
  year={2009},
  publisher={Now Publishers Inc.}
}

@inproceedings{si2014privacy,
  title={Privacy-preserving ir: When information retrieval meets privacy and security},
  author={Si, Luo and Yang, Hui},
  booktitle={Proceedings of the 37th international ACM SIGIR conference on Research \& development in information retrieval},
  pages={1295--1295},
  year={2014}
}
 
@article{saito1992does,
  title={Does fatigue exist in a quantitative measurement of eye movements?},
  author={Saito, Susumu},
  journal={Ergonomics},
  volume={35},
  number={5-6},
  pages={607--615},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{bocca2006total,
  title={Total sleep deprivation effect on disengagement of spatial attention as assessed by saccadic eye movements},
  author={Bocca, Marie-Laure and Denise, Pierre},
  journal={Clinical Neurophysiology},
  volume={117},
  number={4},
  pages={894--899},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{pinkosova2020cortical,
  title={The cortical activity of graded relevance},
  author={Pinkosova, Zuzana and McGeown, William J and Moshfeghi, Yashar},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={299--308},
  year={2020}
}

@inproceedings{robinson2016novel,
  title={A novel approach for ensuring the privacy of EEG signals using application-specific feature extraction and AES algorithm},
  author={Robinson, Vijitha and Varghese, Elizabeth B},
  booktitle={2016 International Conference on Inventive Computation Technologies (ICICT)},
  volume={2},
  pages={1--6},
  year={2016},
  organization={IEEE}
}

@inproceedings{steil2019privaceye,
  title={Privaceye: privacy-preserving head-mounted eye tracking using egocentric scene image and eye movement features},
  author={Steil, Julian and Koelle, Marion and Heuten, Wilko and Boll, Susanne and Bulling, Andreas},
  booktitle={Proceedings of the 11th ACM Symposium on Eye Tracking Research \& Applications},
  pages={1--10},
  year={2019}
}

@inproceedings{holler2018eeg,
  title={Do eeg-biometric templates threaten user privacy?},
  author={H{\"o}ller, Yvonne and Uhl, Andreas},
  booktitle={Proceedings of the 6th ACM Workshop on Information Hiding and Multimedia Security},
  pages={31--42},
  year={2018}
}

@inproceedings{liebling2014privacy,
author = {Liebling, Daniel J. and Preibusch, S\"{o}ren},
title = {Privacy Considerations for a Pervasive Eye Tracking World},
year = {2014},
isbn = {9781450330473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.lib.utexas.edu/10.1145/2638728.2641688},
doi = {10.1145/2638728.2641688},
booktitle = {Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication},
pages = {1169–1177},
numpages = {9},
keywords = {biometrics, gaze tracking, eye tracking, privacy},
location = {Seattle, Washington},
series = {UbiComp '14 Adjunct}
}

@inproceedings{shekh2020eyecrowdata,
  title={EyeCrowdata: Towards a Web-based Crowdsourcing Platform for Web-related Eye-Tracking Data},
  author={Shekh. Khalil, Naziha and Dogruer, Ecem and Elosta, Abdulmohimen KO and Eraslan, Sukru and Yesilada, Yeliz and Harper, Simon},
  booktitle={ACM Symposium on Eye Tracking Research and Applications},
  pages={1--6},
  year={2020}
}

@article{racine2005fmri,
  title={fMRI in the public eye},
  author={Racine, Eric and Bar-Ilan, Ofek and Illes, Judy},
  journal={Nature Reviews Neuroscience},
  volume={6},
  number={2},
  pages={159--164},
  year={2005},
  publisher={Nature Publishing Group}
}

@article{emerging2019neuroethics,
  title={Neuroethics at 15: The current and future environment for neuroethics},
  author={Emerging Issues Task Force, International Neuroethics Society},
  journal={AJOB neuroscience},
  volume={10},
  number={3},
  pages={104--110},
  year={2019},
  publisher={Taylor \& Francis}
}

@misc{marcus2002neuroethics,
  title={Neuroethics: Mapping the field, proceedings of the dana foundation conference},
  author={Marcus, D},
  year={2002},
  publisher={University of Chicago Press Chicago}
}


@article{bailey2009subtle,
  title={Subtle gaze direction},
  author={Bailey, Reynold and McNamara, Ann and Sudarsanam, Nisha and Grimm, Cindy},
  journal={ACM Transactions on Graphics (TOG)},
  volume={28},
  number={4},
  pages={1--14},
  year={2009},
  publisher={ACM New York, NY, USA}
}

@article{sun2018towards,
  title={Towards virtual reality infinite walking: dynamic saccadic redirection},
  author={Sun, Qi and Patney, Anjul and Wei, Li-Yi and Shapira, Omer and Lu, Jingwan and Asente, Paul and Zhu, Suwen and McGuire, Morgan and Luebke, David and Kaufman, Arie},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--13},
  year={2018},
  publisher={ACM New York, NY, USA}
}



@article{parnamets2015biasing,
  author = {P{\"a}rnamets, Philip and Johansson, Petter and Hall, Lars and Balkenius, Christian and Spivey, Michael J. and Richardson, Daniel C.},
	title = {Biasing moral decisions by exploiting the dynamics of eye gaze},
	volume = {112},
	number = {13},
	pages = {4170--4175},
	year = {2015},
	doi = {10.1073/pnas.1415250112},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/112/13/4170},
	eprint = {https://www.pnas.org/content/112/13/4170.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{kroger2018unexpected,
  title={Unexpected inferences from sensor data: a hidden privacy threat in the internet of things},
  author={Kr{\"o}ger, Jacob},
  booktitle={IFIP International Internet of Things Conference},
  pages={147--159},
  year={2018},
  organization={Springer}
}

@inproceedings{kroger2019does,
  title={What does your gaze reveal about you? On the privacy implications of eye tracking},
  author={Kr{\"o}ger, Jacob Leon and Lutz, Otto Hans-Martin and M{\"u}ller, Florian},
  booktitle={IFIP International Summer School on Privacy and Identity Management},
  pages={226--241},
  year={2019},
  organization={Springer}
}



@article{carter2020best,
  title={Best practices in eye tracking research},
  author={Carter, Benjamin T and Luke, Steven G},
  journal={International Journal of Psychophysiology},
  year={2020},
  publisher={Elsevier}
}

@article{howells2012childhood,
  title={Childhood trauma is associated with altered cortical arousal: insights from an EEG study},
  author={Howells, Fleur Margaret and Stein, Dan J and Russell, Vivienne Ann},
  journal={Frontiers in integrative neuroscience},
  volume={6},
  pages={120},
  year={2012},
  publisher={Frontiers}
}

@article{felmingham2011eye,
  title={Eye tracking and physiological reactivity to threatening stimuli in posttraumatic stress disorder},
  author={Felmingham, Kim L and Rennie, Chris and Manor, Barry and Bryant, Richard A},
  journal={Journal of anxiety disorders},
  volume={25},
  number={5},
  pages={668--673},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{feit2017toward,
author = {Feit, Anna Maria and Williams, Shane and Toledo, Arturo and Paradiso, Ann and Kulkarni, Harish and Kane, Shaun and Morris, Meredith Ringel},
title = {Toward Everyday Gaze Input: Accuracy and Precision of Eye Tracking and Implications for Design},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025599},
doi = {10.1145/3025453.3025599},
abstract = {For eye tracking to become a ubiquitous part of our everyday interaction with computers, we first need to understand its limitations outside rigorously controlled labs, and develop robust applications that can be used by a broad range of users and in various environments. Toward this end, we collected eye tracking data from 80 people in a calibration-style task, using two different trackers in two lighting conditions. We found that accuracy and precision can vary between users and targets more than six-fold, and report on differences between lighting, trackers, and screen regions. We show how such data can be used to determine appropriate target sizes and to optimize the parameters of commonly used filters. We conclude with design recommendations and examples how our findings and methodology can inform the design of error-aware adaptive applications.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {1118–1130},
numpages = {13},
keywords = {sensor noise, eye tracking, gaze filters, adaptive interfaces},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{walber2013can,
  title={Can you see it? two novel eye-tracking-based measures for assigning tags to image regions},
  author={Walber, Tina and Scherp, Ansgar and Staab, Steffen},
  booktitle={International Conference on Multimedia Modeling},
  pages={36--46},
  year={2013},
  organization={Springer}
}

@article{hollenstein2019zuco,
  title={Zuco 2.0: A dataset of physiological recordings during natural reading and annotation},
  author={Hollenstein, Nora and Troendle, Marius and Zhang, Ce and Langer, Nicolas},
  journal={arXiv preprint arXiv:1912.00903},
  year={2019}
}

@article{hajimirza2012reading,
  title={Reading users' minds from their eyes: a method for implicit image annotation},
  author={Hajimirza, S Navid and Proulx, Michael J and Izquierdo, Ebroul},
  journal={IEEE Transactions on Multimedia},
  volume={14},
  number={3},
  pages={805--815},
  year={2012},
  publisher={IEEE}
}

@article{panetta2019software,
  title={Software Architecture for Automating Cognitive Science Eye-Tracking Data Analysis and Object Annotation},
  author={Panetta, Karen and Wan, Qianwen and Kaszowska, Aleksandra and Taylor, Holly A and Agaian, Sos},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={49},
  number={3},
  pages={268--277},
  year={2019},
  publisher={IEEE}
}

@inproceedings{karessli2017gaze,
  title={Gaze embeddings for zero-shot image classification},
  author={Karessli, Nour and Akata, Zeynep and Schiele, Bernt and Bulling, Andreas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4525--4534},
  year={2017}
}

@article{lucas2019image,
  title={Image Annotation by Eye Tracking: Accuracy and Precision of Centerlines of Obstructed Small-Bowel Segments Placed Using Eye Trackers},
  author={Lucas, Alfredo and Wang, Kang and Santillan, Cynthia and Hsiao, Albert and Sirlin, Claude B and Murphy, Paul M},
  journal={Journal of digital imaging},
  volume={32},
  number={5},
  pages={855--864},
  year={2019},
  publisher={Springer}
}

@inproceedings{joshi2013more,
  title={More than meets the eye: Study of Human Cognition in Sense Annotation},
  author={Joshi, Salil and Kanojia, Diptesh and Bhattacharyya, Pushpak},
  booktitle={Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={733--738},
  year={2013}
}

@inproceedings{joshi2014measuring,
  title={Measuring sentiment annotation complexity of text},
  author={Joshi, Aditya and Mishra, Abhijit and Senthamilselvan, Nivvedan and Bhattacharyya, Pushpak},
  booktitle={Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={36--41},
  year={2014}
}

@inproceedings{barrett2016weakly,
  title={Weakly supervised part-of-speech tagging using eye-tracking data},
  author={Barrett, Maria and Bingel, Joachim and Keller, Frank and S{\o}gaard, Anders},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={579--584},
  year={2016}
}

@article{hollenstein2018zuco,
  title={ZuCo, a simultaneous EEG and eye-tracking resource for natural sentence reading},
  author={Hollenstein, Nora and Rotsztejn, Jonathan and Troendle, Marius and Pedroni, Andreas and Zhang, Ce and Langer, Nicolas},
  journal={Scientific data},
  volume={5},
  number={1},
  pages={1--13},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{tokunaga2017eye,
  title={An Eye-tracking Study of Named Entity Annotation.},
  author={Tokunaga, Takenobu and Nishikawa, Hitoshi and Iwakura, Tomoya},
  booktitle={RANLP},
  pages={758--764},
  year={2017}
}

@inproceedings{mitsuda2013detecting,
  title={Detecting missing annotation disagreement using eye gaze information},
  author={Mitsuda, Koh and Iida, Ryu and Tokunaga, Takenobu},
  booktitle={Proceedings of the 11th Workshop on Asian Language Resources},
  pages={19--26},
  year={2013}
}

@article{blankertz2009predicting,
  title={Predicting BCI performance to study BCI illiteracy},
  author={Blankertz, Benjamin and Sanelli, Claudia and Halder, Sebastian and Hammer, E and K{\"u}bler, Andrea and M{\"u}ller, Klaus-Robert and Curio, Gabriel and Dickhaus, Thorsten},
  journal={BMC Neuroscience},
  volume={10},
  number={Suppl 1},
  pages={P84},
  year={2009}
}

@article{vidaurre2010towards,
  title={Towards a cure for BCI illiteracy},
  author={Vidaurre, Carmen and Blankertz, Benjamin},
  journal={Brain topography},
  volume={23},
  number={2},
  pages={194--198},
  year={2010},
  publisher={Springer}
}

@book{poldrack2018new,
  title={The new mind readers: What neuroimaging can and cannot reveal about our thoughts},
  author={Poldrack, Russell A},
  year={2018},
  publisher={Princeton University Press}
}

@book{bojko2013eye,
  title={Eye tracking the user experience: A practical guide to research},
  author={Bojko, Aga},
  year={2013},
  publisher={Rosenfeld Media}
}

@incollection{mishra2018applications,
  title={Applications of Eye Tracking in Language Processing and Other Areas},
  author={Mishra, Abhijit and Bhattacharyya, Pushpak},
  booktitle={Cognitively Inspired Natural Language Processing},
  pages={23--46},
  year={2018},
  publisher={Springer}
}

@inproceedings{garkavijs2012glase,
author = {Garkavijs, Viktors and Toshima, Mayumi and Kando, Noriko},
title = {GLASE 0.1: Eyes Tell More than Mice},
year = {2012},
isbn = {9781450314725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.lib.utexas.edu/10.1145/2348283.2348481},
doi = {10.1145/2348283.2348481},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1085–1086},
numpages = {2},
keywords = {image search, within session learning, gaze-based interaction},
location = {Portland, Oregon, USA},
series = {SIGIR ’12}
}
  


@inproceedings{hansen2020factuality,
author = {Hansen, Christian and Hansen, Casper and Simonsen, Jakob Grue and Larsen, Birger and Alstrup, Stephen and Lioma, Christina},
title = {Factuality Checking in News Headlines with Eye Tracking},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.lib.utexas.edu/10.1145/3397271.3401221},
doi = {10.1145/3397271.3401221},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2013–2016},
numpages = {4},
keywords = {fake news, factuality checking, eye tracking},
location = {Virtual Event, China},
series = {SIGIR ’20}
}
  


@inproceedings{palani2020eye,
author = {Palani, Srishti and Fourney, Adam and Williams, Shane and Larson, Kevin and Spiridonova, Irina and Morris, Meredith Ringel},
title = {An Eye Tracking Study of Web Search by People With and Without Dyslexia},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.lib.utexas.edu/10.1145/3397271.3401103},
doi = {10.1145/3397271.3401103},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {729–738},
numpages = {10},
keywords = {information retrieval, web search, eye tracking, dyslexia},
location = {Virtual Event, China},
series = {SIGIR ’20}
}
  


@article{belkin1982ask,
  title={ASK for information retrieval: Part I. Background and theory},
  author={Belkin, Nicholas J and Oddy, Robert N and Brooks, Helen M},
  journal={Journal of documentation},
  year={1982},
  publisher={MCB UP Ltd}
}

@article{brookes1980foundations,
  title={The foundations of information science. Part I. Philosophical aspects},
  author={Brookes, Bertram C},
  journal={Journal of information science},
  volume={2},
  number={3-4},
  pages={125--133},
  year={1980},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{phd-handbook,
  author={School of Information, University of Texas at Austin},
  title = {Doctoral Program Handbook, 2017},
  year = {2017},
  howpublished = "\url{https://www.ischool.utexas.edu/handbooks/doctoral-program-handbook-current.pdf}",
  note = "[Online; accessed 2020-08-05]"
}

@inproceedings{zhai2020interactive-SIGIR-tut,
author = {Zhai, ChengXiang},
title = {Interactive Information Retrieval: Models, Algorithms, and Evaluation},
year = {2020},
isbn = {9781450380164},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.ezproxy.lib.utexas.edu/10.1145/3397271.3401424},
doi = {10.1145/3397271.3401424},
booktitle = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2444–2447},
numpages = {4},
keywords = {user interaction, interactive information retrieval, mathematical models of retrieval, search engines},
location = {Virtual Event, China},
series = {SIGIR ’20}
}
  


@article{kulke2016neural,
  title={Neural differences between covert and overt attention studied using EEG with simultaneous remote eye tracking},
  author={Kulke, Louisa V and Atkinson, Janette and Braddick, Oliver},
  journal={Frontiers in human neuroscience},
  volume={10},
  pages={592},
  year={2016},
  publisher={Frontiers}
}

@book{findlay2003active,
  title={Active vision: The psychology of looking and seeing},
  author={Findlay, John M and Gilchrist, Iain D},
  number={37},
  year={2003},
  publisher={Oxford University Press}
}

@article{klimesch2012alpha,
  title={Alpha-band oscillations, attention, and controlled access to stored information},
  author={Klimesch, Wolfgang},
  journal={Trends in cognitive sciences},
  volume={16},
  number={12},
  pages={606--617},
  year={2012},
  publisher={Elsevier}
}

@article{dien2010separating,
  title={Separating the visual sentence N400 effect from the P400 sequential expectancy effect: Cognitive and neuroanatomical implications},
  author={Dien, Joseph and Michelson, Charles A and Franklin, Michael S},
  journal={Brain Research},
  volume={1355},
  pages={126--140},
  year={2010},
  publisher={Elsevier}
}

@article{dien2009neurocognitive,
  title={The neurocognitive basis of reading single words as seen through early latency ERPs: a model of converging pathways},
  author={Dien, Joseph},
  journal={Biological psychology},
  volume={80},
  number={1},
  pages={10--22},
  year={2009},
  publisher={Elsevier}
}

@book{luck2014introduction,
  title = {An {{Introduction}} to the {{Event}}-{{Related Potential Technique}}},
  author = {Luck, Steven J},
  year = {2014},
  publisher = {{MIT Press}},
  address = {{Cambridge, United States}},
  isbn = {978-0-262-32405-2},
  keywords = {nb-star}
}



@misc{wiki-eeg,
  author={Wikipedia},
  title = {Electroencephalography},
  year = {2020},
  howpublished = "\url{https://en.wikipedia.org/wiki/Electroencephalography}",
  note = "[Online; accessed 2020-06-13]"
}

@book{cohen2014analyzing,
  title={Analyzing neural time series data: theory and practice},
  author={Cohen, Mike X},
  year={2014},
  publisher={MIT press}
}

@article{onorati2013characterization,
  title={Characterization of affective states by pupillary dynamics and autonomic correlates},
  author={Onorati, Francesco and Barbieri, Riccardo and Mauri, Maurizio and Russo, Vincenzo and Mainardi, Luca},
  journal={Frontiers in neuroengineering},
  volume={6},
  pages={9},
  year={2013},
  publisher={Frontiers}
}


@article{kahneman1966pupil,
  title={Pupil diameter and load on memory},
  author={Kahneman, Daniel and Beatty, Jackson},
  journal={Science},
  volume={154},
  number={3756},
  pages={1583--1585},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

@article{krugman1964some,
  title={Some applications of pupil measurement},
  author={Krugman, Herbert E},
  journal={Journal of Marketing Research},
  volume={1},
  number={4},
  pages={15--19},
  year={1964},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{preuschoff2011pupil,
  title={Pupil dilation signals surprise: Evidence for noradrenaline’s role in decision making},
  author={Preuschoff, Kerstin and t Hart, Bernard Marius and Einhauser, Wolfgang},
  journal={Frontiers in neuroscience},
  volume={5},
  pages={115},
  year={2011},
  publisher={Frontiers}
}

@misc{eeg-10-20-a,
  author={Asanagi},
  title = {Electrode locations of International 10-20 system for EEG (electroencephalography) recording},
  year = {2020},
  howpublished = "\url{https://commons.wikimedia.org/wiki/File:21-electrodes-of-International-10-20-system-for-EEG.svg}",
  note = "[Online; accessed 2020-06-13]"
}

@misc{eeg-10-20-b,
  author={Oxley, Brylie},
  title = {International 10-20 system for EEG electrode placement, showing modified combinatorial nomenclature},
  year = {2020},
  howpublished = "\url{https://commons.wikimedia.org/wiki/File:International-10-20-system-for-EEG-MCN.svg}",
  note = "[Online; accessed 2020-06-13]"
}

@misc{brain-lobes,
  author={MacLeod, Victoria},
  title = {Dementia Friends, Alzheimer Scotland},
  year = {2020},
  howpublished = "\url{https://www.argyll-bute.gov.uk/moderngov/documents/s102606/Dementia\%20Friends.pdf}",
  note = "[Online; accessed 2020-06-13]"
}

@inproceedings{syed2020improving,
  title={Improving Learning Outcomes with Gaze Tracking and Automatic Question Generation},
  author={Syed, Rohail and Collins-Thompson, Kevyn and Bennett, Paul N and Teng, Mengqiu and Williams, Shane and Tay, Dr Wendy W and Iqbal, Shamsi},
  booktitle={The Web Conference (WWW)},
  year={2020}
}

@inproceedings{syed2020improving1,
author = {Syed, Rohail and Collins-Thompson, Kevyn and Bennett, Paul N. and Teng, Mengqiu and Williams, Shane and Tay, Dr. Wendy W. and Iqbal, Shamsi},
title = {Improving Learning Outcomes with Gaze Tracking and Automatic Question Generation},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380240},
doi = {10.1145/3366423.3380240},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1693--1703},
numpages = {11},
keywords = {Personalization, Gaze tracking, Lab study, Education/Learning, User modeling},
location = {Taipei, Taiwan},
series = {WWW ’20}
}


@article{saracevic1999information,
  title = {Information Science},
  author = {Saracevic, Tefko},
  year = {1999},
  volume = {50},
  pages = {1051--1063},
  doi = {10.1002/(SICI)1097-4571(1999)50:12<1051::AID-ASI2>3.0.CO;2-Z},
  journal = {Journal of the American Society for Information Science},
}



@article{rayner1998eye,
  title={Eye movements in reading and information processing: 20 years of research.},
  author={Rayner, Keith},
  journal={Psychological bulletin},
  volume={124},
  number={3},
  pages={372},
  year={1998},
  publisher={American Psychological Association}
}

@article{leacock1998combining,
  title={Combining local context and WordNet similarity for word sense identification},
  author={Leacock, Claudia and Chodorow, Martin},
  journal={WordNet: An electronic lexical database},
  volume={49},
  number={2},
  pages={265--283},
  year={1998}
}

@book{just1987psychology,
  title={The psychology of reading and language comprehension.},
  author={Just, Marcel Adam and Carpenter, Patricia Ann},
  year={1987},
  publisher={Allyn \& Bacon}
}

@book{marchionini1995information,
  title = {Information {{Seeking}} in {{Electronic Environments}}},
  author = {Marchionini, Gary},
  year = {1995},
  month = mar,
  publisher = {{Cambridge University Press}},
  isbn = {978-0-521-58674-0},
  language = {en}
}



@article{gwizdka2010distribution,
  title={Distribution of cognitive load in web search},
  author={Gwizdka, Jacek},
  journal={Journal of the American Society for Information Science and Technology},
  volume={61},
  number={11},
  pages={2167--2187},
  year={2010},
  publisher={Wiley Online Library}
}

@article{gwizdka2019introduction,
  title = {Introduction to the Special Issue on Neuro-Information Science},
  author = {Gwizdka, Jacek and Moshfeghi, Yashar and Wilson, Max L.},
  year = {2019},
  volume={70},
  number={9},
  pages={911--916},
  issn = {2330-1643},
  copyright = {\textcopyright{} 2019 ASIS\&T},
  journal = {Journal of the Association for Information Science and Technology},
  language = {en},
  number = {0}
}


@article{landau2020mind,
author = {Landau, Ofir and Puzis, Rami and Nissim, Nir},
title = {Mind Your Mind: EEG-Based Brain-Computer Interfaces and Their Security in Cyber Space},
year = {2020},
issue-date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372043},
doi = {10.1145/3372043},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {17},
numpages = {38},
keywords = {security, EEG, privacy, attack, detection, Brain-computer interface, cyber space}
}
  


@book{holmqvist2011eye,
  title = {Eye {{Tracking}}: {{A}} Comprehensive Guide to Methods and Measures},
  shorttitle = {Eye {{Tracking}}},
  author = {Holmqvist, Kenneth and Nystr{\"o}m, Marcus and Andersson, Richard and Dewhurst, Richard and Jarodzka, Halszka and van de Weijer, Joost},
  year = {2011},
  month = sep,
  publisher = {{Oxford University Press}},
  isbn = {978-0-19-162542-8},
  language = {en}
}


@article{just1980theory,
  title={A theory of reading: From eye fixations to comprehension.},
  author={Just, Marcel A and Carpenter, Patricia A},
  journal={Psychological review},
  volume={87},
  number={4},
  pages={329},
  year={1980},
  publisher={American Psychological Association}
}

@article{huang2013relevance,
  title = {Relevance: {{An}} Improved Framework for Explicating the Notion},
  shorttitle = {Relevance},
  author = {Huang, Xiaoli and Soergel, Dagobert},
  year = {2013},
  volume = {64},
  pages = {18--35},
  issn = {1532-2890},
  doi = {10.1002/asi.22811},
  copyright = {\textcopyright{} 2012 ASIS\&T},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {-tablet,aboutness,nb-star,relevance},
  language = {en},
  number = {1}
}



@article{saracevic2016notion,
  title = {The {{Notion}} of {{Relevance}} in {{Information Science}}: {{Everybody}} Knows What Relevance Is. {{But}}, What Is It Really?},
  shorttitle = {The {{Notion}} of {{Relevance}} in {{Information Science}}},
  author = {Saracevic, Tefko},
  year = {2016},
  journal = {Synthesis Lectures on Information Concepts, Retrieval, and Services}
}



@article{marchionini2006toward,
  title={Toward human-computer information retrieval},
  author={Marchionini, Gary},
  journal={Bulletin of the American Society for Information Science and Technology},
  volume={32},
  number={5},
  pages={20--22},
  year={2006},
  publisher={Wiley Online Library}
}

@article{wilson1999models,
  title = {Models in Information Behaviour Research},
  author = {Wilson, T. D},
  year = {1999},
  volume = {55},
  pages = {249--270},
  journal = {Journal of Documentation},
  keywords = {nb-star,Problem,retrieval;Research,solving;Information},
  number = {3}
}



@incollection{white-2016-iwss-learning,
  title = {Learning and Use},
  booktitle = {Interactions with Search Systems},
  author = {White, Ryen},
  year = {2016},
  pages = {231--248},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781139525305.010},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\Z58VT3B2\\2016-White-Learning and use.pdf},
  place = {Cambridge}
}

@book{white-2016,
  title = {Interactions with Search Systems},
  author = {White, Ryen},
  year = {2016},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9781139525305},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\IGES9AUE\\White - 2016 - Interactions with search systems.pdf},
  keywords = {nb-star},
  place = {Cambridge}
}

@article{ruthven2008interactive,
  title={Interactive information retrieval},
  author={Ruthven, Ian},
  journal={Annual review of information science and technology},
  volume={42},
  number={1},
  pages={43--91},
  year={2008},
  publisher={Wiley Online Library}
}


@article{borlund2013interactive,
  title = {Interactive {{Information Retrieval}}: {{An Introduction}}},
  shorttitle = {Interactive {{Information Retrieval}}},
  author = {Borlund, Pia},
  year = {2013},
  month = sep,
  volume = {1},
  pages = {12--32},
  doi = {10.1633/JISTAP.2013.1.3.2},
  journal = {Journal of Information Science Theory and Practice},
  language = {en},
  number = {3}
}






@misc{dillon2006no-more-is-models,
  author={Dillon, Andrew},
  title = {No more information seeking models please},
  year = {2006},
  howpublished = "\url{https://adillon.ischool.utexas.edu/2006/10/03/no-more-information-seeking-models-please/}",
  note = "[Online; accessed 2020-05-28]"
}

@misc{dillon2020personal-comm,
  author = "Dillon, Andrew",
  date = "2020-04-29",
  year = "2020",
  howpublished = "Personal communication",
  note = "[dated 2020-04-29]"
}

@misc{url-rieh-homepage,
  author={Rieh, Soo Young},
  title = {Research Area 1: Searching as Learning},
  year = {2020},
  howpublished = "\url{https://rieh.ischool.utexas.edu/research}",
  note = "[Online; accessed 2020-04-19]"
}



@article{spink1997study,
  title = {Study of Interactive Feedback during Mediated Information Retrieval.},
  author = {Spink, Amanda},
  year = {1997},
  journal = {Journal of the American Society for Information Science}
}






@book{white2016interactions,
  title={Interactions with search systems},
  author={White, Ryen},
  year={2016},
  publisher={Cambridge University Press}
}

@inproceedings{huurdeman2014multistage,
  title={From multistage information-seeking models to multistage search systems},
  author={Huurdeman, Hugo C and Kamps, Jaap},
  booktitle={Proceedings of the 5th Information Interaction in Context Symposium},
  pages={145--154},
  year={2014}
}

@book{manning2008introduction,
  title={Introduction to information retrieval},
  author={Manning, Christopher D and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year={2008},
  publisher={Cambridge university press}
}



@article{desimone1995neural,
  title={Neural mechanisms of selective visual attention},
  author={Desimone, Robert and Duncan, John},
  journal={Annual review of neuroscience},
  volume={18},
  number={1},
  pages={193--222},
  year={1995},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@inproceedings{beymer2007eye,
  title={An eye tracking study of how pictures influence online reading},
  author={Beymer, David and Orton, Peter Z and Russell, Daniel M},
  booktitle={IFIP Conference on Human-Computer Interaction},
  pages={456--460},
  year={2007},
  organization={Springer}
}

@article{lorigo2006influence,
  title={The influence of task and gender on search and evaluation behavior using Google},
  author={Lorigo, Lori and Pan, Bing and Hembrooke, Helene and Joachims, Thorsten and Granka, Laura and Gay, Geri},
  journal={Information Processing \& Management},
  volume={42},
  number={4},
  pages={1123--1131},
  year={2006},
  publisher={Elsevier}
}

@inproceedings{goldberg2002eye,
  title={Eye tracking in web search tasks: design implications},
  author={Goldberg, Joseph H and Stimson, Mark J and Lewenstein, Marion and Scott, Neil and Wichansky, Anna M},
  booktitle={Proceedings of the 2002 symposium on Eye tracking research \& applications},
  pages={51--58},
  year={2002}
}

@inproceedings{pan2004determinants,
  title={The determinants of web page viewing behavior: an eye-tracking study},
  author={Pan, Bing and Hembrooke, Helene A and Gay, Geri K and Granka, Laura A and Feusner, Matthew K and Newman, Jill K},
  booktitle={Proceedings of the 2004 symposium on Eye tracking research \& applications},
  pages={147--154},
  year={2004}
}

@inproceedings{josephson2002visual,
  title={Visual attention to repeated internet images: testing the scanpath theory on the world wide web},
  author={Josephson, Sheree and Holmes, Michael E},
  booktitle={Proceedings of the 2002 symposium on Eye tracking research \& applications},
  pages={43--49},
  year={2002}
}

@incollection{yarbus1967eye,
  title={Eye movements during perception of complex objects},
  author={Yarbus, Alfred L},
  booktitle={Eye movements and vision},
  pages={171--211},
  year={1967},
  publisher={Springer}
}

@article{simon1956rational,
  title={Rational choice and the structure of the environment.},
  author={Simon, Herbert A},
  journal={Psychological review},
  volume={63},
  number={2},
  pages={129},
  year={1956},
  publisher={American Psychological Association}
}

@inproceedings{eickhoff2014lessons,
  title={Lessons from the journey: a query log analysis of within-session learning},
  author={Eickhoff, Carsten and Teevan, Jaime and White, Ryen and Dumais, Susan},
  booktitle={Proceedings of the 7th ACM international conference on Web search and data mining},
  pages={223--232},
  year={2014}
}

@article{li2008faceted,
  title={A faceted approach to conceptualizing tasks in information seeking},
  author={Li, Yuelin and Belkin, Nicholas J},
  journal={Information Processing \& Management},
  volume={44},
  number={6},
  pages={1822--1837},
  year={2008},
  publisher={Elsevier}
}

@article{liu2010analysis,
  title={Analysis and evaluation of query reformulations in different task types},
  author={Liu, Chang and Gwizdka, Jacek and Liu, Jingjing and Xu, Tao and Belkin, Nicholas J},
  journal={Proceedings of the American Society for Information Science and Technology},
  volume={47},
  number={1},
  pages={1--9},
  year={2010},
  publisher={Wiley Online Library}
}

@inproceedings{boldi2009dango,
  title={From" dango" to" japanese cakes": Query reformulation models and patterns},
  author={Boldi, Paolo and Bonchi, Francesco and Castillo, Carlos and Vigna, Sebastiano},
  booktitle={2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology},
  volume={1},
  pages={183--190},
  year={2009},
  organization={IEEE}
}

@article{broder2002taxonomy,
  title = {A Taxonomy of Web Search},
  author = {Broder, Andrei},
  year = {2002},
  month = sep,
  volume = {36},
  pages = {3--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0163-5840},
  doi = {10.1145/792550.792552},
  issue-date = {September 2002},
  journal = {SIGIR Forum},
  keywords = {nb-star},
  number = {2},
  numpages = {8}
}






@article{yue2018optimizing,
author = {Wang, Yue and Yin, Dawei and Jie, Luo and Wang, Pengyuan and Yamada, Makoto and Chang, Yi and Mei, Qiaozhu},
title = {Optimizing Whole-Page Presentation for Web Search},
year = {2018},
issue-date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {1559-1131},
url = {https://doi.org/10.1145/3204461},
doi = {10.1145/3204461},
journal = {ACM Trans. Web},
month = jul,
articleno = {Article 19},
numpages = {25},
keywords = {user satisfaction, Whole-page optimization}
}
  


@inproceedings{bhattacharya2019measuring,
  title={Measuring learning during search: differences in interactions, eye-gaze, and semantic similarity to expert knowledge},
  author={Bhattacharya, Nilavra and Gwizdka, Jacek},
  booktitle={Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
  pages={63--71},
  year={2019}
}

@inproceedings{yu2018predicting,
  title={Predicting user knowledge gain in informational search sessions},
  author={Yu, Ran and Gadiraju, Ujwal and Holtz, Peter and Rokicki, Markus and Kemkes, Philipp and Dietze, Stefan},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={75--84},
  year={2018}
}

@inproceedings{ghosh2018searching,
  title={Searching as learning: Exploring search behavior and learning outcomes in learning-related tasks},
  author={Ghosh, Souvick and Rath, Manasa and Shah, Chirag},
  booktitle={Proceedings of the 2018 Conference on Human Information Interaction \& Retrieval},
  pages={22--31},
  year={2018}
}

@inproceedings{gadiraju2018analyzing,
  title={Analyzing knowledge gain of users in informational search sessions on the web},
  author={Gadiraju, Ujwal and Yu, Ran and Dietze, Stefan and Holtz, Peter},
  booktitle={Proceedings of the 2018 Conference on Human Information Interaction \& Retrieval},
  pages={2--11},
  year={2018}
}

@article{pollatsek2006tests,
  title={Tests of the EZ Reader model: Exploring the interface between cognition and eye-movement control},
  author={Pollatsek, Alexander and Reichle, Erik D and Rayner, Keith},
  journal={Cognitive Psychology},
  volume={52},
  number={1},
  pages={1--56},
  year={2006},
  publisher={Elsevier}
}



@article{cole2013inferring,
  title={Inferring user knowledge level from eye movement patterns},
  author={Cole, Michael J and Gwizdka, Jacek and Liu, Chang and Belkin, Nicholas J and Zhang, Xiangmin},
  journal={Information Processing \& Management},
  volume={49},
  number={5},
  pages={1075--1091},
  year={2013},
  publisher={Elsevier}
}

@article{meppelink2015exploring,
  title={Exploring the role of health literacy on attention to and recall of text-illustrated health information: an eye-tracking study},
  author={Meppelink, Corine S and Bol, Nadine},
  journal={Computers in Human Behavior},
  volume={48},
  pages={87--93},
  year={2015},
  publisher={Elsevier}
}

@incollection{groner1984looking,
  title={Looking at faces: Local and global aspects of scanpaths},
  author={Groner, Rudolf and Walder, Franziska and Groner, Marina},
  booktitle={Advances in psychology},
  volume={22},
  pages={523--533},
  year={1984},
  publisher={Elsevier}
}

@article{fuhr2008probability,
  title={A probability ranking principle for interactive information retrieval},
  author={Fuhr, Norbert},
  journal={Information Retrieval},
  volume={11},
  number={3},
  pages={251--265},
  year={2008},
  publisher={Springer}
}

@inproceedings{salojarvi2005inferring,
  title={Inferring relevance from eye movements: Feature extraction},
  author={Saloj{\"a}rvi, Jarkko and Puolam{\"a}ki, Kai and Simola, Jaana and Kovanen, Lauri and Kojo, Ilpo and Kaski, Samuel},
  booktitle={Workshop at NIPS 2005, in Whistler, BC, Canada, on December 10, 2005.},
  pages={45},
  year={2005}
}

@article{shneiderman1997clarifying,
  title={Clarifying search: A user-interface framework for text searches},
  author={Shneiderman, Ben and Byrd, Don and Croft, W Bruce},
  year={1997},
  publisher={Corporation for National Research Initiatives}
}

@article{sajda2010blink,
  title={In a blink of an eye and a switch of a transistor: cortically coupled computer vision},
  author={Sajda, Paul and Pohlmeyer, Eric and Wang, Jun and Parra, Lucas C and Christoforou, Christoforos and Dmochowski, Jacek and Hanna, Barbara and Bahlmann, Claus and Singh, Maneesh Kumar and Chang, Shih-Fu},
  journal={Proceedings of the IEEE},
  volume={98},
  number={3},
  pages={462--478},
  year={2010},
  publisher={IEEE}
}

@phdthesis{djordjevic2006user,
  title={User relevance feedback, search and retrieval of visual content},
  author={Djordjevic, Divna},
  year={2006},
  school={University of London}
}

@article{gupta1991theory,
  title={Theory of T-norms and fuzzy inference methods},
  author={Gupta, Madan M and Qi, J},
  journal={Fuzzy sets and systems},
  volume={40},
  number={3},
  pages={431--450},
  year={1991},
  publisher={North-Holland}
}






@book{voorhees2005trec,
  title={TREC: Experiment and evaluation in information retrieval},
  author={Voorhees and Harman},
  volume={1},
  year={2005},
  publisher={MIT press Cambridge}
}

@article{spark1975report,
  title={Report on the need for and provision of an'ideal'information retrieval test collection},
  author={Spark Jones, K and Van Rijsbergen, C. J.},
  journal={Computer Laboratory},
  year={1975},
  publisher={Univ. Cambridge}
}

@inproceedings{voorhees2018,
 author = {Voorhees},
 title = {On Building Fair and Reusable Test Collections Using Bandit Techniques},
 booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '18},
 year = {2018},
 isbn = {978-1-4503-6014-2},
 location = {Torino, Italy},
 pages = {407--416},
 numpages = {10},
 doi = {10.1145/3269206.3271766},
 acmid = {3271766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-arm bandit methods, test collection building},
} 


@inproceedings{sanderson-information-2005,
 author = {Sanderson, Mark and Zobel, Justin},
 title = {Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability},
 booktitle = {Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '05},
 year = {2005},
 isbn = {1-59593-034-5},
 location = {Salvador, Brazil},
 pages = {162--169},
 numpages = {8},
 doi = {10.1145/1076034.1076064},
 acmid = {1076064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mean average precision, precision at 10, significance tests},
} 

@article{vijayanarasimhan2014large,
  title={Large-scale live active learning: Training object detectors with crawled data and crowds},
  author={Vijayanarasimhan, Sudheendra and Grauman, Kristen},
  journal={International Journal of Computer Vision},
  volume={108},
  number={1-2},
  pages={97--114},
  year={2014},
  publisher={Springer}
}

@inproceedings{petrovic2010streaming,
  title={Streaming first story detection with application to twitter},
  author={Petrovi{\'c}, Sa{\v{s}}a and Osborne, Miles and Lavrenko, Victor},
  booktitle={Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics},
  pages={181--189},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{kutlu2018rank,
  title={When Rank Order Isn't Enough: New Statistical-Significance-Aware Correlation Measures},
  author={Kutlu, Mucahid and Elsayed, Tamer and Hasanain, Maram and Lease, Matthew},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={397--406},
  year={2018},
  organization={ACM}
}


@inproceedings{li2017active,
  title={Active Sampling for Large-scale Information Retrieval Evaluation},
  author={Li and Kanoulas, Evangelos},
  booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages={49--58},
  year={2017},
  organization={ACM}
}

@inproceedings{carterette2018offline,
  title={Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback},
  author={Carterette, Ben and Chandar, Praveen},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={705--714},
  year={2018},
  organization={ACM}
}

@inproceedings{cormack2018beyond,
  title={Beyond pooling},
  author={Cormack, Gordon V and Grossman, Maura R},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={1169--1172},
  year={2018},
  organization={ACM}
}


@inproceedings{scholer2013effect,
  title={The effect of threshold priming and need for cognition on relevance calibration and assessment},
  author={Scholer, Falk and Kelly, Diane and Wu, Wan-Ching and Lee, Hanseul S and Webber, William},
  booktitle={Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},
  pages={623--632},
  year={2013},
  organization={ACM}
}

@inproceedings{blaszczynski2013extending,
  title={Extending bagging for imbalanced data},
  author={B{\l}aszczy{\'n}ski, Jerzy and Stefanowski, Jerzy and Idkowiak, {\L}ukasz},
  booktitle={Proceedings of the 8th International Conference on Computer Recognition Systems CORES 2013},
  pages={269--278},
  year={2013},
  organization={Springer}
}

@inproceedings{carterette2009if,
  title={If I had a million queries},
  author={Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A and Allan, James},
  booktitle={European conference on information retrieval},
  pages={288--300},
  year={2009},
  organization={Springer}
}

@article{google-guidelines-2016,
  title={Search Quality Rating Guidelines},
  author={Google},
  journal={Inside Search: How Search Works},
  month={March 28},
  year={2016},
  note={\url{www.google.com/insidesearch/}}
}

@article{sanderson2010test,
  title={Test collection based evaluation of information retrieval systems},
  author={Sanderson, Mark and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={4},
  number={4},
  pages={247--375},
  year={2010},
  publisher={Now Publishers, Inc.}
}

@inproceedings{Lease16-medir,
  title = {{Systematic Review is e-Discovery in Doctor's Clothing}},
  author = {Matthew Lease and Gordon V.\ Cormack and Nguyen, An Thanh and Thomas A.\ Trikalinos and Byron C.\ Wallace},
  booktitle = {Proceedings of the Medical Information Retrieval (MedIR) Workshop at the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year = {2016}
}

@conference{Voorhees06,
  author={Voorhees},
  title={{Overview of the TREC 2005 robust retrieval track}},
  booktitle = {{Proc.\ TREC}},
  year={2006}
}

@inproceedings{Joachims02,
 author = {Joachims, Thorsten},
 title = {Optimizing search engines using clickthrough data},
 booktitle = {{Proc.\ KDD}},
 year = {2002},
 pages = {133--142}
 }

@article{journals/tois/GuiverMR09,
  author = {Guiver, John and Mizzaro, Stefano and Robertson, Stephen},
  date = {2010-02-25},
  journal = {ACM Trans. Inf. Syst.},
  keywords = {dblp},
  title = {A few good topics: Experiments in topic set reduction for retrieval evaluation.},
  year = 2009
}

@inproceedings{MoffatWZ07,
  title={Strategic system comparisons via targeted relevance judgments},
  author={Moffat, Alistair and Webber, William and Zobel, Justin},
  booktitle={Proc.\ SIGIR},
  pages={375--382},
  year={2007}
}

@article{Sakai08,
  title={{On information retrieval metrics designed for evaluation with incomplete relevance assessments}},
  author={Sakai, T. and Kando, N.},
  journal={Information Retrieval},
  volume={11},
  number={5},
  pages={447--470},
  year={2008},
  publisher={Springer}
}

@article{Alonso09,
    author = {Alonso, Omar and Rose, Daniel E. and Stewart, Benjamin},
    journal = {ACM SIGIR Forum},
    number = {2},
    pages = {9--15},
    publisher = {ACM},
    title = {Crowdsourcing for relevance evaluation},
    volume = {42},
    year = {2008}
}

@inproceedings{soboroff2013building,
  title={Building Test Sollections (without running a community evaluation)},
  author={Soboroff},
  booktitle={Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval},
  pages={1132--1132},
  year={2013},
  note={\url{https://isoboroff.github.io/Test-Colls-Tutorial/Tutorial-slides/}}
}

@article{zuccon2013crowdsourcing,
  title={Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems},
  author={Zuccon, Guido and Leelanupab, Teerapong and Whiting, Stewart and Yilmaz, Emine and Jose, Joemon M and Azzopardi, Leif},
  journal={Information retrieval},
  volume={16},
  number={2},
  pages={267--305},
  year={2013},
  publisher={Springer}
}

@inproceedings{carterette2006minimal,
  title={Minimal test collections for retrieval evaluation},
  author={Carterette, Ben and Allan, James and Sitaraman, Ramesh},
  booktitle={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={268--275},
  year={2006},
  organization={ACM}
}

@inproceedings{voorhees1999overview,
  title={Overview of the Seventh Text retrieval Conference (TREC-7)},
  author={Voorhees, E},
  booktitle={Proceedings of the Seventh Text REtrieval Conference},
  year={1999},
  organization={NIST Special Publication}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016},
  organization={ACM}
}

@article{fan2008liblinear,
  title={LIBLINEAR: A library for large linear classification},
  author={Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  journal={Journal of machine learning research},
  volume={9},
  number={Aug},
  pages={1871--1874},
  year={2008}
}

@article{tonon2015pooling,
  title={Pooling-based continuous evaluation of information retrieval systems},
  author={Tonon, Alberto and Demartini, Gianluca and Cudr{\'e}-Mauroux, Philippe},
  journal={Information Retrieval Journal},
  volume={18},
  number={5},
  pages={445--472},
  year={2015},
  publisher={Springer}
}


@inproceedings{sanderson-forming-2004,
	address = {Sheffield, United Kingdom},
	title = {Forming test collections with no system pooling},
	isbn = {978-1-58113-881-8},
	doi = {10.1145/1008992.1009001},
	abstract = {Forming test collection relevance judgments from the pooled output of multiple retrieval systems has become the standard process for creating resources such as the TREC, CLEF, and NTCIR test collections. This paper presents a series of experiments examining three different ways of building test collections where no system pooling is used. First, a collection formation technique combining manual feedback and multiple systems is adapted to work with a single retrieval system. Second, an existing method based on pooling the output of multiple manual searches is re-examined: testing a wider range of searchers and retrieval systems than has been examined before. Third, a new approach is explored where the ranked output of a single automatic search on a single retrieval system is assessed for relevance: no pooling whatsoever. Using established techniques for evaluating the quality of relevance judgments, in all three cases, test collections are formed that are as good as TREC.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 27th annual international conference on {Research} and development in information retrieval  - {SIGIR} '04},
	publisher = {ACM Press},
	author = {Sanderson, Mark and Joho, Hideo},
	year = {2004},
	keywords = {document\-selection, no\-pooling},
	pages = {33}
}

@inproceedings{cormack-scalability-2016,
	address = {Indianapolis, Indiana, USA},
	title = {Scalability of {Continuous} {Active} {Learning} for {Reliable} {High}-{Recall} {Text} {Classification}},
	isbn = {978-1-4503-4073-1},
	url = {http://dl.acm.org/citation.cfm?doid=2983323.2983776},
	doi = {10.1145/2983323.2983776},
	abstract = {For ﬁnite document collections, continuous active learning (“CAL”) has been observed to achieve high recall with high probability, at a labeling cost asymptotically proportional to the number of relevant documents. As the size of the collection increases, the number of relevant documents typically increases as well, thereby limiting the applicability of CAL to low-prevalence high-stakes classes, such as evidence in legal proceedings, or security threats, where human eﬀort proportional to the number of relevant documents is justiﬁed. We present a scalable version of CAL (“S-CAL”) that requires O(log N ) labeling eﬀort and O(N log N ) computational eﬀort—where N is the number of unlabeled training examples—to construct a classiﬁer whose eﬀectiveness for a given labeling cost compares favorably with previously reported methods. At the same time, S-CAL oﬀers calibrated estimates of class prevalence, recall, and precision, facilitating both threshold setting and determination of the adequacy of the classiﬁer.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 25th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management} - {CIKM} '16},
	publisher = {ACM Press},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2016},
	keywords = {active\-learning},
	pages = {1039--1048}
}


@inproceedings{maas-EtAl:2011:ACL-HLT2011, author = {Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher}, title = {Learning Word Vectors for Sentiment Analysis}, booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies}, month = {June}, year = {2011}, address = {Portland, Oregon, USA}, publisher = {Association for Computational Linguistics}, pages = {142--150}, url = {http://www.aclweb.org/anthology/P11-1015} }

@techreport{pavlu2007practical,
  title={A practical sampling strategy for efficient retrieval evaluation},
  author={Pavlu, V and Aslam, J},
  year={2007},
  institution={College of Computer and Information Science, Northeastern University}
}

 @inproceedings{wallace2011class,
  title={Class imbalance, redux},
  author={Wallace, Byron C and Small, Kevin and Brodley, Carla E and Trikalinos, Thomas A},
  booktitle={Data Mining (ICDM), 2011 IEEE 11th International Conference on},
  pages={754--763},
  year={2011}
}

@inproceedings{aslam2006statistical,
  title={A statistical method for system evaluation using incomplete judgments},
  author={Aslam, Javed A and Pavlu, Virgil and Yilmaz, Emine},
  booktitle={Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={541--548},
  year={2006},
  organization={ACM}
}

@article{yilmaz2008estimating,
  title={Estimating average precision when judgments are incomplete},
  author={Yilmaz, Emine and Aslam, Javed A},
  journal={Knowledge and Information Systems},
  volume={16},
  number={2},
  pages={173--211},
  year={2008},
  publisher={Springer}
}

@inproceedings{yilmaz2006estimating,
  title={Estimating average precision with incomplete and imperfect judgments},
  author={Yilmaz, Emine and Aslam, Javed A},
  booktitle={Proceedings of the 15th ACM international conference on Information and knowledge management},
  pages={102--111},
  year={2006},
  organization={ACM}
}

@inproceedings{aslam2007inferring,
  title={Inferring document relevance from incomplete information},
  author={Aslam, Javed A and Yilmaz, Emine},
  booktitle={Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
  pages={633--642},
  year={2007},
  organization={ACM}
}

@inproceedings{soboroff2006dynamic,
  title={Dynamic test collections: measuring search effectiveness on the live web}, 
  author={Soboroff, Ian},
  booktitle={Proc.\ SIGIR},
  pages={276--283},
  year={2006}
}

@inproceedings{denis1998pac,
  title={PAC learning from positive statistical queries},
  author={Denis, Fran{\c{c}}ois},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={112--126},
  year={1998},
  organization={Springer}
}

@inproceedings{oard2004building,
  title={Building an information retrieval test collection for spontaneous conversational speech},
  author={Oard, Douglas W and Soergel, Dagobert and Doermann, David and Huang, Xiaoli and Murray, G Craig and Wang, Jianqiang and Ramabhadran, Bhuvana and Franz, Martin and Gustman, Samuel and Mayfield, James and others},
  booktitle={Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={41--48},
  year={2004},
  organization={ACM}
}

@article{rocchio1971relevance,
  title={Relevance feedback in information retrieval},
  author={Rocchio, Joseph John},
  year={1971},
  publisher={Prentice-Hall, Englewood Cliffs NJ}
}

@article{settles2012active,
  title={Active learning},
  author={Settles, Burr},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={6},
  number={1},
  pages={1--114},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{cormack2014evaluation,
  title={Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
  author={Cormack, Gordon V and Grossman, Maura R},
  booktitle={Proceedings of the 37th international ACM SIGIR conference on Research \& development in information retrieval},
  pages={153--162},
  year={2014},
  organization={ACM}
}

@inproceedings{al2014qualitative,
  title={A qualitative exploration of secondary assessor relevance judging behavior},
  author={Al-Harbi, Aiman L and Smucker, Mark D},
  booktitle={Proceedings of the 5th Information Interaction in Context Symposium},
  pages={195--204},
  year={2014},
  organization={ACM}
}

@inproceedings{wakeling2016comparison,
  title={A comparison of primary and secondary relevance judgements for real-life topics},
  author={Wakeling, Simon and Halvey, Martin and Villa, Robert and Hasler, Laura},
  booktitle={Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval},
  pages={173--182},
  year={2016},
  organization={ACM}
}

@article{voorhees2000variations,
  title={Variations in relevance judgments and the measurement of retrieval effectiveness},
  author={Voorhees},
  journal={Information processing \& management},
  volume={36},
  number={5},
  pages={697--716},
  year={2000},
  publisher={Elsevier}
}

@misc{ellenvoorheesemail,
    title={Email},
	author={Voorhees},
    note = {Personal communication},
    year={2016}
}

@inproceedings{voorhees2001philosophy,
  title={The philosophy of information retrieval evaluation},
  author={Voorhees},
  booktitle={Workshop of the Cross-Language Evaluation Forum for European Languages},
  pages={355--370},
  year={2001},
  organization={Springer}
}

@inproceedings{smith2013dirt,
  title={Dirt Cheap Web-Scale Parallel Text from the Common Crawl.},
  author={Smith, Jason R and Saint-Amand, Herve and Plamada, Magdalena and Koehn, Philipp and Callison-Burch, Chris and Lopez, Adam},
  booktitle={Proc.\ of the Assoc.\ for Comp.\ Linguistics (ACL)},
  pages={1374--1383},
  year={2013}
}

@INPROCEEDINGS{Voorhees00overviewof,
    author = {Voorhees and Harman},
    title = {Overview of the Eighth Text REtrieval Conference (TREC-8)},
    booktitle = {},
    year = {2000},
    pages = {1--24}
}

@inproceedings{buttcher2006trec,
  title={The TREC 2006 Terabyte Track.},
  author={B{\"u}ttcher, Stefan and Clarke, Charles LA and Soboroff, Ian},
  booktitle={TREC},
  volume={6},
  pages={39},
  year={2006}
}

@inproceedings{Collins-Thompson13,
  author    = {Kevyn Collins{-}Thompson and
               Paul N. Bennett and
               Fernando Diaz and
               Charlie Clarke and
               Ellen },
  title     = {{TREC} 2013 Web Track Overview},
  booktitle = {Proceedings of The Twenty-Second Text REtrieval Conference, {TREC}
               2013, Gaithersburg, Maryland, USA, November 19-22, 2013},
  year      = {2013},
 }

@techreport{collins2015trec,
  title={TREC 2014 web track overview},
  author={Collins-Thompson, Kevyn and Macdonald, Craig and Bennett, Paul and Diaz, Fernando and Voorhees},
  year={2015},
  institution={DTIC Document}
}
@article{lewis2004smart,
  title={SMART stopword list},
  author={Lewis, David D and Yang, Yiming and Rose, Tony G and Li, Fan},
  journal={Journal of Machine Learning Research},
  year={2004}
}

@inproceedings{krovetz1993viewing,
  title={Viewing morphology as an inference process},
  author={Krovetz, Robert},
  booktitle={Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={191--202},
  year={1993},
  organization={ACM}
}

@inproceedings{Wallace:2010:ALB:1835804.1835829,
 author = {Wallace, Byron C. and Small, Kevin and Brodley, Carla E. and Trikalinos, Thomas A.},
 title = {Active Learning for Biomedical Citation Screening},
 booktitle = {Proceedings of 16th ACM SIGKDD Intl.\ Conference on Knowledge Discovery \& Data Mining},
 year = {2010},
 pages = {173--182}
 } 

@inproceedings{buttcher2007reliable,
  title={Reliable information retrieval evaluation with incomplete and biased judgements},
  author={B{\"u}ttcher, Stefan and Clarke, Charles LA and Yeung, Peter CK and Soboroff, Ian},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={63--70},
  year={2007},
  organization={ACM}
}

@inproceedings{hui2015selective,
  title={Selective labeling and incomplete label mitigation for low-cost evaluation},
  author={Hui, Kai and Berberich, Klaus},
  booktitle={International Symposium on String Processing and Information Retrieval},
  pages={137--148},
  year={2015},
  organization={Springer}
}
@article{cohn1994improving,
  title={Improving generalization with active learning},
  author={Cohn, David and Atlas, Les and Ladner, Richard},
  journal={Machine learning},
  volume={15},
  number={2},
  pages={201--221},
  year={1994},
  publisher={Springer}
}
@inproceedings{seung1992query,
  title={Query by committee},
  author={Seung, H Sebastian and Opper, Manfred and Sompolinsky, Haim},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={287--294},
  year={1992},
  organization={ACM}
}

@article{freund1997selective,
  title={Selective sampling using the query by committee algorithm},
  author={Freund, Yoav and Seung, H Sebastian and Shamir, Eli and Tishby, Naftali},
  journal={Machine learning},
  volume={28},
  number={2},
  pages={133--168},
  year={1997},
  publisher={Springer}
}

@article{angluin1988queries,
  title={Queries and concept learning},
  author={Angluin, Dana},
  journal={Machine learning},
  volume={2},
  number={4},
  pages={319--342},
  year={1988},
  publisher={Springer}
}
@inproceedings{lewis1994sequential,
  title={A sequential algorithm for training text classifiers},
  author={Lewis, David D and Gale, William A},
  booktitle={Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={3--12},
  year={1994},
  organization={Springer-Verlag New York, Inc.}
}

@inproceedings{tang2002active,
  title={Active learning for statistical natural language parsing},
  author={Tang, Min and Luo, Xiaoqiang and Roukos, Salim},
  booktitle={Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  pages={120--127},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{nguyen2015combining,
  title={Combining crowd and expert labels using decision theoretic active learning},
  author={Nguyen, An Thanh and Wallace, Byron C and Lease, Matthew},
  booktitle={Third AAAI Conference on Human Computation and Crowdsourcing},
  year={2015}
}

@article{horvitz1952generalization,
  title={A generalization of sampling without replacement from a finite universe},
  author={Horvitz, Daniel G and Thompson, Donovan J},
  journal={Journal of the American statistical Association},
  volume={47},
  number={260},
  pages={663--685},
  year={1952},
  publisher={Taylor \& Francis Group}
}

@article{salton1988term,
  title={Term-weighting approaches in automatic text retrieval},
  author={Salton, Gerard and Buckley, Christopher},
  journal={Information processing \& management},
  volume={24},
  number={5},
  pages={513--523},
  year={1988},
  publisher={Elsevier}
}
@inproceedings{buckley2004retrieval,
  title={Retrieval evaluation with incomplete information},
  author={Buckley, Chris and Voorhees},
  booktitle={Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={25--32},
  year={2004},
  organization={ACM}
}
@article{kendall1938new,
  title={A new measure of rank correlation},
  author={Kendall, Maurice G},
  journal={Biometrika},
  volume={30},
  number={1/2},
  pages={81--93},
  year={1938},
  publisher={JSTOR}
}

@phdthesis{liu2004effect,
  title={The effect of oversampling and undersampling on classifying imbalanced text datasets},
  author={Liu, Alexander Yun-chung},
  year={2004},
  school={Citeseer}
}

@inproceedings{zobel1998reliable,
  title={How reliable are the results of large-scale information retrieval experiments?},
  author={Zobel, Justin},
  booktitle={Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={307--314},
  year={1998},
  organization={ACM}
}

@inproceedings{hosseini2012uncertainty,
  title={An uncertainty-aware query selection model for evaluation of IR systems},
  author={Hosseini, Mehdi and Cox, Ingemar J and Milic-Frayling, Natasa and Shokouhi, Milad and Yilmaz, Emine},
  booktitle={Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval},
  pages={901--910},
  year={2012},
  organization={ACM}
}

@article{kutlu2018intelligent,
  title={Intelligent topic selection for low-cost information retrieval evaluation: A New perspective on deep vs. shallow judging},
  author={Kutlu, Mucahid and Elsayed, Tamer and Lease, Matthew},
  journal={Information Processing \& Management},
  volume={54},
  number={1},
  pages={37--59},
  year={2018},
  publisher={Elsevier}
}

@article{guiver2009few,
  title={A few good topics: Experiments in topic set reduction for retrieval evaluation},
  author={Guiver, John and Mizzaro, Stefano and Robertson, Stephen},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={27},
  number={4},
  pages={21},
  year={2009},
  publisher={ACM}
}

@inproceedings{gronqvist2005evaluating,
  title={Evaluating latent semantic vector models with synonym tests and document retrieval},
  author={Gr{\"o}nqvist, Leif},
  booktitle={ELECTRA workshop: Methodologies and evaluation of lexical cohesion techniques in real-world applications beyond bag of words},
  pages={86--88},
  year={2005},
  organization={Citeseer}
}

@inproceedings{cormack1998efficient,
  title={Efficient construction of large test collections},
  author={Cormack, Gordon V and Palmer, Christopher R and Clarke, Charles LA},
  booktitle={Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={282--289},
  year={1998},
  organization={ACM}
}

@inproceedings{carterette2005incremental,
  title={Incremental test collections},
  author={Carterette, Ben and Allan, James},
  booktitle={Proceedings of the 14th ACM international conference on Information and knowledge management},
  pages={680--687},
  year={2005},
  organization={ACM}
}

@inproceedings{carterette2007semiautomatic,
  title={Semiautomatic evaluation of retrieval systems using document similarities},
  author={Carterette, Ben and Allan, James},
  booktitle={Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
  pages={873--876},
  year={2007},
  organization={ACM}
}
@inproceedings{cleverdon1967cranfield,
  title={The Cranfield tests on index language devices},
  author={Cleverdon, Cyril},
  booktitle={Aslib proceedings},
  volume={19},
  number={6},
  pages={173--194},
  year={1967},
  organization={MCB UP Ltd}
}

@inbook{Thompson,
title = {Sampling},
author = {Thompson, Steven K.},
publisher = {John Wiley \& Sons, Inc.},
isbn = {9781118162934},
url = {http://dx.doi.org/10.1002/9781118162934},
doi = {10.1002/9781118162934},
booktitle = {Sampling},
year = {2012},
}

@inproceedings{rajput2012constructing,
  title={Constructing test collections by inferring document relevance via extracted relevant information},
  author={Rajput, Shahzad and Ekstrand-Abueg, Matthew and Pavlu, Virgil and Aslam, Javed A},
  booktitle={Proceedings of the 21st ACM international conference on Information and knowledge management},
  pages={145--154},
  year={2012},
  organization={ACM}
}

@incollection{robbins1985some,
  title={Some aspects of the sequential design of experiments},
  author={Robbins, Herbert},
  booktitle={Herbert Robbins Selected Papers},
  pages={169--177},
  year={1985},
  publisher={Springer}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@article{landauer1998introduction,
  title={An introduction to latent semantic analysis},
  author={Landauer, Thomas K and Foltz, Peter W and Laham, Darrell},
  journal={Discourse processes},
  volume={25},
  number={2-3},
  pages={259--284},
  year={1998},
  publisher={Taylor \& Francis}
}



@article{ahlgren-evaluation-2008,
  title={Evaluation of retrieval effectiveness with incomplete relevance data: Theoretical and experimental comparison of three measures},
  author={Ahlgren, Per and Gr{\"o}nqvist, Leif},
  journal={Information Processing \& Management},
  volume={44},
  number={1},
  pages={212--225},
  year={2008},
  publisher={Elsevier}
}
@article{zhang2010understanding,
  title={Understanding bag-of-words model: a statistical framework},
  author={Zhang, Yin and Jin, Rong and Zhou, Zhi-Hua},
  journal={International Journal of Machine Learning and Cybernetics},
  volume={1},
  number={1-4},
  pages={43--52},
  year={2010},
  publisher={Springer}
}

@article{jardine1971use,
  title={The use of hierarchic clustering in information retrieval},
  author={Jardine, Nick and van Rijsbergen, Cornelis Joost},
  journal={Information storage and retrieval},
  volume={7},
  number={5},
  pages={217--240},
  year={1971},
  publisher={Elsevier}
}

@inproceedings{baruah2016optimizing,
  title={Optimizing Nugget Annotations with Active Learning},
  author={Baruah, Gaurav and Zhang, Haotian and Guttikonda, Rakesh and Lin, Jimmy and Smucker, Mark D and Vechtomova, Olga},
  booktitle={Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
  pages={2359--2364},
  year={2016},
  organization={ACM}
}

@article{roegiest2015trec,
  title={TREC 2015 Total Recall track overview},
  author={Roegiest, Adam and Cormack, Gordon V and Grossman, M and Clarke, C},
  journal={Proc. TREC-2015},
  year={2015}
}

@inproceedings{cormack2015waterloo,
  title={Waterloo (Cormack) Participation in the TREC 2015 Total Recall Track.},
  author={Cormack, Gordon V}
}




@inproceedings{zobel-how-1998,
	address = {Melbourne, Australia},
	title = {How reliable are the results of large-scale information retrieval experiments?},
	isbn = {978-1-58113-015-7},
	doi = {10.1145/290941.291014},
	abstract = {Two stages in measurement of techniques for information retrieval are gathering of documents for relevance assessment and use of the assessments to numerically evaluate eﬀectiveness. We consider both of these stages in the context of the TREC experiments, to determine whether they lead to measurements that are trustworthy and fair. Our detailed empirical investigation of the TREC results shows that the measured relative performance of systems appears to be reliable, but that recall is overestimated: it is likely that many relevant documents have not been found. We propose a new pooling strategy that can signiﬁcantly increase the number of relevant documents found for given eﬀort, without compromising fairness.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '98},
	publisher = {ACM Press},
	author = {Zobel, Justin},
	year = {1998},
	pages = {307--314}
}

@article{moffat-estimating-2018,
	title = {Estimating {Measurement} {Uncertainty} for {Information} {Retrieval} {Effectiveness} {Metrics}},
	volume = {10},
	issn = {19361955},
	doi = {10.1145/3239572},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Journal of Data and Information Quality},
	author = {Moffat, Alistair and Scholer, Falk and Yang, Ziying},
	month = sep,
	year = {2018},
	pages = {1--22}
}

@article{roitero-reproduce.-2018,
	title = {Reproduce. {Generalize}. {Extend}. {On} {Information} {Retrieval} {Evaluation} without {Relevance} {Judgments}},
	volume = {10},
	issn = {19361955},
	doi = {10.1145/3241064},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Journal of Data and Information Quality},
	author = {Roitero, Kevin and Passon, Marco and Serra, Giuseppe and Mizzaro, Stefano},
	month = sep,
	year = {2018},
	pages = {1--32}
}

@article{goldberg-further-2018,
	title = {Further {Insights} on {Drawing} {Sound} {Conclusions} from {Noisy} {Judgments}},
	volume = {36},
	issn = {10468188},
	doi = {10.1145/3186195},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {ACM Transactions on Information Systems},
	author = {Goldberg, David and Trotman, Andrew and Wang, Xiao and Min, Wei and Wan, Zongru},
	month = apr,
	year = {2018},
	pages = {1--31}
}

@book{eiben-introduction-2003,
  title={Introduction to evolutionary computing},
  author={Eiben, Agoston E and Smith, James E and others},
  volume={53},
  year={2003},
  publisher={Springer}
}

@inproceedings{carterette-research-2007,
 author = {Carterette, Ben and Allan, James},
 title = {Research Methodology in Studies of Assessor Effort for Information Retrieval Evaluation},
 booktitle = {Large Scale Semantic Access to Content (Text, Image, Video, and Sound)},
 series = {RIAO '07},
 year = {2007},
 location = {Pittsburgh, Pennsylvania},
 pages = {738--757},
 numpages = {20},
 acmid = {1931461},
 publisher = {LE CENTRE DE HAUTES ETUDES INTERNATIONALES D'INFORMATIQUE DOCUMENTAIRE},
 address = {Paris, France, France},
} 

@article{kendall-new-1938,
  title={A new measure of rank correlation},
  author={Kendall, Maurice G},
  journal={Biometrika},
  volume={30},
  number={1/2},
  pages={81--93},
  year={1938},
  publisher={JSTOR}
}

@inproceedings{yilmaz-new-2008,
  title={A new rank correlation coefficient for information retrieval},
  author={Yilmaz, Emine and Aslam, Javed A and Robertson, Stephen},
  booktitle={Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={587--594},
  year={2008},
  organization={ACM}
}

@inproceedings{kutlu-rank-2018,
  title={When Rank Order Isn't Enough: New Statistical-Significance-Aware Correlation Measures},
  author={Kutlu, Mucahid and Elsayed, Tamer and Hasanain, Maram and Lease, Matthew},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={397--406},
  year={2018},
  organization={ACM}
}

@inproceedings{carterette-rank-2009,
  title={On rank correlation and the distance between rankings},
  author={Carterette, Ben},
  booktitle={Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval},
  pages={436--443},
  year={2009},
  organization={ACM}
}

@inproceedings{gao-head-2015,
  title={A head-weighted gap-sensitive correlation coefficient},
  author={Gao, Ning and Oard, Douglas},
  booktitle={Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={799--802},
  year={2015},
  organization={ACM}
}


@inproceedings{Kutlu18-sigir,
  author = {Mucahid Kutlu and Tyler McDonnell and Yassmine Barkallah and Tamer Elsayed and Matthew Lease},
  title = {{What Can Rationales behind Relevance Judgments Tell Us About Assessor Disagreement?}},
  booktitle = {Proceedings of the 41st international ACM SIGIR conference on Research and development in Information Retrieval},
  year = {2018},
  data = {http://ir.ischool.utexas.edu/webcrowd25k/},
  pages = {805--814}
}


@inproceedings{McDonnell16-hcomp,
  author = {Tyler McDonnell and Matthew Lease and Mucahid Kutlu and Tamer Elsayed},
  title = {{Why Is That Relevant?\ Collecting Annotator Rationales for Relevance Judgments}},
  booktitle = {{Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP)}},
  pages = {139--148},
  year = {2016}
}

@article{spoerri-overlap-2005,
  title={How the overlap between the search results of different retrieval systems correlates with document relevance},
  author={Spoerri, Anselm},
  journal={Proceedings of the American Society for Information Science and Technology},
  volume={42},
  number={1},
  year={2005},
  publisher={Wiley Online Library}
}

@article{roitero-reproduce-2018,
	title = {Reproduce and {Improve}: {An} {Evolutionary} {Approach} to {Select} a {Few} {Good} {Topics} for {Information} {Retrieval} {Evaluation}},
	volume = {10},
	issn = {19361955},
	shorttitle = {Reproduce and {Improve}},
	doi = {10.1145/3239573},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Journal of Data and Information Quality},
	author = {Roitero, Kevin and Soprano, Michael and Brunello, Andrea and Mizzaro, Stefano},
	month = sep,
	year = {2018},
	pages = {1--21}
}

@book{sakai-laboratory-2018,
	address = {New York, NY},
	title = {Laboratory experiments in information retrieval: sample sizes, effect sizes, and statistical power},
	isbn = {9789811311987},
	shorttitle = {Laboratory experiments in information retrieval},
	language = {en},
	publisher = {Springer Berlin Heidelberg},
	author = {Sakai, Tetsuya},
	year = {2018}
}

@incollection{ferrante-modelling-2018,
	address = {Cham},
	title = {Modelling {Randomness} in {Relevance} {Judgments} and {Evaluation} {Measures}},
	volume = {10772},
	isbn = {978-3-319-76940-0 978-3-319-76941-7},
	abstract = {We propose a general stochastic approach which deﬁnes relevance as a set of binomial random variables where the expectation p of each variable indicates the quantity of relevance for each relevance grade. This represents the ﬁrst step in the direction of modelling evaluation measures as a transformation of random variables, turning them into random evaluation measures. We show that a consequence of this new approach is to remove the distinction between binary and multi-graded measures and, at the same time, to deal with incomplete information, providing a single uniﬁed framework for all these diﬀerent aspects. We experiment on TREC collections to show how these new random measures correlate to existing ones and which desirable properties, such as robustness to pool downsampling and discriminative power, they have.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Ferrante, Marco and Ferro, Nicola and Pontarollo, Silvia},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	doi = {10.1007/978-3-319-76941-7-15},
	pages = {197--209}
}

@inproceedings{urbano-stochastic-2018,
	address = {Ann Arbor, MI, USA},
	title = {Stochastic {Simulation} of {Test} {Collections}: {Evaluation} {Scores}},
	isbn = {978-1-4503-5657-2},
	shorttitle = {Stochastic {Simulation} of {Test} {Collections}},
	doi = {10.1145/3209978.3210043},
	abstract = {Part of Information Retrieval evaluation research is limited by the fact that we do not know the distributions of system efectiveness over the populations of topics and, by extension, their true mean scores. The workaround usually consists in resampling topics from an existing collection and approximating the statistics of interest with the observations made between random subsamples, as if one represented the population and the other a random sample. However, this methodology is clearly limited by the availability of data, the impossibility to control the properties of these data, and the fact that we do not really measure what we intend to. To overcome these limitations, we propose a method based on vine copulas for stochastic simulation of evaluation results where the true system distributions are known upfront. In the basic use case, it takes the scores from an existing collection to build a semi-parametric model representing the set of systems and the population of topics, which can then be used to make realistic simulations of the scores by the same systems but on random new topics. Our ability to simulate this kind of data not only eliminates the current limitations, but also ofers new opportunities for research. As an example, we show the beneits of this approach in two sample applications replicating typical experiments found in the literature. We provide a full R package to simulate new data following the proposed method, which can also be used to fully reproduce the results in this paper.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}  - {SIGIR} '18},
	publisher = {ACM Press},
	author = {Urbano, Julián and Nagler, Thomas},
	year = {2018},
	pages = {695--704}
}

@article{benham-improving-nodate,
	title = {Improving {Recall} {In} {Text} {Retrieval} {Using} {Rank} {Fusion}},
	language = {en},
	author = {Benham, Rodger},
	pages = {79}
}

@incollection{tomlinson-measuring-2017,
	address = {Berlin, Heidelberg},
	title = {Measuring {Effectiveness} in the {TREC} {Legal} {Track}},
	volume = {37},
	isbn = {978-3-662-53816-6 978-3-662-53817-3},
	abstract = {In this chapter, we report our experiences from attempting to measure the effectiveness of large electronic discovery (e-Discovery) result sets in the Text Retrieval Conference (TREC) Legal Track campaigns of 2006–2011. For effectiveness measures, we have focused on recall, precision and F1. We state the estimators that we have used for these measures, and we outline both the rankbased and set-based approaches to sampling that we have taken. We share our experiences with the sampling error in the resulting estimates for the absolute effectiveness on individual topics, relative effectiveness on individual topics, mean effectiveness across topics and relative effectiveness across topics. Finally, we discuss our experiences with assessor error, which we have found has often had a larger impact than sampling error.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Current {Challenges} in {Patent} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tomlinson, Stephen and Hedin, Bruce},
	editor = {Lupu, Mihai and Mayer, Katja and Kando, Noriko and Trippe, Anthony J.},
	year = {2017},
	doi = {10.1007/978-3-662-53817-3-6},
	pages = {163--182}
}

@inproceedings{carterette-but-2017,
	address = {Shinjuku, Tokyo, Japan},
	title = {But {Is} {It} {Statistically} {Significant}?: {Statistical} {Significance} in {IR} {Research}, 1995-2014},
	isbn = {978-1-4503-5022-8},
	shorttitle = {But {Is} {It} {Statistically} {Significant}?},
	doi = {10.1145/3077136.3080738},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR} '17},
	publisher = {ACM Press},
	author = {Carterette, Ben},
	year = {2017},
	pages = {1125--1128}
}

@article{li-active-2017,
	title = {Active {Sampling} for {Large}-scale {Information} {Retrieval} {Evaluation}},
	doi = {10.1145/3132847.3133015},
	language = {en},
	urldate = {2018-10-21},
	journal = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management  - CIKM '17},
	author = {Li and Kanoulas, Evangelos},
	year = {2017},
	note = {arXiv: 1709.01709},
	pages = {49--58}
}

@article{lu-effect-2016,
	title = {The effect of pooling and evaluation depth on {IR} metrics},
	volume = {19},
	issn = {1386-4564, 1573-7659},
	doi = {10.1007/s10791-016-9282-6},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {Information Retrieval Journal},
	author = {Lu, Xiaolu and Moffat, Alistair and Culpepper, J. Shane},
	month = aug,
	year = {2016},
	pages = {416--445}
}

@article{lu-efcient-nodate,
	title = {Efﬁcient and {Effective} {Retrieval} {Using} {Higher}-{Order} {Proximity} {Models}},
	language = {en},
	author = {Lu, Xiaolu and Culpepper, Dr Jason Shane and Moffat, Alistair},
	pages = {194}
}

@article{tonon-pooling-based-2015,
	title = {Pooling-based continuous evaluation of information retrieval systems},
	volume = {18},
	issn = {1386-4564, 1573-7659},
	doi = {10.1007/s10791-015-9266-y},
	abstract = {The dominant approach to evaluate the effectiveness of information retrieval (IR) systems is by means of reusable test collections built following the Cranﬁeld paradigm. In this paper, we propose a new IR evaluation methodology based on pooled testcollections and on the continuous use of either crowdsourcing or professional editors to obtain relevance judgements. Instead of building a static collection for a ﬁnite set of systems known a priori, we propose an IR evaluation paradigm where retrieval approaches are evaluated iteratively on the same collection. Each new retrieval technique takes care of obtaining its missing relevance judgements and hence contributes to augmenting the overall set of relevance judgements of the collection. We also propose two metrics: Fairness Score, and opportunistic number of relevant documents, which we then use to deﬁne new pooling strategies. The goal of this work is to study the behavior of standard IR metrics, IR system ranking, and of several pooling techniques in a continuous evaluation context by comparing continuous and non-continuous evaluation results on classic test collections. We both use standard and crowdsourced relevance judgements, and we actually run a continuous evaluation campaign over several existing IR systems.},
	language = {en},
	number = {5},
	urldate = {2018-10-21},
	journal = {Information Retrieval Journal},
	author = {Tonon, Alberto and Demartini, Gianluca and Cudré-Mauroux, Philippe},
	month = oct,
	year = {2015},
	pages = {445--472}
}

@inproceedings{soboroff-building-2017,
	address = {Shinjuku, Tokyo, Japan},
	title = {Building {Test} {Collections}: {An} {Interactive} {Guide} for {Students} and {Others} {Without} {Their} {Own} {Evaluation} {Conference} {Series}},
	isbn = {978-1-4503-5022-8},
	shorttitle = {Building {Test} {Collections}},
	doi = {10.1145/3077136.3082064},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR} '17},
	publisher = {ACM Press},
	author = {Soboroff, Ian},
	year = {2017},
	pages = {1407--1410}
}

@inproceedings{yilmaz-simple-2008,
	address = {Singapore, Singapore},
	title = {A simple and efficient sampling method for estimating {AP} and {NDCG}},
	isbn = {978-1-60558-164-4},
	doi = {10.1145/1390334.1390437},
	abstract = {We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive eﬀort required to judge tens of thousands of documents. While the ﬁrst method proposed by Aslam et al. [1] is quite accurate and eﬃcient, it is overly complex, making it diﬃcult to be used by the community, and while the second method proposed by Yilmaz et al., infAP [14], is relatively simple, it is less eﬃcient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide conﬁdence intervals on the estimated values.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '08},
	publisher = {ACM Press},
	author = {Yilmaz, Emine and Kanoulas, Evangelos and Aslam, Javed A.},
	year = {2008},
	keywords = {statistical},
	pages = {603}
}

@article{cormack-autonomy-2015,
	title = {Autonomy and {Reliability} of {Continuous} {Active} {Learning} for {Technology}-{Assisted} {Review}},
	abstract = {We enhance the autonomy of the continuous active learning method shown by Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted review, in which documents from a collection are retrieved and reviewed, using relevance feedback, until substantially all of the relevant documents have been reviewed. Autonomy is enhanced through the elimination of topic-specific and dataset-specific tuning parameters, so that the sole input required by the user is, at the outset, a short query, topic description, or single relevant document; and, throughout the review, ongoing relevance assessments of the retrieved documents. We show that our enhancements consistently yield superior results to Cormack and Grossman's version of continuous active learning, and other methods, not only on average, but on the vast majority of topics from four separate sets of tasks: the legal datasets examined by Cormack and Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and the construction of the TREC 2002 filtering test collection.},
	language = {en},
	urldate = {2018-10-21},
	journal = {arXiv:1504.06868 [cs]},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.06868},
	keywords = {beyond\-pooling\-full\-version, cormack}
}

@inproceedings{moffat-pooled-2015,
	address = {Melbourne, Australia},
	title = {Pooled {Evaluation} {Over} {Query} {Variations}: {Users} are as {Diverse} as {Systems}},
	isbn = {978-1-4503-3794-6},
	shorttitle = {Pooled {Evaluation} {Over} {Query} {Variations}},
	doi = {10.1145/2806416.2806606},
	abstract = {Evaluation of information retrieval systems with test collections makes use of a suite of ﬁxed resources: a document corpus; a set of topics; and associated judgments of the relevance of each document to each topic. With large modern collections, exhaustive judging is not feasible. Therefore an approach called pooling is typically used where, for example, the documents to be judged can be determined by taking the union of all documents returned in the top positions of the answer lists returned by a range of systems. Conventionally, pooling uses system variations to provide diverse documents to be judged for a topic; different user queries are not considered. We explore the ramiﬁcations of user query variability on pooling, and demonstrate that conventional test collections do not cover this source of variation. The effect of user query variation on the size of the judging pool is just as strong as the effect of retrieval system variation. We conclude that user query variation should be incorporated early in test collection construction, and cannot be considered effectively post hoc.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 24th {ACM} {International} on {Conference} on {Information} and {Knowledge} {Management} - {CIKM} '15},
	publisher = {ACM Press},
	author = {Moffat, Alistair and Scholer, Falk and Thomas, Paul and Bailey, Peter},
	year = {2015},
	pages = {1759--1762}
}

@inproceedings{hashemi-reusability-2015,
	address = {Santiago, Chile},
	title = {On the {Reusability} of {Open} {Test} {Collections}},
	isbn = {978-1-4503-3621-5},
	doi = {10.1145/2766462.2767788},
	abstract = {Creating test collections for modern search tasks is increasingly more challenging due to the growing scale and dynamic nature of content, and need for richer contextualization of the statements of request. To address these issues, the TREC Contextual Suggestion Track explored an open test collection, where participants were allowed to submit any web page as a result for a personalized venue recommendation task. This prompts the question on the reusability of the resulting test collection: How does the open nature aﬀect the pooling process? Can participants reliably evaluate variant runs with the resulting qrels? Can other teams evaluate new runs reliably? In short, does the set of pooled and judged documents eﬀectively produce a post hoc test collection? Our main ﬁndings are the following: First, while there is a strongly signiﬁcant rank correlation, the effect of pooling is notable and results in underestimation of performance, implying the evaluation of non-pooled systems should be done with great care. Second, we extensively analyze impacts of open corpus on the fraction of judged documents, explaining how low recall aﬀects the reusability, and how the personalization and low pooling depth aggravate that problem. Third, we outline a potential solution by deriving a ﬁxed corpus from open web submissions.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '15},
	publisher = {ACM Press},
	author = {Hashemi, Seyyed Hadi and Clarke, Charles L.A. and Dean-Hall, Adriel and Kamps, Jaap and Kiseleva, Julia},
	year = {2015},
	pages = {827--830}
}

@article{kutlu-intelligent-2018,
	title = {Intelligent topic selection for low-cost information retrieval evaluation: {A} {New} perspective on deep vs. shallow judging},
	volume = {54},
	issn = {03064573},
	shorttitle = {Intelligent topic selection for low-cost information retrieval evaluation},
	doi = {10.1016/j.ipm.2017.09.002},
	abstract = {While test collections provide the cornerstone for Cranﬁeld-based evaluation of information retrieval (IR) systems, it has become practically infeasible to rely on traditional pooling techniques to construct test collections at the scale of today’s massive document collections (e.g., ClueWeb12’s 700M+ Webpages). This has motivated a ﬂurry of studies proposing more costeﬀective yet reliable IR evaluation methods. In this paper, we propose a new intelligent topic selection method which reduces the number of search topics (and thereby costly human relevance judgments) needed for reliable IR evaluation. To rigorously assess our method, we integrate previously disparate lines of research on intelligent topic selection and deep vs. shallow judging (i.e., whether it is more cost-eﬀective to collect many relevance judgments for a few topics or a few judgments for many topics). While prior work on intelligent topic selection has never been evaluated against shallow judging baselines, prior work on deep vs. shallow judging has largely argued for shallowed judging, but assuming random topic selection. We argue that for evaluating any topic selection method, ultimately one must ask whether it is actually useful to select topics, or should one simply perform shallow judging over many topics? In seeking a rigorous answer to this over-arching question, we conduct a comprehensive investigation over a set of relevant factors never previously studied together: 1) method of topic selection; 2) the eﬀect of topic familiarity on human judging speed; and 3) how diﬀerent topic generation processes (requiring varying human eﬀort) impact (i) budget utilization and (ii) the resultant quality of judgments. Experiments on NIST TREC Robust 2003 and Robust 2004 test collections show that not only can we reliably evaluate IR systems with fewer topics, but also that: 1) when topics are intelligently selected, deep judging is often more cost-eﬀective than shallow judging in evaluation reliability; and 2) topic familiarity and topic generation costs greatly impact the evaluation cost vs. reliability trade-oﬀ. Our ﬁndings challenge conventional wisdom in showing that deep judging is often preferable to shallow judging when topics are selected intelligently.},
	language = {en},
	number = {1},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Kutlu, Mucahid and Elsayed, Tamer and Lease, Matthew},
	month = jan,
	year = {2018},
	pages = {37--59}
}

@article{hashemi-easter-2016,
	title = {An {Easter} {Egg} {Hunting} {Approach} to {Test} {Collection} {Building} in {Dynamic} {Domains}},
	abstract = {Test collections for oﬄine evaluation remain crucial for information retrieval research and industrial practice, yet the classical Sparck Jones and Van Rijsbergen approach to test collection building based on the pooling of runs on a large collection is expensive and being pushed beyond its limits with the ever increasing size and dynamic nature of the collections. We experiment with a novel approach to reusable test collection building, where we inject judged pages into an existing corpus, and have systems retrieve pages from the extended corpus with the aim to create a reusable test collection. In a metaphorical way, we hide the Easter eggs for systems to retrieve. Our experiments exploit the unique setup of the TREC Contextual Suggestion Track, which allowed both submissions from a ﬁxed corpus (ClueWeb12) as well as from the open web. We conduct an extensive analysis of the reusability of the test collection based on ClueWeb12, and ﬁnd it too low for reliable oﬄine testing. Then, we detail the expansion with judged pages from the open web, and do extensive analysis on the reusability of the resulting expanded test collection, and observe a dramatic increase in reusability. Our approach oﬀers novel and cost eﬀective ways to build new test collections, and to refresh and update existing test collections. This explores new ways of eﬀective maintenance of oﬄine test collections for dynamic domains such as the web.},
	language = {en},
	author = {Hashemi, Seyyed Hadi and Clarke, Charles L A and Dean-Hall, Adriel and Kamps, Jaap and Kiseleva, Julia},
	year = {2016},
	pages = {8}
}

@inproceedings{tan-reusability-2017,
	address = {Shinjuku, Tokyo, Japan},
	title = {On the {Reusability} of "{Living} {Labs}" {Test} {Collections}:: {A} {Case} {Study} of {Real}-{Time} {Summarization}},
	isbn = {978-1-4503-5022-8},
	shorttitle = {On the {Reusability} of "{Living} {Labs}" {Test} {Collections}},
	doi = {10.1145/3077136.3080644},
	abstract = {Information retrieval test collections are typically built using data from large-scale evaluations in international forums such as TREC, CLEF, and NTCIR. Previous validation studies on pool-based test collections for ad hoc retrieval have examined their reusability to accurately assess the e ectiveness of systems that did not participate in the original evaluation. To our knowledge, the reusability of test collections derived from “living labs” evaluations, based on logs of user activity, has not been explored. In this paper, we performed a “leave-one-out” analysis of human judgment data derived from the TREC 2016 Real-Time Summarization Track and show that those judgments do not appear to be reusable. While this nding is limited to one speci c evaluation, it does call into question the reusability of test collections built from living labs in general, and at the very least suggests the need for additional work in validating such experimental instruments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR} '17},
	publisher = {ACM Press},
	author = {Tan, Luchen and Baruah, Gaurav and Lin, Jimmy},
	year = {2017},
	pages = {793--796}
}

@inproceedings{carterette-bayesian-2015,
	address = {Northampton, Massachusetts, USA},
	title = {Bayesian {Inference} for {Information} {Retrieval} {Evaluation}},
	isbn = {978-1-4503-3833-2},
	doi = {10.1145/2808194.2809469},
	abstract = {A key component of experimentation in IR is statistical hypothesis testing, which researchers and developers use to make inferences about the eﬀectiveness of their system relative to others. A statistical hypothesis test can tell us the likelihood that small mean diﬀerences in eﬀectiveness (on the order of 5\%, say) is due to randomness or measurement error, and thus is critical for making progress in research. But the tests typically used in IR—the t-test, the Wilcoxon signed-rank test—are very general, not developed speciﬁcally for the problems we face in information retrieval evaluation. A better approach would take advantage of the fact that the atomic unit of measurement in IR is the relevance judgment rather than the eﬀectiveness measure, and develop tests that model relevance directly. In this work we present such an approach, showing theoretically that modeling relevance in this way naturally gives rise to the eﬀectiveness measures we care about. We demonstrate the usefulness of our model on both simulated data and a diverse set of runs from various TREC tracks.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {Theory} of {Information} {Retrieval} - {ICTIR} '15},
	publisher = {ACM Press},
	author = {Carterette, Ben},
	year = {2015},
	pages = {31--40}
}

@inproceedings{jayasinghe-improving-2014,
	address = {Melbourne, VIC, Australia},
	title = {Improving test collection pools with machine learning},
	isbn = {978-1-4503-3000-8},
	doi = {10.1145/2682862.2682864},
	abstract = {IR experiments typically use test collections for evaluation. Such test collections are formed by judging a pool of documents retrieved by a combination of automatic and manual runs for each topic. The proportion of relevant documents found for each topic depends on the diversity across each of the runs submitted and the depth to which runs are assessed (pool depth). Manual runs are commonly believed to reduce bias in test collections when evaluating new IR systems.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 2014 {Australasian} {Document} {Computing} {Symposium} on - {ADCS} '14},
	publisher = {ACM Press},
	author = {Jayasinghe, Gaya K. and Webber, William and Sanderson, Mark and Culpepper, J. Shane},
	year = {2014},
	pages = {2--9}
}

@inproceedings{hui-dealing-2017,
	address = {Amsterdam, The Netherlands},
	title = {Dealing with {Incomplete} {Judgments} in {Cascade} {Measures}},
	isbn = {978-1-4503-4490-6},
	doi = {10.1145/3121050.3121064},
	abstract = {Cascade measures like α-nDCG, ERR-IA, and NRBP take into account novelty and diversity of query results and are computed using judgments provided by humans, which are costly to collect. ese measures expect that all documents in the result list of a query are judged and cannot make use of judgments beyond the assigned labels. Existing work has demonstrated that condensing the query results by taking out documents without judgment can address this problem to some extent. However, how highly incomplete judgments can a ect cascade measures and how to cope with such incompleteness have not been addressed yet. In this paper, we propose an approach which mitigates incomplete judgments by leveraging the content of documents relevant to the query’s subtopics. ese language models are estimated at each rank taking into account the document and the upper ranked ones. en, our method determines gain values based on the Kullback-Leibler divergence between the language models. Experiments on the diversity tasks of the TREC Web Track 2009–2012 show that with only 15\% of the judgments our method accurately reconstructs the original rankings determined by the established cascade measures.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the {ACM} {SIGIR} {International} {Conference} on {Theory} of {Information} {Retrieval}  - {ICTIR} '17},
	publisher = {ACM Press},
	author = {Hui, Kai and Berberich, Klaus and Mele, Ida},
	year = {2017},
	pages = {83--90}
}

@inproceedings{makary-using-2017,
	address = {Lyon, France},
	title = {Using {Supervised} {Machine} {Learning} to {Automatically} {Build} {Relevance} {Judgments} for a {Test} {Collection}},
	isbn = {978-1-5386-1051-0},
	doi = {10.1109/DEXA.2017.38},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {2017 28th {International} {Workshop} on {Database} and {Expert} {Systems} {Applications} ({DEXA})},
	publisher = {IEEE},
	author = {Makary, Mireille and Oakes, Michael and Mitkov, Ruslan and Yammout, Fadi},
	month = aug,
	year = {2017},
	pages = {108--112}
}

@incollection{gao-reducing-2014,
	address = {Cham},
	title = {Reducing {Reliance} on {Relevance} {Judgments} for {System} {Comparison} by {Using} {Expectation}-{Maximization}},
	volume = {8416},
	isbn = {978-3-319-06027-9 978-3-319-06028-6},
	abstract = {Relevance judgments are often the most expensive part of information retrieval evaluation, and techniques for comparing retrieval systems using fewer relevance judgments have received signiﬁcant attention in recent years. This paper proposes a novel system comparison method using an expectationmaximization algorithm. In the expectation step, real-valued pseudo-judgments are estimated from a set of system results. In the maximization step, new system weights are learned from a combination of a limited number of actual human judgments and system pseudo-judgments for the other documents. The method can work without any human judgments, and is able to improve its accuracy by incrementally adding human judgments. Experiments using TREC Ad Hoc collections demonstrate strong correlations with system rankings using pooled human judgments, and comparison with existing baselines indicates that the new method achieves the same comparison reliability with fewer human judgments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Gao, Ning and Webber, William and Oard, Douglas W.},
	editor = {de Rijke, Maarten and Kenter, Tom and de Vries, Arjen P. and Zhai, ChengXiang and de Jong, Franciska and Radinsky, Kira and Hofmann, Katja},
	year = {2014},
	doi = {10.1007/978-3-319-06028-6-1},
	pages = {1--12}
}

@inproceedings{moffat-judgment-2016,
	address = {Caulfield, VIC, Australia},
	title = {Judgment {Pool} {Effects} {Caused} by {Query} {Variations}},
	isbn = {978-1-4503-4865-2},
	doi = {10.1145/3015022.3015025},
	abstract = {Batch-mode retrieval evaluation relies on suitable relevance judgments being available. Here we explore the implications on pool size of adopting a “query variations” approach to collection construction. Using the resources provided as part of the UQV100 collection [Bailey et al., SIGIR 2016] and a total of ﬁve different systems, we show that pool size is as much affected by the number of query variations involved as it is by the number of contributing systems, and that systems and users are independent effects. That is, if both system and query variation are to be accommodated in retrieval experimentation, the cost of performing the required judgments compounds.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 21st {Australasian} {Document} {Computing} {Symposium} on {ZZZ} - {ADCS} '16},
	publisher = {ACM Press},
	author = {Moffat, Alistair},
	year = {2016},
	pages = {65--68}
}

@inproceedings{jayasinghe-extending-2014,
	address = {Gold Coast, Queensland, Australia},
	title = {Extending test collection pools without manual runs},
	isbn = {978-1-4503-2257-7},
	doi = {10.1145/2600428.2609473},
	abstract = {Information retrieval test collections traditionally use a combination of automatic and manual runs to create a pool of documents to be judged. The quality of the ﬁnal judgments produced for a collection is a product of the variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through manual runs. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval - {SIGIR} '14},
	publisher = {ACM Press},
	author = {Jayasinghe, Gaya K. and Webber, William and Sanderson, Mark and Culpepper, J. Shane},
	year = {2014},
	pages = {915--918}
}

@inproceedings{vinjumur-assessing-2014,
	address = {Gold Coast, Queensland, Australia},
	title = {Assessing the reliability and reusability of an {E}-discovery privilege test collection},
	isbn = {978-1-4503-2257-7},
	doi = {10.1145/2600428.2609506},
	abstract = {In some jurisdictions, parties to a lawsuit can request documents from each other, but documents subject to a claim of privilege may be withheld. The TREC 2010 Legal Track developed what is presently the only public test collection for evaluating privilege classiﬁcation. This paper examines the reliability and reusability of that collection. For reliability, the key question is the extent to which privilege judgments correctly reﬂect the opinion of the senior litigator whose judgment is authoritative. For reusability, the key question is the degree to which systems whose results contributed to creation of the test collection can be fairly compared with other systems that use those privilege judgments in the future. These correspond to measurement error and sampling error, respectively. The results indicate that measurement error is the larger problem.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval - {SIGIR} '14},
	publisher = {ACM Press},
	author = {Vinjumur, Jyothi K. and Oard, Douglas W. and Paik, Jiaul H.},
	year = {2014},
	pages = {1047--1050}
}

@inproceedings{makary-towards-2016,
	address = {Porto, Portugal},
	title = {Towards automatic generation of relevance judgments for a test collection},
	isbn = {978-1-5090-2641-8},
	doi = {10.1109/ICDIM.2016.7829763},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {2016 {Eleventh} {International} {Conference} on {Digital} {Information} {Management} ({ICDIM})},
	publisher = {IEEE},
	author = {Makary, Mireille and Oakes, Michael and Yamout, Fadi},
	month = sep,
	year = {2016},
	pages = {121--126}
}

@article{rajagopal-relevance-2014,
	title = {{RELEVANCE} {JUDGMENTS} {EXCLUSIVE} {OF} {HUMAN} {ASSESSORS} {IN} {LARGE} {SCALE} {INFORMATION} {RETRIEVAL} {EVALUATION} {EXPERIMENTATION}},
	volume = {27},
	abstract = {Inconsistent judgments by various human assessors’ compromises the reliability of the relevance judgments generated for large scale test collections. An automated method that creates a similar set of relevance judgments (pseudo relevance judgments) that eliminate the human efforts and errors introduced in creating relevance judgments is investigated in this study. Traditionally, the participating systems in TREC are measured by using a chosen metrics and ranked according to its performance scores. In order to generate these scores, the documents retrieved by these systems for each topic are matched with the set of relevance judgments (often assessed by humans). In this study, the number of occurrences of each document per topic from the various runs will be used with an assumption, the higher the number of occurrences of a document, the possibility of the document being relevant is higher. The study proposesa method with a pool depth of 100 using the cutoff percentage of {\textgreater}35\% that could provide an alternate way of generating consistent relevance judgments without the involvement of human assessors.},
	language = {en},
	author = {Rajagopal, Prabha and Ravana, Sri Devi and Ismail, Maizatul Akmar},
	year = {2014},
	pages = {15}
}

@inproceedings{buckley-retrieval-2004,
	address = {Sheffield, United Kingdom},
	title = {Retrieval evaluation with incomplete information},
	isbn = {978-1-58113-881-8},
	doi = {10.1145/1008992.1009000},
	abstract = {This paper examines whether the Cranﬁeld evaluation methodology is robust to gross violations of the completeness assumption (i.e., the assumption that all relevant documents within a test collection have been identiﬁed and are present in the collection). We show that current evaluation measures are not robust to substantially incomplete relevance judgments. A new measure is introduced that is both highly correlated with existing measures when complete judgments are available and more robust to incomplete judgment sets. This ﬁnding suggests that substantially larger or dynamic test collections built using current pooling practices should be viable laboratory tools, despite the fact that the relevance information will be incomplete and imperfect.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 27th annual international conference on {Research} and development in information retrieval  - {SIGIR} '04},
	publisher = {ACM Press},
	author = {Buckley, Chris and Voorhees},
	year = {2004},
	pages = {25}
}

@article{sanderson-information-nodate,
	title = {Information {Retrieval} {System} {Evaluation}: {Effort}, {Sensitivity}, and {Reliability}},
	abstract = {The effectiveness of information retrieval systems is measured by comparing performance on a common set of queries and documents. Significance tests are often used to evaluate the reliability of such comparisons. Previous work has examined such tests, but produced results with limited application. Other work established an alternative benchmark for significance, but the resulting test was too stringent. In this paper, we revisit the question of how such tests should be used. We find that the t-test is highly reliable (more so than the sign or Wilcoxon test), and is far more reliable than simply showing a large percentage difference in effectiveness measures between IR systems. Our results show that past empirical work on significance tests overestimated the error of such tests. We also re-consider comparisons between the reliability of precision at rank 10 and mean average precision, arguing that past comparisons did not consider the assessor effort required to compute such measures. This investigation shows that assessor effort would be better spent building test collections with more topics, each assessed in less detail.},
	language = {en},
	author = {Sanderson, Mark and Zobel, Justin},
	pages = {8}
}

@inproceedings{soboroff-ranking-2001,
	address = {New Orleans, Louisiana, United States},
	title = {Ranking retrieval systems without relevance judgments},
	isbn = {978-1-58113-331-8},
	doi = {10.1145/383952.383961},
	abstract = {The most prevalent experimental methodology for comparing the eﬀectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics. It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text REtrieval Conference (TREC) has shown that diﬀerences in human judgments of relevance do not aﬀect the relative measured performance of retrieval systems. Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to as pseudo-relevance judgments. Rankings of systems with our methodology correlate positively with oﬃcial TREC rankings, although the performance of the top systems is not predicted well. The correlations are stable over a variety of pool depths and sampling techniques. With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '01},
	publisher = {ACM Press},
	author = {Soboroff and Nicholas, Charles and Cahan, Patrick},
	year = {2001},
	pages = {66--73}
}

@inproceedings{carterette-minimal-2006,
	address = {Seattle, Washington, USA},
	title = {Minimal test collections for retrieval evaluation},
	isbn = {978-1-59593-369-0},
	doi = {10.1145/1148170.1148219},
	abstract = {Accurate estimation of information retrieval evaluation metrics such as average precision require large sets of relevance judgments. Building sets large enough for evaluation of realworld implementations is at best ineﬃcient, at worst infeasible. In this work we link evaluation with test collection construction to gain an understanding of the minimal judging eﬀort that must be done to have high conﬁdence in the outcome of an evaluation. A new way of looking at average precision leads to a natural algorithm for selecting documents to judge and allows us to estimate the degree of conﬁdence by deﬁning a distribution over possible document judgments. A study with annotators shows that this method can be used by a small group of researchers to rank a set of systems in under three hours with 95\% conﬁdence.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Carterette, Ben and Allan, James and Sitaraman, Ramesh},
	year = {2006},
	pages = {268}
}

@inproceedings{aslam-statistical-2006,
	address = {Seattle, Washington, USA},
	title = {A statistical method for system evaluation using incomplete judgments},
	isbn = {978-1-59593-369-0},
	doi = {10.1145/1148170.1148263},
	abstract = {We consider the problem of large-scale retrieval evaluation, and we propose a statistical method for evaluating retrieval systems using incomplete judgments. Unlike existing techniques that (1) rely on eﬀectively complete, and thus prohibitively expensive, relevance judgment sets, (2) produce biased estimates of standard performance measures, or (3) produce estimates of non-standard measures thought to be correlated with these standard measures, our proposed statistical technique produces unbiased estimates of the standard measures themselves.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Aslam, Javed A. and Pavlu, Virgil and Yilmaz, Emine},
	year = {2006},
	pages = {541}
}

@article{fidel1993qualitative,
  title={Qualitative methods in information retrieval research},
  author={Fidel, Raya},
  journal={Library and information science research},
  volume={15},
  pages={219--219},
  year={1993},
  publisher={ABLEX PUBLISHING CO}
}

@article{mandl2008recent,
  title={Recent developments in the evaluation of information retrieval systems: Moving towards diversity and practical relevance},
  author={Mandl, Thomas},
  journal={Informatica},
  volume={32},
  number={1},
  year={2008}
}

@article{buckley-bias-2007,
	title = {Bias and the limits of pooling for large collections},
	volume = {10},
	issn = {1386-4564, 1573-7659},
	doi = {10.1007/s10791-007-9032-x},
	abstract = {Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic. The idea behind pooling is to ﬁnd enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is suﬃciently complete and unbiased. Yet a constant-size pool represents an increasingly small percentage of the document set as document sets grow larger, and at some point the assumption of approximately complete judgments must become invalid. This paper shows that the judgment sets produced by traditional pooling when the pools are too small relative to the total document set size can be biased in that they favor relevant documents that contain topic title words. This phenomenon is wholly dependent on the collection size and does not depend on the number of relevant documents for a given topic. We show that the AQUAINT test collection constructed in the recent TREC 2005 workshop exhibits this biased relevance set; it is likely that the test collections based on the much larger GOV2 document set also exhibit the bias. The paper concludes with suggested modiﬁcations to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built.},
	language = {en},
	number = {6},
	urldate = {2018-10-21},
	journal = {Information Retrieval},
	author = {Buckley, Chris and Dimmick, Darrin and Soboroff, Ian and Voorhees},
	month = oct,
	year = {2007},
	pages = {491--508}
}

@inproceedings{carterette-robust-2007,
	address = {Amsterdam, The Netherlands},
	title = {Robust test collections for retrieval evaluation},
	isbn = {978-1-59593-597-7},
	doi = {10.1145/1277741.1277754},
	abstract = {Low-cost methods for acquiring relevance judgments can be a boon to researchers who need to evaluate new retrieval tasks or topics but do not have the resources to make thousands of judgments. While these judgments are very useful for a one-time evaluation, it is not clear that they can be trusted when re-used to evaluate new systems. In this work, we formally deﬁne what it means for judgments to be reusable: the conﬁdence in an evaluation of new systems can be accurately assessed from an existing set of relevance judgments. We then present a method for augmenting a set of relevance judgments with relevance estimates that require no additional assessor eﬀort. Using this method practically guarantees reusability: with as few as ﬁve judgments per topic taken from only two systems, we can reliably evaluate a larger set of ten systems. Even the smallest sets of judgments can be useful for evaluation of new systems.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Carterette, Ben},
	year = {2007},
	pages = {55}
}

@inproceedings{turpin-including-2009,
	address = {Boston, MA, USA},
	title = {Including summaries in system evaluation},
	isbn = {978-1-60558-483-6},
	doi = {10.1145/1571941.1572029},
	abstract = {In batch evaluation of retrieval systems, performance is calculated based on predetermined relevance judgements applied to a list of documents returned by the system for a query. This evaluation paradigm, however, ignores the current standard operation of search systems which require the user to view summaries of documents prior to reading the documents themselves.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Turpin, Andrew and Scholer, Falk and Jarvelin, Kalvero and Wu, Mingfang and Culpepper, J. Shane},
	year = {2009},
	pages = {508}
}

@inproceedings{carterette-incremental-2005,
	address = {Bremen, Germany},
	title = {Incremental test collections},
	isbn = {978-1-59593-140-5},
	doi = {10.1145/1099554.1099723},
	abstract = {Corpora and topics are readily available for information retrieval research. Relevance judgments, which are necessary for system evaluation, are expensive; the cost of obtaining them prohibits in-house evaluation of retrieval systems on new corpora or new topics. We present an algorithm for cheaply constructing sets of relevance judgments. Our method intelligently selects documents to be judged and decides when to stop in such a way that with very little work there can be a high degree of conﬁdence in the result of the evaluation. We demonstrate the algorithm’s eﬀectiveness by showing that it produces small sets of relevance judgments that reliably discriminate between two systems. The algorithm can be used to incrementally design retrieval systems by simultaneously comparing sets of systems. The number of additional judgments needed after each incremental design change decreases at a rate reciprocal to the number of systems being compared. To demonstrate the eﬀectiveness of our method, we evaluate TREC ad hoc submissions, showing that with 95\% fewer relevance judgments we can reach a Kendall’s tau rank correlation of at least 0.9.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 14th {ACM} international conference on {Information} and knowledge management  - {CIKM} '05},
	publisher = {ACM Press},
	author = {Carterette, Ben and Allan, James},
	year = {2005},
	pages = {680}
}

@article{spoerri-using-2007,
	title = {Using the structure of overlap between search results to rank retrieval systems without relevance judgments},
	volume = {43},
	issn = {03064573},
	doi = {10.1016/j.ipm.2006.09.009},
	abstract = {This paper addresses the problem of how to rank retrieval systems without the need for human relevance judgments, which are very resource intensive to obtain. Using TREC 3, 6, 7 and 8 data, it is shown how the overlap structure between the search results of multiple systems can be used to infer relative performance diﬀerences. In particular, the overlap structures for random groupings of ﬁve systems are computed, so that each system is selected an equal number of times. It is shown that the average percentage of a system’s documents that are only found by it and no other systems is strongly and negatively correlated with its retrieval performance eﬀectiveness, such as its mean average precision or precision at 1000. The presented method uses the degree of consensus or agreement a retrieval system can generate to infer its quality. This paper also addresses the question of how many documents in a ranked list need to be examined to be able to rank the systems. It is shown that the overlap structure of the top 50 documents can be used to rank the systems, often producing the best results. The presented method signiﬁcantly improves upon previous attempts to rank retrieval systems without the need for human relevance judgments. This ‘‘structure of overlap’’ method can be of value to communities that need to identify the best experts or rank them, but do not have the resources to evaluate the experts’ recommendations, since it does not require knowledge about the domain being searched or the information being requested.},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Spoerri, Anselm},
	month = jul,
	year = {2007},
	pages = {1059--1070}
}

@incollection{efron-using-2009,
	address = {Berlin, Heidelberg},
	title = {Using {Multiple} {Query} {Aspects} to {Build} {Test} {Collections} without {Human} {Relevance} {Judgments}},
	volume = {5478},
	isbn = {978-3-642-00957-0 978-3-642-00958-7},
	abstract = {Collecting relevance judgments (qrels) is an especially challenging part of building an information retrieval test collection. This paper presents a novel method for creating test collections by oﬀering a substitute for relevance judgments. Our method is based on an old idea in IR: a single information need can be represented by many query articulations. We call diﬀerent articulations of a particular need query aspects. By combining the top k documents retrieved by a single system for multiple query aspects, we build judgment-free qrels whose rank ordering of IR systems correlates highly with rankings based on human relevance judgments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Efron, Miles},
	editor = {Boughanem, Mohand and Berrut, Catherine and Mothe, Josiane and Soule-Dupuy, Chantal},
	year = {2009},
	doi = {10.1007/978-3-642-00958-7-26},
	pages = {276--287}
}

@inproceedings{hauff2010case,
  title={A case for automatic system evaluation},
  author={Hauff, Claudia and Hiemstra, Djoerd and Azzopardi, Leif and De Jong, Franciska},
  booktitle={European conference on information retrieval},
  pages={153--165},
  year={2010},
  organization={Springer}
}

@article{makary-using-nodate,
	title = {Using key phrases as new queries in building relevance judgments automatically},
	abstract = {We describe a new technique for building a relevance judgment list (qrels) for TREC test collections with no human intervention. For each TREC topic, a set of new queries is automatically generated from key phrases extracted from the top k documents retrieved from 12 different Terrier weighting models when the initial TREC topic is submitted. We assign a score to each key phrase based on its similarity to the original TREC topic. The key phrases with the highest scores become the new queries for a second search, this time using the Terrier BM25 weighting model. The union of the documents retrieved forms the automatically-build set of qrels.},
	language = {en},
	author = {Makary, Mireille and Oakes, Michael and Yamout, Fadi},
	pages = {2}
}

@article{makary-building-nodate,
	title = {{BUILDING} {A} {RELEVANCE} {JUDGMENT} {LIST} {WITH} {MINIMAL} {HUMAN} {INTERVENTION}},
	volume = {4},
	abstract = {This paper describes a new technique for building a relevance judgment list (qrels) for TREC test collections with minimal human intervention. We run twelve different Terrier weighting models using TREC topics. For each topic, we collect the common set of documents which were retrieved by all weighting models. Using the Keyphrase Extraction Algorithm (KEA) implemented as a plugin to GATE, we extract key phrases from each of the selected documents. Then, we assign a score to each key phrase based on the number of terms it shares with the original TREC topic. The key phrases with the highest scores become the queries for a second search, this time using the Terrier BM25 weighting model. The union of the documents retrieved forms the set of qrels for the original TREC query. We evaluate the relevance judgment list (qrels) obtained by ranking the twelve weighting models provided by Terrier, used for retrieval (surrogates for different retrieval systems) both with original TREC queries and with the qrels derived by our method, and finding the correlation between these rankings.},
	language = {en},
	number = {6},
	author = {Makary, Mireille},
	pages = {4}
}

@article{carterette-low-cost-2008,
	title = {Low-cost and robust evaluation of information retrieval systems},
	volume = {42},
	issn = {01635840},
	doi = {10.1145/1480506.1480527},
	language = {en},
	number = {2},
	urldate = {2018-10-21},
	journal = {ACM SIGIR Forum},
	author = {Carterette, Benjamin A.},
	month = nov,
	year = {2008},
	pages = {104}
}

@article{sakai-ranking-2010,
	title = {Ranking {Retrieval} {Systems} without {Relevance} {Assessments} – {Revisited}},
	abstract = {We re-examine the problem of ranking retrieval systems without relevance assessments in the context of collaborative evaluation forums such as TREC and NTCIR. The problem was ﬁrst tackled by Soboroﬀ, Nicholas and Cahan in 2001, using data from TRECs 3-8 [16]. Our long-term goal is to semi-automate repeated evaluation of search engines; our short-term goal is to provide NTCIR participants with a “system ranking forecast” prior to conducting manual relevance assessments, thereby reducing researchers’ idle time and accelerating research. Our extensive experiments using graded-relevance test collections from TREC and NTCIR compare several existing methods for ranking systems without relevance assessments. We show that (a) The simplest method of forming “pseudo-qrels” based on how many systems returned each pooled document performs as well as any other existing method; and that (b) the NTCIR system rankings tend to be easier to predict than the TREC robust track system rankings, and moreover, the NTCIR pseudoqrels yield fewer false alarms than the TREC pseudo-qrels do in statistical signiﬁcance testing. These diﬀerences between TREC and NTCIR may be because TREC sorts pooled documents by document IDs before relevance assessments, while NTCIR sorts them primarily by the number of systems that returned the document. However, we show that, even for the TREC robust data, documents returned by many systems are indeed more likely to be relevant than those returned by fewer systems.},
	language = {en},
	author = {Sakai, Tetsuya and Lin, Chin-Yew},
	year = {2010},
	pages = {9}
}

@article{moghadasi-low-cost-2013,
	title = {Low-cost evaluation techniques for information retrieval systems: {A} review},
	volume = {7},
	issn = {17511577},
	shorttitle = {Low-cost evaluation techniques for information retrieval systems},
	doi = {10.1016/j.joi.2012.12.001},
	language = {en},
	number = {2},
	urldate = {2018-10-21},
	journal = {Journal of Informetrics},
	author = {Moghadasi, Shiva Imani and Ravana, Sri Devi and Raman, Sudharshan N.},
	month = apr,
	year = {2013},
	pages = {301--312}
}

@inproceedings{sakai-boiling-2010,
 author = {Sakai, Tetsuya and Mitamura, Teruko},
 title = {Boiling Down Information Retrieval Test Collections},
 booktitle = {Adaptivity, Personalization and Fusion of Heterogeneous Information},
 series = {RIAO '10},
 year = {2010},
 location = {Paris, France},
 pages = {49--56},
 numpages = {8},
 acmid = {1937066},
 publisher = {LE CENTRE DE HAUTES ETUDES INTERNATIONALES D'INFORMATIQUE DOCUMENTAIRE},
 address = {Paris, France, France},
 keywords = {NTCIR, TREC, relevance assessment, test collection},
} 

@article{sakai-robustness-2009,
	title = {On the {Robustness} of {Information} {Retrieval} {Metrics} to {Biased} {Relevance} {Assessments}},
	volume = {17},
	issn = {1882-6652},
	doi = {10.2197/ipsjjip.17.156},
	language = {en},
	urldate = {2018-10-21},
	journal = {Journal of Information Processing},
	author = {Sakai, Tetsuya},
	year = {2009},
	pages = {156--166}
}


@incollection{baillie-retrieval-2007,
	address = {Berlin, Heidelberg},
	title = {A {Retrieval} {Evaluation} {Methodology} for {Incomplete} {Relevance} {Assessments}},
	volume = {4425},
	isbn = {978-3-540-71494-1 978-3-540-71496-5},
	abstract = {In this paper we a propose an extended methodology for laboratory based Information Retrieval evaluation under incomplete relevance assessments. This new protocol aims to identify potential uncertainty during system comparison that may result from incompleteness. We demonstrate how this methodology can lead towards a ﬁner grained analysis of systems. This is advantageous, because the detection of uncertainty during the evaluation process can guide and direct researchers when evaluating new systems over existing and future test collections.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Baillie, Mark and Azzopardi, Leif and Ruthven, Ian},
	editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
	year = {2007},
	doi = {10.1007/978-3-540-71496-5-26},
	pages = {271--282}
}

@article{li-evaluating-nodate,
	title = {Evaluating {Information} {Retrieval} {Systems} {With} {Multiple} {Non}-{Expert} {Assessors}},
	language = {en},
	author = {Li, Le},
	pages = {95}
}

@inproceedings{carterette-low-2010,
	address = {Geneva, Switzerland},
	title = {Low cost evaluation in information retrieval},
	isbn = {978-1-4503-0153-4},
	doi = {10.1145/1835449.1835675},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceeding of the 33rd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '10},
	publisher = {ACM Press},
	author = {Carterette, Ben and Kanoulas, Evangelos and Yilmaz, Emine},
	year = {2010},
	pages = {903}
}

@inproceedings{cormack-beyond-2018,
	address = {Ann Arbor, MI, USA},
	title = {Beyond {Pooling}},
	isbn = {978-1-4503-5657-2},
	doi = {10.1145/3209978.3210119},
	abstract = {Dynamic Sampling is a novel, non-uniform, statistical sampling strategy in which documents are selected for relevance assessment based on the results of prior assessments. Unlike static and dynamic pooling methods that are commonly used to compile relevance assessments for the creation of information retrieval test collections, Dynamic Sampling yields a statistical sample from which substantially unbiased estimates of effectiveness measures may be derived. In contrast to static sampling strategies, which make no use of relevance assessments, Dynamic Sampling is able to select documents from a much larger universe, yielding superior test collections for a given budget of relevance assessments. These assertions are supported by simulation studies using secondary data from the TREC 2017 Common Core Track.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}  - {SIGIR} '18},
	publisher = {ACM Press},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2018},
	keywords = {cormack},
	pages = {1169--1172}
}

@incollection{mcdonald-active-2018,
	address = {Cham},
	title = {Active {Learning} {Strategies} for {Technology} {Assisted} {Sensitivity} {Review}},
	volume = {10772},
	isbn = {978-3-319-76940-0 978-3-319-76941-7},
	abstract = {Government documents must be reviewed to identify and protect any sensitive information, such as personal information, before the documents can be released to the public. However, in the era of digital government documents, such as e-mail, traditional sensitivity review procedures are no longer practical, for example due to the volume of documents to be reviewed. Therefore, there is a need for new technology assisted review protocols to integrate automatic sensitivity classiﬁcation into the sensitivity review process. Moreover, to eﬀectively assist sensitivity review, such assistive technologies must incorporate reviewer feedback to enable sensitivity classiﬁers to quickly learn and adapt to the sensitivities within a collection, when the types of sensitivity are not known a priori. In this work, we present a thorough evaluation of active learning strategies for sensitivity review. Moreover, we present an active learning strategy that integrates reviewer feedback, from sensitive text annotations, to identify features of sensitivity that enable us to learn an eﬀective sensitivity classiﬁer (0.7 Balanced Accuracy) using signiﬁcantly less reviewer eﬀort, according to the sign test (p {\textless} 0.01). Moreover, this approach results in a 51\% reduction in the number of documents required to be reviewed to achieve the same level of classiﬁcation accuracy, compared to when the approach is deployed without annotation features.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {McDonald, Graham and Macdonald, Craig and Ounis, Iadh},
	editor = {Pasi, Gabriella and Piwowarski, Benjamin and Azzopardi, Leif and Hanbury, Allan},
	year = {2018},
	doi = {10.1007/978-3-319-76941-7-33},
	pages = {439--453}
}

@inproceedings{grossman-automatic-2017,
	address = {Shinjuku, Tokyo, Japan},
	title = {Automatic and {Semi}-{Automatic} {Document} {Selection} for {Technology}-{Assisted} {Review}},
	isbn = {978-1-4503-5022-8},
	doi = {10.1145/3077136.3080675},
	abstract = {In the TREC Total Recall Track (2015-2016), participating teams could employ either fully automatic or human-assisted (“semi-automatic”) methods to select documents for relevance assessment by a simulated human reviewer. According to the TREC 2016 evaluation, the fully automatic baseline method achieved a recall-precision breakeven (“R-precision”) score of 0.71, while the two semi-automatic efforts achieved scores of 0.67 and 0.51. In this work, we investigate the extent to which the observed effectiveness of the different methods may be confounded by chance, by inconsistent adherence to the Track guidelines, by selection bias in the evaluation method, or by discordant relevance assessments. We find no evidence that any of these factors could yield relative effectiveness scores inconsistent with the official TREC 2016 ranking.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR} '17},
	publisher = {ACM Press},
	author = {Grossman, Maura R. and Cormack, Gordon V. and Roegiest, Adam},
	year = {2017},
	pages = {905--908}
}

@inproceedings{zou-technology-2018,
	address = {Ann Arbor, MI, USA},
	title = {Technology {Assisted} {Reviews}: {Finding} the {Last} {Few} {Relevant} {Documents} by {Asking} {Yes}/{No} {Questions} to {Reviewers}},
	isbn = {978-1-4503-5657-2},
	shorttitle = {Technology {Assisted} {Reviews}},
	doi = {10.1145/3209978.3210102},
	abstract = {The goal of a technology-assisted review is to achieve high recall with low human effort. Continuous active learning algorithms have demonstrated good performance in locating the majority of relevant documents in a collection, however their performance is reaching a plateau when 80\%-90\% of them has been found. Finding the last few relevant documents typically requires exhaustively reviewing the collection. In this paper, we propose a novel method to identify these last few, but significant, documents efficiently. Our method makes the hypothesis that entities carry vital information in documents, and that reviewers can answer questions about the presence or absence of an entity in the missing relevance documents. Based on this we devise a sequential Bayesian search method that selects the optimal sequence of questions to ask. The experimental results show that our proposed method can greatly improve performance requiring less reviewing effort.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}  - {SIGIR} '18},
	publisher = {ACM Press},
	author = {Zou, Jie and Li, Dan and Kanoulas, Evangelos},
	year = {2018},
	pages = {949--952}
}

@incollection{harris-hybrid-2014,
	address = {Cham},
	title = {Hybrid {Crowd}-{Machine} {Methods} as {Alternatives} to {Pooling} and {Expert} {Judgments}},
	volume = {8870},
	isbn = {978-3-319-12843-6 978-3-319-12844-3},
	abstract = {Pooling is a document sampling strategy commonly used to collect relevance judgments when multiple retrieval/ranking algorithms are involved. A fixed number of top ranking documents from each algorithm form a pool. Traditionally, expensive experts judge the pool of documents for relevance. We propose and test two hybrid algorithms as alternatives that reduce assessment costs and are effective. The machine part selects documents to judge from the full set of retrieved documents. The human part uses inexpensive crowd workers to make judgments. We present a clustered and a non-clustered approach for document selection and two experiments testing our algorithms. The first is designed to be statistically robust, controlling for variations across crowd workers, collections, domains and topics. The second is designed along natural lines and investigates more topics. Our results demonstrate high quality can be achieved and at low cost. Moreover, this can be done by judging far fewer documents than with pooling. Precision, recall, F-scores and LAM are very strong, indicating that our algorithms with crowd sourcing offer viable alternatives to collecting judgments via pooling with expert assessments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Information {Retrieval} {Technology}},
	publisher = {Springer International Publishing},
	author = {Harris, Christopher G. and Srinivasan, Padmini},
	editor = {Jaafar, Azizah and Mohamad Ali, Nazlena and Mohd Noah, Shahrul Azman and Smeaton, Alan F. and Bruza, Peter and Bakar, Zainab Abu and Jamil, Nursuriati and Sembok, Tengku Mohd Tengku},
	year = {2014},
	doi = {10.1007/978-3-319-12844-3-6},
	pages = {60--72}
}

@inproceedings{sakai-alternatives-2007,
	address = {Amsterdam, The Netherlands},
	title = {Alternatives to {Bpref}},
	isbn = {978-1-59593-597-7},
    doi = {10.1145/1277741.1277756},
	abstract = {Recently, a number of TREC tracks have adopted a retrieval eﬀectiveness metric called bpref which has been designed for evaluation environments with incomplete relevance data. A graded-relevance version of this metric called rpref has also been proposed. However, we show that the application of Qmeasure, normalised Discounted Cumulative Gain (nDCG) or Average Precision (AveP) to condensed lists, obtained by ﬁltering out all unjudged documents from the original ranked lists, is actually a better solution to the incompleteness problem than bpref. Furthermore, we show that the use of graded relevance boosts the robustness of IR evaluation to incompleteness and therefore that Q-measure and nDCG based on condensed lists are the best choices. To this end, we use four graded-relevance test collections from NTCIR to compare ten diﬀerent IR metrics in terms of system ranking stability and pairwise discriminative power.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Sakai, Tetsuya},
	year = {2007},
	pages = {71}
}

@inproceedings{carterette-evaluation-2008,
	address = {Singapore, Singapore},
	title = {Evaluation over thousands of queries},
	isbn = {978-1-60558-164-4},
	doi = {10.1145/1390334.1390445},
	abstract = {Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness. There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available. In light of this, it should be possible to evaluate over many more queries without much more total judging eﬀort. The Million Query Track at TREC 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries. We present results of the track, along with deeper analysis: investigating tradeoﬀs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more costeﬀective and as reliable as fewer queries with more judgments. Total assessor eﬀort can be reduced by 95\% with no appreciable increase in evaluation errors.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '08},
	publisher = {ACM Press},
	author = {Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A. and Allan, James},
	year = {2008},
	pages = {651}
}

@article{alonso-using-2012,
	title = {Using crowdsourcing for {TREC} relevance assessment},
	volume = {48},
	issn = {03064573},
	doi = {10.1016/j.ipm.2012.01.004},
	abstract = {Crowdsourcing has recently gained a lot of attention as a tool for conducting different kinds of relevance evaluations. At a very high level, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee. This crowdsourcing approach makes possible to conduct information retrieval experiments extremely fast, with good results at a low cost.},
	language = {en},
	number = {6},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Alonso, Omar and Mizzaro, Stefano},
	month = nov,
	year = {2012},
	pages = {1053--1066}
}

@inproceedings{carterette-effect-2010,
	address = {Geneva, Switzerland},
	title = {The effect of assessor error on {IR} system evaluation},
	isbn = {978-1-4503-0153-4},
	doi = {10.1145/1835449.1835540},
	abstract = {Recent eﬀorts in test collection building have focused on scaling back the number of necessary relevance judgments and then scaling up the number of search topics. Since the largest source of variation in a Cranﬁeld-style experiment comes from the topics, this is a reasonable approach. However, as topic set sizes grow, and researchers look to crowdsourcing and Amazon’s Mechanical Turk to collect relevance judgments, we are faced with issues of quality control. This paper examines the robustness of the TREC Million Query track methods when some assessors make signiﬁcant and systematic errors. We ﬁnd that while averages are robust, assessor errors can have a large eﬀect on system rankings.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceeding of the 33rd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '10},
	publisher = {ACM Press},
	author = {Carterette, Ben and Soboroff, Ian},
	year = {2010},
	pages = {539}
}

@techreport{allan-million-2007,
	address = {Fort Belvoir, VA},
	title = {Million {Query} {Track} 2007 {Overview}:},
	shorttitle = {Million {Query} {Track} 2007 {Overview}},
	language = {en},
	urldate = {2018-10-21},
	institution = {Defense Technical Information Center},
	author = {Allan, James and Carterette, Ben and Aslam, Javed A. and Pavlu, Virgil and Dachev, Blagovest and Kanoulas, Evangelos},
	month = jan,
	year = {2007},
	doi = {10.21236/ADA477388}
}

@incollection{hosseini-aggregating-2012,
	address = {Berlin, Heidelberg},
	title = {On {Aggregating} {Labels} from {Multiple} {Crowd} {Workers} to {Infer} {Relevance} of {Documents}},
	volume = {7224},
	isbn = {978-3-642-28996-5 978-3-642-28997-2},
	abstract = {We consider the problem of acquiring relevance judgements for information retrieval (IR) test collections through crowdsourcing when no true relevance labels are available. We collect multiple, possibly noisy relevance labels per document from workers of unknown labelling accuracy. We use these labels to infer the document relevance based on two methods. The first method is the commonly used majority voting (MV) which determines the document relevance based on the label that received the most votes, treating all the workers equally. The second is a probabilistic model that concurrently estimates the document relevance and the workers accuracy using expectation maximization (EM). We run simulations and conduct experiments with crowdsourced relevance labels from the INEX 2010 Book Search track to investigate the accuracy and robustness of the relevance assessments to the noisy labels. We observe the effect of the derived relevance judgments on the ranking of the search systems. Our experimental results show that the EM method outperforms the MV method in the accuracy of relevance assessments and IR systems ranking. The performance improvements are especially noticeable when the number of labels per document is small and the labels are of varied quality.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hosseini, Mehdi and Cox, Ingemar J. and Milić-Frayling, Nataša and Kazai, Gabriella and Vinay, Vishwa},
	editor = {Baeza-Yates, Ricardo and de Vries, Arjen P. and Zaragoza, Hugo and Cambazoglu, B. Barla and Murdock, Vanessa and Lempel, Ronny and Silvestri, Fabrizio and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	doi = {10.1007/978-3-642-28997-2-16},
	pages = {182--194}
}

@article{oard-evaluation-2010,
	title = {Evaluation of information retrieval for {E}-discovery},
	volume = {18},
	issn = {0924-8463, 1572-8382},
	doi = {10.1007/s10506-010-9093-9},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {Artificial Intelligence and Law},
	author = {Oard, Douglas W. and Baron, Jason R. and Hedin, Bruce and Lewis, David D. and Tomlinson, Stephen},
	month = dec,
	year = {2010},
	pages = {347--386}
}

@inproceedings{voorhees-topic-2009,
	address = {Boston, MA, USA},
	title = {Topic set size redux},
	isbn = {978-1-60558-483-6},
	doi = {10.1145/1571941.1572138},
	abstract = {The cost as well as the power and reliability of a retrieval test collection are all proportional to the number of topics included in it. Test collections created through community evaluations such as TREC generally use 50 topics. Prior work estimated the reliability of 50-topic sets by extrapolating conﬁdence levels from those of smaller sets, and concluded that 50 topics are suﬃcient to have high conﬁdence in a comparison, especially when the comparison is statistically signiﬁcant. Using topic sets that actually contain 50 topics, this paper shows that statistically signiﬁcant diﬀerences can be wrong, even when statistical signiﬁcance is accompanied by moderately large ({\textgreater}10\%) relative diﬀerences in scores. Further, using standardized evaluation scores rather than raw evaluation scores does not increase the reliability of these paired comparisons. Researchers should continue to be skeptical of conclusions demonstrated on only a single test collection.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Voorhees},
	year = {2009},
	pages = {806}
}

@incollection{carterette-if-2009,
	address = {Berlin, Heidelberg},
	title = {If {I} {Had} a {Million} {Queries}},
	volume = {5478},
	isbn = {978-3-642-00957-0 978-3-642-00958-7},
	abstract = {As document collections grow larger, the information needs and relevance judgments in a test collection must be well-chosen within a limited budget to give the most reliable and robust evaluation results. In this work we analyze a sample of queries categorized by length and corpus-appropriateness to determine the right proportion needed to distinguish between systems. We also analyze the appropriate division of labor between developing topics and making relevance judgments, and show that only a small, biased sample of queries with sparse judgments is needed to produce the same results as a much larger sample of queries.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A. and Allan, James},
	editor = {Boughanem, Mohand and Berrut, Catherine and Mothe, Josiane and Soule-Dupuy, Chantal},
	year = {2009},
	doi = {10.1007/978-3-642-00958-7-27},
	pages = {288--300}
}

@inproceedings{pavlu-ir-2012,
	address = {Seattle, Washington, USA},
	title = {{IR} system evaluation using nugget-based test collections},
	isbn = {978-1-4503-0747-5},
	doi = {10.1145/2124295.2124343},
	abstract = {The development of information retrieval systems such as search engines relies on good test collections, including assessments of retrieved content. The widely employed “Cranﬁeld paradigm” dictates that the information relevant to a topic be encoded at the level of documents, therefore requiring eﬀectively complete document relevance assessments. As this is no longer practical for modern corpora, numerous problems arise, including scalability, reusability, and applicability. We propose a new method for relevance assessment based on relevant information, not relevant documents. Once the relevant “nuggets” are collected, our matching method [23] can assess any document for relevance with high accuracy, and so any retrieved list of documents can be assessed for performance. In this paper we analyze the performance of the matching function by looking at speciﬁc cases and by comparing with other methods. We then show how these inferred relevance assessments can be used to perform IR system evaluation, and we discuss in particular reusability and scalability. Our main contribution is a methodology for producing test collections that are highly accurate, more complete, scalable, reusable, and can be generated with similar amounts of eﬀort as existing methods, with great potential for future applications.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the fifth {ACM} international conference on {Web} search and data mining - {WSDM} '12},
	publisher = {ACM Press},
	author = {Pavlu, Virgil and Rajput, Shahzad and Golbus, Peter B. and Aslam, Javed A.},
	year = {2012},
	pages = {393}
}

@inproceedings{webber-score-2009,
	address = {Boston, MA, USA},
	title = {Score adjustment for correction of pooling bias},
	isbn = {978-1-60558-483-6},
    doi = {10.1145/1571941.1572018},
	abstract = {Information retrieval systems are evaluated against test collections of topics, documents, and assessments of which documents are relevant to which topics. Documents are chosen for relevance assessment by pooling runs from a set of existing systems. New systems can return unassessed documents, leading to an evaluation bias against them. In this paper, we propose to estimate the degree of bias against an unpooled system, and to adjust the system’s score accordingly. Bias estimation can be done via leave-one-out experiments on the existing, pooled systems, but this requires the problematic assumption that the new system is similar to the existing ones. Instead, we propose that all systems, new and pooled, be fully assessed against a common set of topics, and the bias observed against the new system on the common topics be used to adjust scores on the existing topics. We demonstrate using resampling experiments on TREC test sets that our method leads to a marked reduction in error, even with only a relatively small number of common topics, and that the error decreases as the number of topics increases.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Webber, William and Park, Laurence A. F.},
	year = {2009},
	pages = {444}
}

@inproceedings{smucker-agreement-2009,
	address = {Boston, MA, USA},
	title = {Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes},
	isbn = {978-1-60558-483-6},
   doi = {10.1145/1571941.1572050},
	abstract = {Research has shown that little practical diﬀerence exists between the randomization, Student’s paired t, and bootstrap tests of statistical signiﬁcance for TREC ad-hoc retrieval experiments with 50 topics. We compared these three tests on runs with topic sizes down to 10 topics. We found that these tests show increasing disagreement as the number of topics decreases. At smaller numbers of topics, the randomization test tended to produce smaller p-values than the t-test for p-values less than 0.1. The bootstrap exhibited a systematic bias towards p-values strictly less than the t-test with this bias increasing as the number of topics decreased. We recommend the use of the randomization test although the t-test appears to be suitable even when the number of topics is small.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Smucker, Mark D. and Allan, James and Carterette, Ben},
	year = {2009},
	pages = {630}
}

@inproceedings{yilmaz-deep-2009,
	address = {Boston, MA, USA},
	title = {Deep versus shallow judgments in learning to rank},
	isbn = {978-1-60558-483-6},
	doi = {10.1145/1571941.1572066},
	abstract = {Much research in learning to rank has been placed on developing sophisticated learning methods, treating the training set as a given. However, the number of judgments in the training set directly aﬀects the quality of the learned system. Given the expense of obtaining relevance judgments for constructing training data, one often has a limited budget in terms of how many judgments he can get. The major problem then is how to distribute this judgment eﬀort across diﬀerent queries. In this paper, we investigate the tradeoﬀ between the number of queries and the number of judgments per query when training sets are constructed. In particular, we show that up to a limit, training sets with more queries but shallow (less) judgments per query are more cost effective than training sets with less queries but deep (more) judgments per query.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Yilmaz, Emine and Robertson, Stephen},
	year = {2009},
	pages = {662}
}



@article{sakai-topic-2016,
	title = {Topic set size design},
	volume = {19},
	issn = {1386-4564, 1573-7659},
	doi = {10.1007/s10791-015-9273-z},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Information Retrieval Journal},
	author = {Sakai, Tetsuya},
	month = jun,
	year = {2016},
	pages = {256--283}
}

@inproceedings{berendsen-pseudo-2013,
	address = {Dublin, Ireland},
	title = {Pseudo test collections for training and tuning microblog rankers},
	isbn = {978-1-4503-2034-4},
	doi = {10.1145/2484028.2484063},
	abstract = {Recent years have witnessed a persistent interest in generating pseudo test collections, both for training and evaluation purposes. We describe a method for generating queries and relevance judgments for microblog search in an unsupervised way. Our starting point is this intuition: tweets with a hashtag are relevant to the topic covered by the hashtag and hence to a suitable query derived from the hashtag. Our baseline method selects all commonly used hashtags, and all associated tweets as relevance judgments; we then generate a query from these tweets. Next, we generate a timestamp for each query, allowing us to use temporal information in the training process. We then enrich the generation process with knowledge derived from an editorial test collection for microblog search.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 36th international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '13},
	publisher = {ACM Press},
	author = {Berendsen, Richard and Tsagkias, Manos and Weerkamp, Wouter and de Rijke, Maarten},
	year = {2013},
	pages = {53}
}

@inproceedings{sakai-designing-2014,
	address = {Shanghai, China},
	title = {Designing {Test} {Collections} for {Comparing} {Many} {Systems}},
	isbn = {978-1-4503-2598-1},
	doi = {10.1145/2661829.2661893},
	abstract = {A researcher decides to build a test collection for comparing her new information retrieval (IR) systems with several state-of-theart baselines. She wants to know the number of topics (n) she needs to create in advance, so that she can start looking for (say) a query log large enough for sampling n good topics, and estimating the relevance assessment cost. We provide practical solutions to researchers like her using power analysis and sample size design techniques, and demonstrate its usefulness for several IR tasks and evaluation measures. We consider not only the paired t-test but also one-way analysis of variance (ANOVA) for signiﬁcance testing to accommodate comparison of m(≥ 2) systems under a given set of statistical requirements (α: the Type I error rate, β: the Type II error rate, and minD: the minimum detectable difference between the best and the worst systems). Using our simple Excel tools and some pooled variance estimates from past data, researchers can design statistically well-designed test collections. We demonstrate that, as different evaluation measures have different variances across topics, they inevitably require different topic set sizes. This suggests that the evaluation measures should be chosen at the test collection design phase. Moreover, through a pool depth reduction experiment with past data, we show how the relevance assessment cost can be reduced dramatically while freezing the set of statistical requirements. Based on the cost analysis and the available budget, researchers can determine the right balance betweeen n and the pool depth pd . Our techniques and tools are applicable to test collections for non-IR tasks as well.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 23rd {ACM} {International} {Conference} on {Conference} on {Information} and {Knowledge} {Management} - {CIKM} '14},
	publisher = {ACM Press},
	author = {Sakai, Tetsuya},
	year = {2014},
	pages = {61--70}
}

@inproceedings{lipani-splitting-2015,
	address = {Santiago, Chile},
	title = {Splitting {Water}: {Precision} and {Anti}-{Precision} to {Reduce} {Pool} {Bias}},
	isbn = {978-1-4503-3621-5},
	shorttitle = {Splitting {Water}},
	doi = {10.1145/2766462.2767749},
	abstract = {For many tasks in evaluation campaigns, especially those modeling narrow domain-speciﬁc challenges, lack of participation leads to a potential pooling bias due to the scarce number of pooled runs. It is well known that the reliability of a test collection is proportional to the number of topics and relevance assessments provided for each topic, but also to same extent to the diversity in participation in the challenges. Hence, in this paper we present a new perspective in reducing the pool bias by studying the eﬀect of merging an unpooled run with the pooled runs. We also introduce an indicator used by the bias correction method to decide whether the correction needs to be applied or not. This indicator gives strong clues about the potential of a “good” run tested on an “unfriendly” test collection (i.e. a collection where the pool was contributed to by runs very diﬀerent from the one at hand). We demonstrate the correctness of our method on a set of ﬁfteen test collections from the Text REtrieval Conference (TREC). We observe a reduction in system ranking error and absolute score diﬀerence error.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '15},
	publisher = {ACM Press},
	author = {Lipani, Aldo and Lupu, Mihai and Hanbury, Allan},
	year = {2015},
	pages = {103--112}
}

@incollection{hosseini-selecting-2011,
	address = {Berlin, Heidelberg},
	title = {Selecting a {Subset} of {Queries} for {Acquisition} of {Further} {Relevance} {Judgements}},
	volume = {6931},
	isbn = {978-3-642-23317-3 978-3-642-23318-0},
	abstract = {Assessing the relative performance of search systems requires the use of a test collection with a pre-deﬁned set of queries and corresponding relevance assessments. The state-of-the-art process of constructing test collections involves using a large number of queries and selecting a set of documents, submitted by a group of participating systems, to be judged per query. However, the initial set of judgments may be insuﬃcient to reliably evaluate the performance of future as yet unseen systems. In this paper, we propose a method that expands the set of relevance judgments as new systems are being evaluated. We assume that there is a limited budget to build additional relevance judgements. From the documents retrieved by the new systems we create a pool of unjudged documents. Rather than uniformly distributing the budget across all queries, we ﬁrst select a subset of queries that are eﬀective in evaluating systems and then uniformly allocate the budget only across these queries. Experimental results on TREC 2004 Robust track test collection demonstrate the superiority of this budget allocation strategy.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval} {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hosseini, Mehdi and Cox, Ingemar J. and Milic-Frayling, Natasa and Vinay, Vishwa and Sweeting, Trevor},
	editor = {Amati, Giambattista and Crestani, Fabio},
	year = {2011},
	doi = {10.1007/978-3-642-23318-0-12},
	pages = {113--124}
}

@inproceedings{hosseini-uncertainty-2012,
	address = {Portland, Oregon, USA},
	title = {An uncertainty-aware query selection model for evaluation of {IR} systems},
	isbn = {978-1-4503-1472-5},
	doi = {10.1145/2348283.2348403},
	abstract = {We propose a mathematical framework for query selection as a mechanism for reducing the cost of constructing information retrieval test collections. In particular, our mathematical formulation explicitly models the uncertainty in the retrieval eﬀectiveness metrics that is introduced by the absence of relevance judgments. Since the optimization problem is computationally intractable, we devise an adaptive query selection algorithm, referred to as Adaptive, that provides an approximate solution. Adaptive selects queries iteratively and assumes that no relevance judgments are available for the query under consideration. Once a query is selected, the associated relevance assessments are acquired and then used to aid the selection of subsequent queries. We demonstrate the eﬀectiveness of the algorithm on two TREC test collections as well as a test collection of an online search engine with 1000 queries. Our experimental results show that the queries chosen by Adaptive produce reliable performance ranking of systems. The ranking is better correlated with the actual systems ranking than the rankings produced by queries that were selected using the considered baseline methods.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 35th international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '12},
	publisher = {ACM Press},
	author = {Hosseini, Mehdi and Cox, Ingemar J. and Milic-Frayling, Natasa and Shokouhi, Milad and Yilmaz, Emine},
	year = {2012},
	pages = {901}
}

@inproceedings{kim-ir-2015,
	address = {Santiago, Chile},
	title = {{IR} {Evaluation}: {Designing} an {End}-to-{End} {Offline} {Evaluation} {Pipeline}},
	isbn = {978-1-4503-3621-5},
	shorttitle = {{IR} {Evaluation}},
	doi = {10.1145/2766462.2767875},
	abstract = {This tutorial aims to provide attendees with a detailed understanding of end-to-end evaluation pipeline based on human judgments (oﬄine measurement).},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '15},
	publisher = {ACM Press},
	author = {Kim, Jin Young and Yilmaz, Emine},
	year = {2015},
	pages = {1129--1132}
}







@incollection{hui-selective-2015,
	address = {Cham},
	title = {Selective {Labeling} and {Incomplete} {Label} {Mitigation} for {Low}-{Cost} {Evaluation}},
	volume = {9309},
	isbn = {978-3-319-23825-8 978-3-319-23826-5},
	abstract = {Information retrieval evaluation heavily relies on human eﬀort to assess the relevance of result documents. Recent years have seen eﬀorts and good progress to reduce the human eﬀort and thus lower the cost of evaluation. Selective labeling strategies carefully choose a subset of result documents to label, for instance, based on their aggregate rank in results; strategies to mitigate incomplete labels seek to make up for missing labels, for instance, predicting them using machine learning methods. How diﬀerent strategies interact, though, is unknown.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {String {Processing} and {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Hui, Kai and Berberich, Klaus},
	editor = {Iliopoulos, Costas and Puglisi, Simon and Yilmaz, Emine},
	year = {2015},
	doi = {10.1007/978-3-319-23826-5-14},
	pages = {137--148}
}

@inproceedings{raiber-identifying-2010,
	address = {Toronto, ON, Canada},
	title = {On identifying representative relevant documents},
	isbn = {978-1-4503-0099-5},
    doi = {10.1145/1871437.1871454},
	abstract = {Using relevance feedback can signiﬁcantly improve the effectiveness of ad hoc (query-based) retrieval. However, retrieval performance can signiﬁcantly vary with respect to the given set of relevant documents. Our goal is to establish a quantitative analysis of what makes a relevant document a good representative of the relevant-documents set regardless of the retrieval approach employed. That is, we would like to estimate the extent to which a relevant document can eﬀectively help in ﬁnding (other) relevant documents using some relevance-feedback method employed over the corpus. We present various representativeness estimates; some of which treat documents independently and some utilize inter-document similarities. Empirical evaluation shows that relevant documents that are centrally located within the similarity space of the relevant-documents set tend to be good representatives. In addition, we show that there exist highly representativeclusters of similar relevant documents, and devise methods for ranking clusters based on their presumed representativeness. Finally, we study the connection between representativeness and TREC’s gradual relevance judgments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 19th {ACM} international conference on {Information} and knowledge management - {CIKM} '10},
	publisher = {ACM Press},
	author = {Raiber, Fiana and Kurland, Oren},
	year = {2010},
	pages = {99}
}

@article{molla-document-2015,
	title = {Document {Distance} for the {Automated} {Expansion} of {Relevance} {Judgements} for {Information} {Retrieval} {Evaluation}},
	abstract = {This paper reports the use of a document distance-based approach to automatically expand the number of available relevance judgements when these are limited and reduced to only positive judgements. This may happen, for example, when the only available judgements are extracted from a list of references in a published review paper. We compare the results on two document sets: OHSUMED, based on medical research publications, and TREC-8, based on news feeds. We show that evaluations based on these expanded relevance judgements are more reliable than those using only the initially available judgements, especially when the number of available judgements is very limited.},
	language = {en},
	urldate = {2018-10-21},
	journal = {arXiv:1501.06380 [cs]},
	author = {Mollá, Diego and Amini, Iman and Martinez, David},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.06380}
}

@inproceedings{molla-towards-2013,
	address = {Brisbane, Queensland, Australia},
	title = {Towards information retrieval evaluation with reduced and only positive judgements},
	isbn = {978-1-4503-2524-0},
	doi = {10.1145/2537734.2537748},
	abstract = {This paper proposes a document distance-based approach to automatically expand the number of available relevance judgements when those are limited and reduced to only positive judgements. This may happen, for example, when the only available judgements are extracted from a list of references in a published clinical systematic review. We show that evaluations based on these expanded relevance judgements are more reliable than those using only the initially available judgements. We also show the impact of such an evaluation approach as the number of initial judgements decreases.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 18th {Australasian} {Document} {Computing} {Symposium} on - {ADCS} '13},
	publisher = {ACM Press},
	author = {Mollá, Diego and Martinez, David and Amini, Iman},
	year = {2013},
	pages = {109--112}
}

@article{hui-automatic-nodate,
	title = {Automatic {Methods} for {Low}-{Cost} {Evaluation} and {Position}-{Aware} {Models} for {Neural} {Information} {Retrieval}},
	language = {en},
	author = {Hui, Kai},
	pages = {144}
}

@inproceedings{moffat-strategic-2007,
	address = {Amsterdam, The Netherlands},
	title = {Strategic system comparisons via targeted relevance judgments},
	isbn = {978-1-59593-597-7},
    doi = {10.1145/1277741.1277806},
	abstract = {Relevance judgments are used to compare text retrieval systems. Given a collection of documents and queries, and a set of systems being compared, a standard approach to forming judgments is to manually examine all documents that are highly ranked by any of the systems. However, not all of these relevance judgments provide the same beneﬁt to the ﬁnal result, particularly if the aim is to identify which systems are best, rather than to fully order them. In this paper we propose new experimental methodologies that can signiﬁcantly reduce the volume of judgments required in system comparisons. Using rank-biased precision, a recently proposed effectiveness measure, we show that judging around 200 documents for each of 50 queries in a TREC-scale system evaluation containing over 100 runs is sufﬁcient to identify the best systems.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Moffat, Alistair and Webber, William and Zobel, Justin},
	year = {2007},
	pages = {375}
}

@inproceedings{robertson-new-2008,
	address = {Singapore, Singapore},
	title = {A new interpretation of average precision},
	isbn = {978-1-60558-164-4},
	doi = {10.1145/1390334.1390453},
	abstract = {We consider the question of whether Average Precision, as a measure of retrieval eﬀectiveness, can be regarded as deriving from a model of user searching behaviour. It turns out that indeed it can be so regarded, under a very simple stochastic model of user behaviour.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '08},
	publisher = {ACM Press},
	author = {Robertson, Stephen},
	year = {2008},
	keywords = {evaluation\-metrics},
	pages = {689}
}



@inproceedings{scholer-quantifying-2011,
	address = {Beijing, China},
	title = {Quantifying test collection quality based on the consistency of relevance judgements},
	isbn = {978-1-4503-0757-4},
	doi = {10.1145/2009916.2010057},
	abstract = {Relevance assessments are a key component for test collectionbased evaluation of information retrieval systems. This paper reports on a feature of such collections that is used as a form of ground truth data to allow analysis of human assessment error. A wide range of test collections are retrospectively examined to determine how accurately assessors judge the relevance of documents. Our results demonstrate a high level of inconsistency across the collections studied. The level of irregularity is shown to vary across topics, with some showing a very high level of assessment error.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on {Research} and development in {Information} - {SIGIR} '11},
	publisher = {ACM Press},
	author = {Scholer, Falk and Turpin, Andrew and Sanderson, Mark},
	year = {2011},
	pages = {1063}
}

@inproceedings{buckley-bias-2006,
	address = {Seattle, Washington, USA},
	title = {Bias and the limits of pooling},
	isbn = {978-1-59593-369-0},
	doi = {10.1145/1148170.1148284},
	abstract = {Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic. The idea behind pooling is to ﬁnd enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is suﬃciently complete and unbiased. As document sets grow larger, a constant-size pool represents an increasingly small percentage of the document set, and at some point the assumption of approximately complete judgments must become invalid. This paper demonstrates that the AQUAINT 2005 test collection exhibits bias caused by pools that were too shallow for the document set size despite having many diverse runs contribute to the pools. The existing judgment set favors relevant documents that contain topic title words even though relevant documents containing few topic title words are known to exist in the document set. The paper concludes with suggested modiﬁcations to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Buckley, Chris and Dimmick, Darrin and Soboroff, Ian and Voorhees},
	year = {2006},
	pages = {619}
}

@inproceedings{amitay-scaling-2004,
	address = {Sheffield, United Kingdom},
	title = {Scaling {IR}-system evaluation using term relevance sets},
	isbn = {978-1-58113-881-8},
	doi = {10.1145/1008992.1008997},
	abstract = {This paper describes an evaluation method based on Term Relevance Sets (Trels) that measures an IR system’s quality by examining the content of the retrieved results rather than by looking for pre-speciﬁed relevant pages. Trels consist of a list of terms believed to be relevant for a particular query as well as a list of irrelevant terms. The proposed method does not involve any document relevance judgments, and as such is not adversely aﬀected by changes to the underlying collection. Therefore, it can better scale to very large, dynamic collections such as the Web. Moreover, this method can evaluate a system’s eﬀectiveness on an updatable “live” collection, or on collections derived from diﬀerent data sources. Our experiments show that the proposed method is very highly correlated with oﬃcial TREC measures.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 27th annual international conference on {Research} and development in information retrieval  - {SIGIR} '04},
	publisher = {ACM Press},
	author = {Amitay, Einat and Carmel, David and Lempel, Ronny and Soffer, Aya},
	year = {2004},
	pages = {10}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={27},
  year={2011},
  publisher={Acm}
}

@article{allan2017trec,
  title={Trec 2017 common core track overview},
  author={Allan, James and Harman, Donna and Kanoulas, Evangelos and Li, Dan and Van Gysel, Christophe and Vorhees},
  year={2017}
}

@article{hastings1970monte,
  title={Monte Carlo sampling methods using Markov chains and their applications},
  author={Hastings, W Keith},
  year={1970},
  publisher={Oxford University Press}
}

@inproceedings{schnabel-unbiased-2016,
  title={Unbiased comparative evaluation of ranking functions},
  author={Schnabel, Tobias and Swaminathan, Adith and Frazier, Peter I and Joachims, Thorsten},
  booktitle={Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
  pages={109--118},
  year={2016},
  organization={ACM}
}

@inproceedings{aslam-inferring-2006,
	address = {Seattle, Washington, USA},
	title = {Inferring document relevance via average precision},
	isbn = {978-1-59593-369-0},
	doi = {10.1145/1148170.1148275},
	abstract = {We consider the problem of evaluating retrieval systems using a limited number of relevance judgments. Recent work has demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of documents. In this work, we demonstrate that given values or estimates of average precision, one can accurately infer the relevances of unjudged documents. Combined, we thus show how one can eﬃciently and accurately infer a large judged pool from a relatively small number of judged documents, thus permitting accurate and eﬃcient retrieval evaluation on a large scale.},
	language = {en},
	urldate = {2019-02-15},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Aslam, Javed A. and Yilmaz, Emine},
	year = {2006},
	pages = {601}
}



@article{aslam-practical-2007,
	title = {A {Practical} {Sampling} {Strategy} for {Efficient} {Retrieval} {Evaluation}},
	abstract = {We consider the problem of large-scale retrieval evaluation, with a focus on the considerable eﬀort required to judge tens of thousands of documents using traditional test collection construction methodologies. Recently, two methods based on random sampling were proposed to help alleviate this burden: While the ﬁrst method proposed by Aslam et al. is very accurate and eﬃcient, it is also very complex, and while the second method proposed by Yilmaz et al. is relatively simple, its accuracy and eﬃciency are signiﬁcantly lower than the former.},
	language = {en},
	author = {Aslam, Javed A and Pavlu, Virgil},
	keywords = {document\-selection, statAP, statistical},
	pages = {10},
	year = {2007},
}

@inproceedings{rajput-constructing-2012,
	address = {Maui, Hawaii, USA},
	title = {Constructing test collections by inferring document relevance via extracted relevant information},
	isbn = {978-1-4503-1156-4},
	doi = {10.1145/2396761.2396783},
	abstract = {The goal of a typical information retrieval system is to satisfy a user’s information need—e.g., by providing an answer or information “nugget”—while the actual search space of a typical information retrieval system consists of documents—i.e., collections of nuggets. In this paper, we characterize this relationship between nuggets and documents and discuss applications to system evaluation.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 21st {ACM} international conference on {Information} and knowledge management - {CIKM} '12},
	publisher = {ACM Press},
	author = {Rajput, Shahzad and Ekstrand-Abueg, Matthew and Pavlu, Virgil and Aslam, Javed A.},
	year = {2012},
	pages = {145}
}

@article{kleinberg1999authoritative,
  title={Authoritative sources in a hyperlinked environment},
  author={Kleinberg, Jon M},
  journal={Journal of the ACM (JACM)},
  volume={46},
  number={5},
  pages={604--632},
  year={1999},
  publisher={ACM}
}

@inproceedings{voorhees-effect-2002,
 author = {Voorhees and Buckley, Chris},
 title = {The Effect of Topic Set Size on Retrieval Experiment Error},
 booktitle = {Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '02},
 year = {2002},
 isbn = {1-58113-561-0},
 location = {Tampere, Finland},
 pages = {316--323},
 numpages = {8},
 doi = {10.1145/564376.564432},
 acmid = {564432},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {measurement error, test collections},
}

@inproceedings{sakai-evaluating-2006,
 author = {Sakai, Tetsuya},
 title = {Evaluating Evaluation Metrics Based on the Bootstrap},
 booktitle = {Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '06},
 year = {2006},
 isbn = {1-59593-369-7},
 location = {Seattle, Washington, USA},
 pages = {525--532},
 numpages = {8},
 doi = {10.1145/1148170.1148261},
 acmid = {1148261},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bootstrap, evaluation, graded relevance, test collection},
} 

@inproceedings{hauff-relying-2009,
  title={Relying on topic subsets for system ranking estimation},
  author={Hauff, Claudia and Hiemstra, Djoerd and De Jong, Franciska and Azzopardi, Leif},
  booktitle={Proceedings of the 18th ACM conference on Information and knowledge management},
  pages={1859--1862},
  year={2009},
  organization={ACM}
}

@article{li-short-2011,
  title={A short introduction to learning to rank},
  author={Li},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={94},
  number={10},
  pages={1854--1862},
  year={2011},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@inproceedings{zaidan-using-2007,
  title={Using “annotator rationales” to improve machine learning for text categorization},
  author={Zaidan, Omar and Eisner, Jason and Piatko, Christine},
  booktitle={Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics; proceedings of the main conference},
  pages={260--267},
  year={2007}
}

@book{sutton-introduction-1998,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G and others},
  volume={135},
  year={1998},
  publisher={MIT press Cambridge}
}

@inproceedings{Bailey-test-2016,
 author = {Bailey, Peter and Moffat, Alistair and Scholer, Falk and Thomas, Paul},
 title = {UQV100: A Test Collection with Query Variability},
 booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '16},
 year = {2016},
 isbn = {978-1-4503-4069-4},
 location = {Pisa, Italy},
 pages = {725--728},
 numpages = {4},
 doi = {10.1145/2911451.2914671},
 acmid = {2914671},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backstory, clueweb, information need, information retrieval, test collection, trec, user variability},
} 

@article{howe-rise-2006,
  title={The rise of crowdsourcing},
  author={Howe, Jeff},
  journal={Wired magazine},
  volume={14},
  number={6},
  pages={1--4},
  year={2006}
}

@article{sparck1976information,
  title={Information retrieval test collections},
  author={Sparck Jones, Karen and Van Rijsbergen, Cornelis Joost},
  journal={Journal of documentation},
  volume={32},
  number={1},
  pages={59--75},
  year={1976},
  publisher={MCB UP Ltd}
}

@inproceedings{cattelan-evaluation-2009,
  title={IR evaluation without a common set of topics},
  author={Cattelan, Matteo and Mizzaro, Stefano},
  booktitle={Conference on the Theory of Information Retrieval},
  pages={342--345},
  year={2009},
  organization={Springer}
}

@inproceedings{hauff-case-2010,
  title={A case for automatic system evaluation},
  author={Hauff, Claudia and Hiemstra, Djoerd and Azzopardi, Leif and De Jong, Franciska},
  booktitle={European conference on information retrieval},
  pages={153--165},
  year={2010},
  organization={Springer}
}

@inproceedings{robertson-contributions-2011,
  title={On the contributions of topics to system evaluation},
  author={Robertson, Stephen},
  booktitle={European conference on information retrieval},
  pages={129--140},
  year={2011},
  organization={Springer}
}

@incollection{carterette-overview-2011,
	address = {Berlin, Heidelberg},
	title = {Overview of {Information} {Retrieval} {Evaluation}},
	volume = {29},
	isbn = {978-3-642-19230-2 978-3-642-19231-9},
	abstract = {An important property of information retrieval (IR) system performance is its effectiveness at ﬁnding and ranking relevant documents in response to a user query. Research and development in IR requires rapid evaluation of effectiveness in order to test new approaches. This chapter covers the test collections required to evaluate effectiveness as well as traditional and newer measures of effectiveness.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Current {Challenges} in {Patent} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Carterette, Ben and Voorhees},
	editor = {Lupu, Mihai and Mayer, Katja and Tait, John and Trippe, Anthony J.},
	year = {2011},
	doi = {10.1007/978-3-642-19231-9-3},
	pages = {69--85}
}

@article{kutlu-mix-2018,
	title = {Mix and {Match}: {Collaborative} {Expert}-{Crowd} {Judging} for {Building} {Test} {Collections} {Accurately} and {Affordably}},
	shorttitle = {Mix and {Match}},
	abstract = {Crowdsourcing offers an affordable and scalable means to collect relevance judgments for IR test collections. However, crowd assessors may show higher variance in judgment quality than trusted assessors. In this paper, we investigate how to effectively utilize both groups of assessors in partnership. We specifically investigate how agreement in judging is correlated with three factors: relevance category, document rankings, and topical variance. Based on this, we then propose two collaborative judging methods in which a portion of the document-topic pairs are assessed by in-house judges while the rest are assessed by crowd-workers. Experiments conducted on two TREC collections show encouraging results when we distribute work intelligently between our two groups of assessors.},
	language = {en},
	urldate = {2018-10-21},
	journal = {arXiv:1806.00755 [cs]},
	author = {Kutlu, Mucahid and McDonnell, Tyler and Sheshadri, Aashish and Elsayed, Tamer and Lease, Matthew},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.00755}
}

@inproceedings{kutlu-crowd-2018,
	address = {Ann Arbor, MI, USA},
	title = {Crowd vs. {Expert}: {What} {Can} {Relevance} {Judgment} {Rationales} {Teach} {Us} {About} {Assessor} {Disagreement}?},
	isbn = {978-1-4503-5657-2},
	shorttitle = {Crowd vs. {Expert}},
	doi = {10.1145/3209978.3210033},
	abstract = {While crowdsourcing offers a low-cost, scalable way to collect relevance judgments, lack of transparency with remote crowd work has limited understanding about the quality of collected judgments. In prior work, we showed a variety of benefits from asking crowd workers to provide rationales for each relevance judgment [21]. In this work, we scale up our rationale-based judging design to assess its reliability on the 2014 TREC Web Track, collecting roughly 25K crowd judgments for 5K document-topic pairs. We also study having crowd judges perform topic-focused judging, rather than across topics, finding this improves quality. Overall, we show that crowd judgments can be used to reliably rank IR systems for evaluation. We further explore the potential of rationales to shed new light on reasons for judging disagreement between experts and crowd workers. Our qualitative and quantitative analysis distinguishes subjective vs. objective forms of disagreement, as well as the relative importance of each disagreement cause, and we present a new taxonomy for organizing the different types of disagreement we observe. We show that many crowd disagreements seem valid and plausible, with disagreement in many cases due to judging errors by the original TREC assessors. We also share our WebCrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}  - {SIGIR} '18},
	publisher = {ACM Press},
	author = {Kutlu, Mucahid and McDonnell, Tyler and Barkallah, Yassmine and Elsayed, Tamer and Lease, Matthew},
	year = {2018},
	pages = {805--814}
}

@inproceedings{aslam-inferring-2007,
	address = {Lisbon, Portugal},
	title = {Inferring document relevance from incomplete information},
	isbn = {978-1-59593-803-9},
    doi = {10.1145/1321440.1321529},
	abstract = {Recent work has shown that average precision can be accurately estimated from a small random sample of judged documents. Unfortunately, such “random pools” cannot be used to evaluate retrieval measures in any standard way. In this work, we show that given such estimates of average precision, one can accurately infer the relevances of the remaining unjudged documents, thus obtaining a fully judged pool that can be used in standard ways for system evaluation of all kinds. Using TREC data, we demonstrate that our inferred judged pools are well correlated with assessor judgments, and we further demonstrate that our inferred pools can be used to accurately infer precision recall curves and all commonly used measures of retrieval performance.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the sixteenth {ACM} conference on {Conference} on information and knowledge management  - {CIKM} '07},
	publisher = {ACM Press},
	author = {Aslam, Javed A. and Yilmaz, Emine},
	year = {2007},
	pages = {633}
}

@inproceedings{bompada-robustness-2007,
	address = {Amsterdam, The Netherlands},
	title = {On the robustness of relevance measures with incomplete judgments},
	isbn = {978-1-59593-597-7},
    doi = {10.1145/1277741.1277804},
	abstract = {We investigate the robustness of three widely-used IR relevance measures for large data collections with incomplete judgments. The relevance measures we consider are the bpref measure introduced by Buckley and Voorhees [7], the inferred average precision (infAP) introduced by Aslam and Yilmaz [4], and the normalized discounted cumulative gain (NDCG) measure introduced by J¨arvelin and Kek¨al¨ainen [8]. Our main results show that NDCG consistently performs better than both bpref and infAP. The experiments are performed on standard TREC datasets, under diﬀerent levels of incompleteness of judgments, and using two different evaluation methods, namely, the Kendall correlation measures order between system rankings and pairwise statistical signiﬁcance testing; the latter may be of independent interest.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Bompada, Tanuja and Chang, Chi-Chao and Chen, John and Kumar, Ravi and Shenoy, Rajesh},
	year = {2007},
	pages = {359}
}

@inproceedings{kamps-comparative-2009,
	address = {Barcelona, Spain},
	title = {Comparative analysis of clicks and judgments for {IR} evaluation},
	isbn = {978-1-60558-434-8},
	doi = {10.1145/1507509.1507522},
	abstract = {Queries and click-through data taken from search engine transaction logs is an attractive alternative to traditional test collections, due to its volume and the direct relation to end-user querying. The overall aim of this paper is to answer the question: How does click-through data diﬀer from explicit human relevance judgments in information retrieval evaluation? We compare a traditional test collection with manual judgments to transaction log based test collections—by using queries as topics and subsequent clicks as pseudorelevance judgments for the clicked results.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 2009 workshop on {Web} {Search} {Click} {Data} - {WSCD} '09},
	publisher = {ACM Press},
	author = {Kamps, Jaap and Koolen, Marijn and Trotman, Andrew},
	year = {2009},
	pages = {80--87}
}

@article{trotman-ir-nodate,
	title = {{IR} {Evaluation} {Using} {Multiple} {Assessors} per {Topic}},
	abstract = {Information retrieval test sets consist of three parts: documents, topics, and assessments. Assessments are time-consuming to generate. Even using pooling it took about 7 hours per topic to assess for INEX 2006.},
	language = {en},
	author = {Trotman, Andrew and Jenkinson, Dylan},
	pages = {8}
}

@incollection{wu-evaluation-2006,
author="Wu, Shengli
and McClean, Sally",
editor="Larsen, Henrik Legind
and Pasi, Gabriella
and Ortiz-Arroyo, Daniel
and Andreasen, Troels
and Christiansen, Henning",
title="Evaluation of System Measures for Incomplete Relevance Judgment in IR",
booktitle="Flexible Query Answering Systems",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="245--256",
abstract="Incomplete relevance judgment has become a norm for the evaluation of some major information retrieval evaluation events such as TREC, but its effect on some system measures has not been well understood. In this paper, we evaluate four system measures, namely mean average precision, R-precision, normalized average precision over all documents, and normalized discount cumulative gain, under incomplete relevance judgment. Among them, the measure of normalized average precision over all documents is introduced, and both mean average precision and R-precision are generalized for graded relevance judgment. These four measures have a common characteristic: complete relevance judgment is required for the calculation of their accurate values. We empirically investigate these measures through extensive experimentation of TREC data and aim to find the effect of incomplete relevance judgment on them. From these experiments, we conclude that incomplete relevance judgment affects all these four measures' values significantly. When using the pooling method in TREC, the more incomplete the relevance judgment is, the higher the values of all these measures usually become. We also conclude that mean average precision is the most sensitive but least reliable measure, normalized discount cumulative gain and normalized average precision over all documents are the most reliable but least sensitive measures, while R-precision is in the middle.",
isbn="978-3-540-34639-5"
}


@article{aslam-unified-2003,
 author = {Aslam, Javed A. and Pavlu, Virgiliu and Savell, Robert},
 title = {A Unified Model for Metasearch, Pooling, and System Evaluation},
 booktitle = {Proceedings of the Twelfth International Conference on Information and Knowledge Management},
 series = {CIKM '03},
 year = {2003},
 isbn = {1-58113-723-0},
 location = {New Orleans, LA, USA},
 pages = {484--491},
 numpages = {8},
 doi = {10.1145/956863.956953},
 acmid = {956953},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active learning, evaluation, metasearch, pooling},
} 
@article{can-automatic-2004,
	title = {Automatic performance evaluation of {Web} search engines},
	volume = {40},
	issn = {03064573},
	doi = {10.1016/S0306-4573(03)00040-2},
	abstract = {Measuring the information retrieval eﬀectiveness of World Wide Web search engines is costly because of human relevance judgments involved. However, both for business enterprises and people it is important to know the most eﬀective Web search engines, since such search engines help their users ﬁnd higher number of relevant Web pages with less eﬀort. Furthermore, this information can be used for several practical purposes. In this study we introduce automatic Web search engine evaluation method as an eﬃcient and eﬀective assessment tool of such systems. The experiments based on eight Web search engines, 25 queries, and binary user relevance judgments show that our method provides results consistent with human-based evaluations. It is shown that the observed consistencies are statistically signiﬁcant. This indicates that the new method can be successfully used in the evaluation of Web search engines.},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Can, Fazli and Nuray, Rabia and Sevdik, Ayisigi B.},
	month = may,
	year = {2004},
	pages = {495--514}
}

@inproceedings{cormack-efficient-1998,
	address = {Melbourne, Australia},
	title = {Efficient construction of large test collections},
	isbn = {978-1-58113-015-7},
	doi = {10.1145/290941.291009},
	abstract = {Test collections with a million or more documents are needed for the evaluation of modern information retrieval systems. Yet their construction requires a great deal of effort. Judgements must be rendered as to whether or not documents are relevant to each of a set of queries. Exhaustive judging, in which every document is examined and a judgement rendered, is infeasible for collections of this size. Current practice is represented by the “pooling method”, as used in the TREC conference series, in which only the first k documents from each of a number of sources are judged. We propose two methods, Intemctive Searching and Judging and Moveto-front Pooling, that yield effective test collections while requiring many fewer judgements. Interactive Searching and Judging selects documents to be judged using an interactive search system, and may be used by a small research team to develop an effective test collection using minimal resources. Move-to-Front Pooling directly improves on the standard pooling method by using a variable number of documents from each source depending on its retrieval performance. Move-to-Front Pooling would be an appropriate replacement for the standard pooling method in future collection development efforts involving many independent groups.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '98},
	publisher = {ACM Press},
	author = {Cormack, Gordon V. and Palmer, Christopher R. and Clarke, Charles L. A.},
	year = {1998},
	pages = {282--289}
}

@article{kuriyama-pooling-nodate,
	title = {Pooling for a {Large}-{Scale} {Test} {Collection}: {An} {Analysis} of the {Search} {Results} from the {First} {NTCIR} {Workshop}},
	abstract = {We have conducted a study to: (1) verify the exhaustiveness of pooling for the purpose of constructing a large-scale test collection, and (2) examine whether a difference in the number of pool documents can affect the relative evaluation of IR systems. We carried out the experiments using search topics, their relevance assessments, and the search results that were submitted for both the pre-test and test of the ﬁrst NTCIR Workshop.},
	language = {en},
	author = {Kuriyama, Kazuko},
	pages = {19}
}

@article{nuray-automatic-2006,
	title = {Automatic ranking of information retrieval systems using data fusion},
	volume = {42},
	issn = {03064573},
    doi = {10.1016/j.ipm.2005.03.023},
	abstract = {Measuring eﬀectiveness of information retrieval (IR) systems is essential for research and development and for monitoring search quality in dynamic environments. In this study, we employ new methods for automatic ranking of retrieval systems. In these methods, we merge the retrieval results of multiple systems using various data fusion algorithms, use the top-ranked documents in the merged result as the ‘‘(pseudo) relevant documents,’’ and employ these documents to evaluate and rank the systems. Experiments using Text REtrieval Conference (TREC) data provide statistically signiﬁcant strong correlations with human-based assessments of the same systems. We hypothesize that the selection of systems that would return documents diﬀerent from the majority could eliminate the ordinary systems from data fusion and provide better discrimination among the documents and systems. This could improve the eﬀectiveness of automatic ranking. Based on this intuition, we introduce a new method for the selection of systems to be used for data fusion. For this purpose, we use the bias concept that measures the deviation of a system from the norm or majority and employ the systems with higher bias in the data fusion process. This approach provides even higher correlations with the humanbased results. We demonstrate that our approach outperforms the previously proposed automatic ranking methods.},
	language = {en},
	number = {3},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Nuray, Rabia and Can, Fazli},
	month = may,
	year = {2006},
	pages = {595--614}
}

@inproceedings{lu-can-2017,
	address = {Shinjuku, Tokyo, Japan},
	title = {Can {Deep} {Effectiveness} {Metrics} {Be} {Evaluated} {Using} {Shallow} {Judgment} {Pools}?},
	isbn = {978-1-4503-5022-8},
	doi = {10.1145/3077136.3080793},
	abstract = {Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep e ectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 40th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}  - {SIGIR} '17},
	publisher = {ACM Press},
	author = {Lu, Xiaolu and Moffat, Alistair and Culpepper, J. Shane},
	year = {2017},
	pages = {35--44}
}

@article{robertson-history-2008,
	title = {On the history of evaluation in {IR}},
	volume = {34},
	issn = {0165-5515, 1741-6485},
	doi = {10.1177/0165551507086989},
	abstract = {This paper is a personal take on the history of evaluation experiments in information retrieval. It describes some of the early experiments that were formative in our understanding, and goes on to discuss the current dominance of TREC (the Text REtrieval Conference) and to assess its impact.},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {Journal of Information Science},
	author = {Robertson, Stephen},
	month = aug,
	year = {2008},
	pages = {439--456}
}

@inproceedings{soboroff-evaluating-2004,
	address = {Sheffield, United Kingdom},
	title = {On evaluating web search with very few relevant documents},
	isbn = {978-1-58113-881-8},
    doi = {10.1145/1008992.1009105},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 27th annual international conference on {Research} and development in information retrieval  - {SIGIR} '04},
	publisher = {ACM Press},
	author = {Soboroff, Ian},
	year = {2004},
	pages = {530}
}

@inproceedings{lipani-impact-2016,
	address = {Newark, Delaware, USA},
	title = {The {Impact} of {Fixed}-{Cost} {Pooling} {Strategies} on {Test} {Collection} {Bias}},
	isbn = {978-1-4503-4497-5},
	doi = {10.1145/2970398.2970429},
	abstract = {In Information Retrieval, test collections are usually built using the pooling method. Many pooling strategies have been developed for the pooling method. Herein, we address the question of identifying the best pooling strategy when evaluating systems using precision-oriented measures in presence of budget constraints on the number of documents to be evaluated. As a quality measurement we use the bias introduced by the pooling strategy, measured both in terms of Mean Absolute Error of the scores and in terms of ranking errors. Based on experiments on 15 test collections, we conclude that, for precision-oriented measures, the best strategies are based on Rank-Biased Precision (RBP). These results can inform collection builders because they suggest that, under ﬁxed assessment budget constraints, RBP-based sampling produces less biased pools than other alternatives.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 2016 {ACM} {International} {Conference} on the {Theory} of {Information} {Retrieval} - {ICTIR} '16},
	publisher = {ACM Press},
	author = {Lipani, Aldo and Zuccon, Guido and Lupu, Mihai and Koopman, Bevan and Hanbury, Allan},
	year = {2016},
	pages = {105--108}
}

@inproceedings{vu-machine-2006,
	address = {New York, New York},
	title = {A machine learning based approach to evaluating retrieval systems},
	doi = {10.3115/1220835.1220886},
	abstract = {Test collections are essential to evaluate Information Retrieval (IR) systems. The relevance assessment set has been recognized as the key bottleneck in test collection building, especially on very large sized document collections. This paper addresses the problem of eﬃciently selecting documents to be included in the assessment set. We will show how machine learning techniques can ﬁt this task. This leads to smaller pools than traditional round robin pooling, thus reduces signiﬁcantly the manual assessment workload. Experimental results on TREC collections1 consistently demonstrate the effectiveness of our approach according to diﬀerent evaluation criteria.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the main conference on {Human} {Language} {Technology} {Conference} of the {North} {American} {Chapter} of the {Association} of {Computational} {Linguistics}  -},
	publisher = {Association for Computational Linguistics},
	author = {Vu, Huyen-Trang and Gallinari, Patrick},
	year = {2006},
	pages = {399--406}
}

@inproceedings{bodoff-test-2007,
	address = {Amsterdam, The Netherlands},
	title = {Test theory for assessing {IR} test collections},
	isbn = {978-1-59593-597-7},
	doi = {10.1145/1277741.1277805},
	abstract = {How good is an IR test collection? A series of papers in recent years has addressed the question by empirically enumerating the consistency of performance comparisons using alternate subsets of the collection. In this paper we propose using Test Theory, which is based on analysis of variance and is specifically designed to assess test collections. Using the method, we not only can measure test reliability after the fact, but we can estimate the test collection’s reliability before it is even built or used. We can also determine an optimal allocation of resources before the fact, e.g. whether to invest in more judges or queries. The method, which is in widespread use in the field of educational testing, complements data-driven approaches to assessing test collections. Whereas the data-driven method focuses on test results, test theory focuses on test designs. It offers unique practical results, as well as insights about the variety and implications of alternative test designs.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Bodoff, David and Li, Pu},
	year = {2007},
	pages = {367}
}

@inproceedings{webber-statistical-2008,
	address = {Napa Valley, California, USA},
	title = {Statistical power in retrieval experimentation},
	isbn = {978-1-59593-991-3},
	doi = {10.1145/1458082.1458158},
	abstract = {The power of a statistical test speciﬁes the sample size required to reliably detect a given true effect. In IR evaluation, the power corresponds to the number of topics that are likely to be sufﬁcient to detect a certain degree of superiority of one system over another. To predict the power of a test, one must estimate the variability of the population being sampled from; here, of between-system score deltas. This paper demonstrates that basing such an estimation either on previous experience or on trial experiments leaves wide margins of error. Iteratively adding more topics to the test set until power is achieved is more efﬁcient; however, we show that it leads to a bias in favour of ﬁnding both power and signiﬁcance. A hybrid methodology is proposed, and the reporting requirements of the experimenter using this methodology are laid out. We also demonstrate that greater statistical power is achieved for the same relevance assessment effort by evaluating a large number of topics shallowly than a small number deeply.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceeding of the 17th {ACM} conference on {Information} and knowledge mining - {CIKM} '08},
	publisher = {ACM Press},
	author = {Webber, William and Moffat, Alistair and Zobel, Justin},
	year = {2008},
	pages = {571}
}

@article{voorhees-test-2008,
	title = {On test collections for adaptive information retrieval},
	volume = {44},
	issn = {03064573},
    doi = {10.1016/j.ipm.2007.12.011},
	abstract = {Traditional Cranﬁeld test collections represent an abstraction of a retrieval task that Sparck Jones calls the ‘‘core competency” of retrieval: a task that is necessary, but not suﬃcient, for user retrieval tasks. The abstraction facilitates research by controlling for (some) sources of variability, thus increasing the power of experiments that compare system eﬀectiveness while reducing their cost. However, even within the highly-abstracted case of the Cranﬁeld paradigm, meta-analysis demonstrates that the user/topic eﬀect is greater than the system eﬀect, so experiments must include a relatively large number of topics to distinguish systems’ eﬀectiveness. The evidence further suggests that changing the abstraction slightly to include just a bit more characterization of the user will result in a dramatic loss of power or increase in cost of retrieval experiments. Deﬁning a new, feasible abstraction for supporting adaptive IR research will require winnowing the list of all possible factors that can aﬀect retrieval behavior to a minimum number of essential factors.},
	language = {en},
	number = {6},
	urldate = {2018-10-21},
	journal = {Information Processing \& Management},
	author = {Voorhees},
	month = nov,
	year = {2008},
	pages = {1879--1885}
}

@article{carterette-here-nodate,
	title = {Here or {There}: {Preference} {Judgments} for {Relevance}},
	abstract = {Information retrieval systems have traditionally been evaluated over absolute judgments of relevance: each document is judged for relevance on its own, independent of other documents that may be on topic. We hypothesize that preference judgments of the form “document A is more relevant than document B” are easier for assessors to make than absolute judgments, and provide evidence for our hypothesis through a study with assessors. We then investigate methods to evaluate search engines using preference judgments. Furthermore, we show that by using inferences and clever selection of pairs to judge, we need not compare all pairs of documents in order to apply evaluation methods.},
	language = {en},
	author = {Carterette, Ben and Bennett, Paul N and Chickering, David Maxwell and Dumais, Susan T},
	pages = {13}
}

@article{zhu-analysis-2010,
	title = {An {Analysis} of {Assessor} {Behavior} in {Crowdsourced} {Preference} {Judgments}},
	abstract = {We describe a pilot study using Amazon’s Mechanical Turk to collect preference judgments between pairs of full-page layouts including both search results and image results. Speciﬁcally, we analyze the behavior of assessors that participated in our study to identify some patterns that may be broadly indicative of unreliable assessments. We believe this analysis can inform future experimental design and analysis when using crowdsourced human judgments.},
	language = {en},
	author = {Zhu, Dongqing and Carterette, Ben},
	year = {2010},
	pages = {6}
}

@article{shi-using-nodate,
	title = {Using {Clustering} to {Improve} {Retrieval} {Evaluation} without {Relevance} {Judgments}},
	abstract = {Retrieval evaluation without relevance judgments is a hard but also very meaningful work. In this paper, we use clustering technique to improve the performance of judgment free retrieval evaluation. By using one system to represent all the systems that are similar to it, we can largely reduce the negative effect of similar retrieval results in Retrieval evaluation. Experimental results demonstrated that our method outperformed all the previous judgment free evaluation methods significantly. Its overall average performance outperformed the best previous result by 20.5\%. Besides, our work is a general framework that can be applied to any other judgment free evaluation method for performance improvement.},
	language = {en},
	author = {Shi, Zhiwei and Li, Peng and Wang, Bin},
	pages = {9}
}

@inproceedings{berto-using-2013,
	address = {Copenhagen, Denmark},
	title = {On {Using} {Fewer} {Topics} in {Information} {Retrieval} {Evaluations}},
	isbn = {978-1-4503-2107-5},
	doi = {10.1145/2499178.2499184},
	abstract = {The possibility of using fewer topics in TREC, and in TREClike initiatives, has been studied recently, with encouraging results: even when decreasing consistently the number of topics (for example, using a topic subset of cardinality only 10, in place of the usual 50) it is possible, at least potentially, to obtain similar results when evaluating system effectiveness. However, the generality of this approach has been questioned, since the topic subset selected on one system population does not seem adequate to evaluate other systems. In this paper we reconsider that generality issue: we emphasize some limitations in the previous work and we show some experimental results that are instead more positive. The obtained results support the hypothesis that, by taking special care, the few topics selected on the basis of a given system population are also adequate to evaluate a di↵erent system population as well.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 2013 {Conference} on the {Theory} of {Information} {Retrieval} - {ICTIR} '13},
	publisher = {ACM Press},
	author = {Berto, Andrea and Mizzaro, Stefano and Robertson, Stephen},
	year = {2013},
	pages = {30--37}
}

@article{buckley-evaluating-2000,
 author = {Buckley, Chris and Voorhees},
 title = {Evaluating Evaluation Measure Stability},
 booktitle = {Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '00},
 year = {2000},
 isbn = {1-58113-226-3},
 location = {Athens, Greece},
 pages = {33--40},
 numpages = {8},
 doi = {10.1145/345508.345543},
 acmid = {345543},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{voorhees-variations-2000,
  title={Variations in relevance judgments and the measurement of retrieval effectiveness},
  author={Voorhees},
  journal={Information processing \& management},
  volume={36},
  number={5},
  pages={697--716},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{aslam-document-2009,
	address = {Boston, MA, USA},
	title = {Document selection methodologies for efficient and effective learning-to-rank},
	isbn = {978-1-60558-483-6},
    doi = {10.1145/1571941.1572022},
	abstract = {Learning-to-rank has attracted great attention in the IR community. Much thought and research has been placed on query-document feature extraction and development of sophisticated learning-to-rank algorithms. However, relatively little research has been conducted on selecting documents for learning-to-rank data sets nor on the eﬀect of these choices on the eﬃciency and eﬀectiveness of learning-to-rank algorithms.},
	language = {en},
	urldate = {2019-01-07},
	booktitle = {Proceedings of the 32nd international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '09},
	publisher = {ACM Press},
	author = {Aslam, Javed A. and Kanoulas, Evangelos and Pavlu, Virgil and Savev, Stefan and Yilmaz, Emine},
	year = {2009},
	pages = {468}
}


@incollection{roitero-easy-2017,
	address = {Cham},
	title = {Do {Easy} {Topics} {Predict} {Effectiveness} {Better} {Than} {Difficult} {Topics}?},
	volume = {10193},
	isbn = {978-3-319-56607-8 978-3-319-56608-5},
	abstract = {After a network-based analysis of TREC results, Mizzaro and Robertson [4] found the rather unpleasant result that topic ease (i.e., the average eﬀectiveness of the participating systems, measured with average precision) correlates with the ability of topics to predict system eﬀectiveness (deﬁned as topic hubness). We address this issue by: (i) performing a more detailed analysis, and (ii) using three diﬀerent datasets. Our results are threefold. First, we conﬁrm that the original result is indeed correct and general across datasets. Second, we show that, however, that result is less worrying than what might seem at ﬁrst glance, since it depends on considering the least eﬀective systems in the analysis. In other terms, easy topics discriminate most and least eﬀective systems, but when focussing on the most eﬀective systems only this is no longer true. Third, we also clarify what happens when using the GMAP metric.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer International Publishing},
	author = {Roitero, Kevin and Maddalena, Eddy and Mizzaro, Stefano},
	editor = {Jose, Joemon M and Hauff, Claudia and Altıngovde, Ismail Sengor and Song, Dawei and Albakour, Dyaa and Watt, Stuart and Tait, John},
	year = {2017},
	doi = {10.1007/978-3-319-56608-5-55},
	pages = {605--611}
}

@inproceedings{mizzaro-hits-2007,
	address = {Amsterdam, The Netherlands},
	title = {Hits hits {TREC}: exploring {IR} evaluation results with network analysis},
	isbn = {978-1-59593-597-7},
	shorttitle = {Hits hits {TREC}},
	doi = {10.1145/1277741.1277824},
	abstract = {We propose a novel method of analysing data gathered from TREC or similar information retrieval evaluation experiments. We deﬁne two normalized versions of average precision, that we use to construct a weighted bipartite graph of TREC systems and topics. We analyze the meaning of well known — and somewhat generalized — indicators from social network analysis on the Systems-Topics graph. We apply this method to an analysis of TREC 8 data; among the results, we ﬁnd that authority measures systems performance, that hubness of topics reveals that some topics are better than others at distinguishing more or less eﬀective systems, that with current measures a system that wants to be eﬀective in TREC needs to be eﬀective on easy topics, and that by using diﬀerent eﬀectiveness measures this is no longer the case.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '07},
	publisher = {ACM Press},
	author = {Mizzaro, Stefano and Robertson, Stephen},
	year = {2007},
	pages = {479}
}

@article{guiver-few-2009,
	title = {A few good topics: {Experiments} in topic set reduction for retrieval evaluation},
	volume = {27},
	issn = {10468188},
	shorttitle = {A few good topics},
	doi = {10.1145/1629096.1629099},
	language = {en},
	number = {4},
	urldate = {2018-10-21},
	journal = {ACM Transactions on Information Systems},
	author = {Guiver, John and Mizzaro, Stefano and Robertson, Stephen},
	month = nov,
	year = {2009},
	pages = {1--26}
}

@article{pal2011evaluation,
  title={Evaluation effort, reliability and reusability in XML retrieval},
  author={Pal, Sukomal and Mitra, Mandar and Kamps, Jaap},
  journal={Journal of the American Society for Information Science and Technology},
  volume={62},
  number={2},
  pages={375--394},
  year={2011},
  publisher={Wiley Online Library}
}

@article{banks-blind-1999,
	title = {Blind {Men} and {Elephants}: {Six} {Approaches} to {TREC} data},
	abstract = {The paper reviews six recent efforts to better understand performance measurements on information retrieval (IR) systems within the framework of the Text REtrieval Conferences (TREC): analysis of variance, cluster analyses, rank correlations, beadplots, multidimensional scaling, and item response analysis. None of this work has yielded any substantial new insights. Prospects that additional work along these lines will yield more interesting results vary but are in general not promising. Some suggestions are made for paying greater attention to richer descriptions of IR system behavior but within smaller, better controlled settings.},
	year = {1999},
	language = {en},
	author = {Banks, David and Over, Paul and Zhang, Nien-Fan},
	pages = {28}
}

@incollection{mizzaro-good-2008,
	address = {Berlin, Heidelberg},
	title = {The {Good}, the {Bad}, the {Difficult}, and the {Easy}: {Something} {Wrong} with {Information} {Retrieval} {Evaluation}?},
	volume = {4956},
	isbn = {978-3-540-78645-0 978-3-540-78646-7},
	shorttitle = {The {Good}, the {Bad}, the {Difficult}, and the {Easy}},
	abstract = {TREC-like evaluations do not consider topic ease and diﬃculty. However, it seems reasonable to reward good eﬀectiveness on diﬃcult topics more than good eﬀectiveness on easy topics, and to penalize bad eﬀectiveness on easy topics more than bad eﬀectiveness on diﬃcult topics. This paper shows how this approach leads to evaluation results that could be more reasonable, and that are diﬀerent to some extent. I provide a general analysis of this issue, propose a novel framework, and experimentally validate a part of it.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mizzaro, Stefano},
	editor = {Macdonald, Craig and Ounis, Iadh and Plachouras, Vassilis and Ruthven, Ian and White, Ryen},
	year = {2008},
	doi = {10.1007/978-3-540-78646-7-71},
	pages = {642--646}
}

@inproceedings{roitero-effectiveness-2018,
	address = {Ann Arbor, MI, USA},
	title = {Effectiveness {Evaluation} with a {Subset} of {Topics}: {A} {Practical} {Approach}},
	isbn = {978-1-4503-5657-2},
	shorttitle = {Effectiveness {Evaluation} with a {Subset} of {Topics}},
	doi = {10.1145/3209978.3210108},
	abstract = {Several researchers have proposed to reduce the number of topics used in TREC-like initiatives. One research direction that has been pursued is what is the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way. Such a research direction has been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. We propose such a practical criterion for topic selection: we rely on the methods for automatic system evaluation without relevance judgments, and by running some experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than random topics.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {The 41st {International} {ACM} {SIGIR} {Conference} on {Research} \& {Development} in {Information} {Retrieval}  - {SIGIR} '18},
	publisher = {ACM Press},
	author = {Roitero, Kevin and Soprano, Michael and Mizzaro, Stefano},
	year = {2018},
	pages = {1145--1148}
}

@article{samimi-creation-2014,
	title = {Creation of {Reliable} {Relevance} {Judgments} in {Information} {Retrieval} {Systems} {Evaluation} {Experimentation} through {Crowdsourcing}: {A} {Review}},
	volume = {2014},
	issn = {2356-6140, 1537-744X},
	shorttitle = {Creation of {Reliable} {Relevance} {Judgments} in {Information} {Retrieval} {Systems} {Evaluation} {Experimentation} through {Crowdsourcing}},
	doi = {10.1155/2014/135641},
	language = {en},
	urldate = {2018-10-21},
	journal = {The Scientific World Journal},
	author = {Samimi, Parnia and Ravana, Sri Devi},
	year = {2014},
	pages = {1--13}
}

@inproceedings{carterette-measuring-2010,
	address = {New York, New York, USA},
	title = {Measuring the reusability of test collections},
	isbn = {978-1-60558-889-6},
    doi = {10.1145/1718487.1718516},
	abstract = {While test collection construction is a time-consuming and expensive process, the true cost is amortized by reusing the collection over hundreds or thousands of experiments. Some of these experiments may involve systems that retrieve documents not judged during the initial construction phase, and some of these systems may be “hard” to evaluate: depending on which judgments are missing and which judged documents were retrieved, the experimenter’s conﬁdence in an evaluation could potentially be very low. We propose two methods for quantifying the reusability of a test collection for evaluating new systems. The proposed methods provide simple yet highly eﬀective tests for determining whether an existing set of judgments is useful for evaluating a new system. Empirical evaluations using TREC datasets conﬁrm the usefulness of our proposed reusability measures. In particular, we show that our methods can reliably estimate conﬁdence intervals that are indicative of collection reusability.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the third {ACM} international conference on {Web} search and data mining - {WSDM} '10},
	publisher = {ACM Press},
	author = {Carterette, Ben and Gabrilovich, Evgeniy and Josifovski, Vanja and Metzler, Donald},
	year = {2010},
	pages = {231}
}

@inproceedings{carterette-semiautomatic-2007,
	address = {Lisbon, Portugal},
	title = {Semiautomatic evaluation of retrieval systems using document similarities},
	isbn = {978-1-59593-803-9},
	doi = {10.1145/1321440.1321564},
	abstract = {Taking advantage of the well-known cluster hypothesis that “closely associated documents tend to be relevant to the same request”, we can use inter-document similarity to provide more accurate and robust evaluation of retrieval systems. Using our method, we are able to accurately rank retrieval systems with up to 99\% fewer relevance judgments than collected for the TREC conferences, and signiﬁcantly more accurately than other algorithms given the same number of judgments.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the sixteenth {ACM} conference on {Conference} on information and knowledge management  - {CIKM} '07},
	publisher = {ACM Press},
	author = {Carterette, Ben and Allan, James},
	year = {2007},
	pages = {873}
}

@inproceedings{verma-obtaining-2016,
	address = {San Francisco, California, USA},
	title = {On {Obtaining} {Effort} {Based} {Judgements} for {Information} {Retrieval}},
	isbn = {978-1-4503-3716-8},
	doi = {10.1145/2835776.2835840},
	abstract = {Document relevance has been the primary focus in the design, optimization and evaluation of retrieval systems. Traditional test collections are constructed by asking judges the relevance grade for a document with respect to an input query. Recent work [44] found an evidence that eﬀort is another important factor in determining document utility, suggesting that more thought should be given into incorporating eﬀort into information retrieval. However, that work did not ask judges to directly assess the level of eﬀort required to consume a document or analyse how eﬀort judgements relate to traditional relevance judgements.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the {Ninth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining} - {WSDM} '16},
	publisher = {ACM Press},
	author = {Verma, Manisha and Yilmaz, Emine and Craswell, Nick},
	year = {2016},
	keywords = {crowd},
	pages = {277--286}
}

@article{maddalena-crowdsourcing-nodate,
	title = {Crowdsourcing {Relevance} {Assessments}: {The} {Unexpected} {Benefits} of {Limiting} the {Time} to {Judge}},
	abstract = {Crowdsourcing has become an alternative approach to collect relevance judgments at scale thanks to the availability of crowdsourcing platforms and quality control techniques that allow to obtain reliable results. Previous work has used crowdsourcing to ask multiple crowd workers to judge the relevance of a document with respect to a query and studied how to best aggregate multiple judgments of the same topicdocument pair.},
	language = {en},
	author = {Maddalena, Eddy and Basaldella, Marco and Nart, Dario De and Degl'Innocenti, Dante and Mizzaro, Stefano and Demartini, Gianluca},
	keywords = {crowd},
	pages = {10}
}

@incollection{aslam-query-2007,
	address = {Berlin, Heidelberg},
	title = {Query {Hardness} {Estimation} {Using} {Jensen}-{Shannon} {Divergence} {Among} {Multiple} {Scoring} {Functions}},
	volume = {4425},
	isbn = {978-3-540-71494-1 978-3-540-71496-5},
	abstract = {We consider the issue of query performance, and we propose a novel method for automatically predicting the diﬃculty of a query. Unlike a number of existing techniques which are based on examining the ranked lists returned in response to perturbed versions of the query with respect to the given collection or perturbed versions of the collection with respect to the given query, our technique is based on examining the ranked lists returned by multiple scoring functions (retrieval engines) with respect to the given query and collection. In essence, we propose that the results returned by multiple retrieval engines will be relatively similar for “easy” queries but more diverse for “diﬃcult” queries. By appropriately employing Jensen-Shannon divergence to measure the “diversity” of the returned results, we demonstrate a methodology for predicting query diﬃculty whose performance exceeds existing state-ofthe-art techniques on TREC collections, often remarkably so.},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Aslam, Javed A. and Pavlu, Virgil},
	editor = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
	year = {2007},
	doi = {10.1007/978-3-540-71496-5-20},
	keywords = {query\-performance\-prediction},
	pages = {198--209}
}

@inproceedings{yilmaz-estimating-2006,
	address = {Arlington, Virginia, USA},
	title = {Estimating average precision with incomplete and imperfect judgments},
	isbn = {978-1-59593-433-8},
    doi = {10.1145/1183614.1183633},
	language = {en},
	urldate = {2018-10-21},
	booktitle = {Proceedings of the 15th {ACM} international conference on {Information} and knowledge management  - {CIKM} '06},
	publisher = {ACM Press},
	author = {Yilmaz, Emine and Aslam, Javed A.},
	year = {2006},
	pages = {102}
}

@article{efron-query-2010,
	title = {Query polyrepresentation for ranking retrieval systems without relevance judgments},
	issn = {15322882, 15322890},
	doi = {10.1002/asi.21310},
	language = {en},
	urldate = {2018-10-22},
	journal = {Journal of the American Society for Information Science and Technology},
	author = {Efron, Miles and Winget, Megan},
	year = {2010},
	keywords = {without\-judgment},
	pages = {n/a--n/a}
}

@inproceedings{voorhees-building-2018,
 author = {Voorhees},
 title = {On Building Fair and Reusable Test Collections Using Bandit Techniques},
 booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
 series = {CIKM '18},
 year = {2018},
 isbn = {978-1-4503-6014-2},
 location = {Torino, Italy},
 pages = {407--416},
 numpages = {10},
 doi = {10.1145/3269206.3271766},
 acmid = {3271766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-arm bandit methods, test collection building},
} 

@inproceedings{culpepper-trec:-2014,
	address = {Gold Coast, Queensland, Australia},
	title = {{TREC}: topic engineering exercise},
	isbn = {978-1-4503-2257-7},
	shorttitle = {{TREC}},
	doi = {10.1145/2600428.2609531},
	abstract = {In this work, we investigate approaches to engineer better topic sets in information retrieval test collections. By recasting the TREC evaluation exercise from one of building more effective systems to an exercise in building better topics, we present two possible approaches to quantify topic “goodness”: topic ease and topic set predictivity. A novel interpretation of a well known result and a twofold analysis of data from several TREC editions lead to a result that has been neglected so far: both topic ease and topic set predictivity have changed signiﬁcantly across the years, sometimes in a perhaps undesirable way.},
	language = {en},
	urldate = {2018-11-04},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval - {SIGIR} '14},
	publisher = {ACM Press},
	author = {Culpepper, J Shane and Mizzaro, Stefano and Sanderson, Mark and Scholer, Falk},
	year = {2014},
	pages = {1147--1150}
}

@incollection{dai-extended-2012,
	address = {Berlin, Heidelberg},
	title = {Extended {Expectation} {Maximization} for {Inferring} {Score} {Distributions}},
	volume = {7224},
	isbn = {978-3-642-28996-5 978-3-642-28997-2},
	abstract = {Inferring the distributions of relevant and nonrelevant documents over a ranked list of scored documents returned by a retrieval system has a broad range of applications including information ﬁltering, recall-oriented retrieval, metasearch, and distributed IR. Typically, the distribution of documents over scores is modeled by a mixture of two distributions, one for the relevant and one for the nonrelevant documents, and expectation maximization (EM) is run to estimate the mixture parameters. A large volume of work has focused on selecting the appropriate form of the two distributions in the mixture. In this work we consider the form of the distributions as a given and we focus on the inference algorithm. We extend the EM algorithm (a) by simultaneously considering the ranked lists of documents returned by multiple retrieval systems, and (b) by encoding in the algorithm the constraint that the same document retrieved by multiple systems should have the same, global, probability of relevance. We test the new inference algorithm using TREC data and we demonstrate that it outperforms the regular EM algorithm; it is better calibrated in inferring the probability of document’s relevance, and it is more eﬀective when applied on the task of metasearch.},
	language = {en},
	urldate = {2018-11-04},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dai, Keshi and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A.},
	editor = {Baeza-Yates, Ricardo and de Vries, Arjen P. and Zaragoza, Hugo and Cambazoglu, B. Barla and Murdock, Vanessa and Lempel, Ronny and Silvestri, Fabrizio and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
	year = {2012},
	doi = {10.1007/978-3-642-28997-2-25},
	pages = {293--304}
}

@article{allan-trec-nodate,
	title = {{TREC} 2017 {Common} {Core} {Track} {Overview}},
	language = {en},
	author = {Allan, James and Harman, Donna and Kanoulas, Evangelos and Li, Dan and Gysel, Christophe Van and Vorhees},
	keywords = {topic\-budget\-allocation},
	pages = {14},
	file = {Allan et al. - TREC 2017 Common Core Track Overview.pdf:/home/nahid/Zotero/storage/9LVPL5LQ/Allan et al. - TREC 2017 Common Core Track Overview.pdf:application/pdf}
}

@inproceedings{zhang-sampling-2016,
	address = {Pisa, Italy},
	title = {Sampling {Strategies} and {Active} {Learning} for {Volume} {Estimation}},
	isbn = {978-1-4503-4069-4},
	doi = {10.1145/2911451.2914685},
	abstract = {This paper tackles the challenge of accurately and eﬃciently estimating the number of relevant documents in a collection for a particular topic. One real-world application is estimating the volume of social media posts (e.g., tweets) pertaining to a topic, which is fundamental to tracking the popularity of politicians and brands, the potential sales of a product, etc. Our insight is to leverage active learning techniques to ﬁnd all the “easy” documents, and then to use sampling techniques to infer the number of relevant documents in the residual collection. We propose a simple yet eﬀective technique for determining this “switchover” point, which intuitively can be understood as the “knee” in an eﬀort vs. recall gain curve, as well as alternative sampling strategies beyond the knee. We show on several TREC datasets and a collection of tweets that our best technique yields more accurate estimates (with the same eﬀort) than several alternatives.},
	language = {en},
	urldate = {2018-12-13},
	booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '16},
	publisher = {ACM Press},
	author = {Zhang, Haotian and Lin, Jimmy and Cormack, Gordon V. and Smucker, Mark D.},
	year = {2016},
	keywords = {topic\-budget\-allocation},
	pages = {981--984}
}

@article{qin-letor:-2010,
	title = {{LETOR}: {A} benchmark collection for research on learning to rank for information retrieval},
	volume = {13},
	issn = {1386-4564, 1573-7659},
	shorttitle = {{LETOR}},
	doi = {10.1007/s10791-009-9123-y},
	abstract = {LETOR is a benchmark collection for the research on learning to rank for information retrieval, released by Microsoft Research Asia. In this paper, we describe the details of the LETOR collection and show how it can be used in diﬀerent kinds of researches. Speciﬁcally, we describe how the document corpora and query sets in LETOR are selected, how the documents are sampled, how the learning features and meta information are extracted, and how the datasets are partitioned for comprehensive evaluation. We then compare several state-of-the-art learning to rank algorithms on LETOR, report their ranking performances, and make discussions on the results. After that, we discuss possible new research topics that can be supported by LETOR, in addition to algorithm comparison. We hope that this paper can help people to gain deeper understanding of LETOR, and enable more interesting research projects on learning to rank and related topics.},
	language = {en},
	number = {4},
	urldate = {2018-12-13},
	journal = {Information Retrieval},
	author = {Qin, Tao and Liu, Tie-Yan and Xu, Jun and Li, Hang},
	month = aug,
	year = {2010},
	pages = {346--374}
}

@inproceedings{cormack-evaluation-2014,
	address = {Gold Coast, Queensland, Australia},
	title = {Evaluation of machine-learning protocols for technology-assisted review in electronic discovery},
	isbn = {978-1-4503-2257-7},
	doi = {10.1145/2600428.2609601},
	abstract = {Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the eﬀectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks – four derived from the TREC 2009 Legal Track and four derived from actual legal matters – recall was measured as a function of human review eﬀort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and signiﬁcantly less human review eﬀort (P {\textless} 0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, signiﬁcantly less human review eﬀort (P {\textless} 0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of “stabilization” – determining when training is adequate, and therefore may stop.},
	language = {en},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 37th international {ACM} {SIGIR} conference on {Research} \& development in information retrieval - {SIGIR} '14},
	publisher = {ACM Press},
	author = {Cormack, Gordon V. and Grossman, Maura R.},
	year = {2014},
	keywords = {cormack},
	pages = {153--162}
}

@article{cormack-machine-nodate,
	title = {Machine {Learning} for {Information} {Retrieval}: {TREC} 2009 {Web}, {Relevance} {Feedback} and {Legal} {Tracks}},
	language = {en},
	author = {Cormack, Gordon V and Mojdeh, Mona},
	keywords = {cormack, cormack\-features\-for\-TAR},
	pages = {9}
}

@inproceedings{hui-cluster-2016,
	address = {Montr\&\#233;al, Qu\&\#233;bec, Canada},
	title = {Cluster {Hypothesis} in {Low}-{Cost} {IR} {Evaluation} with {Different} {Document} {Representations}},
	isbn = {978-1-4503-4144-8},
	doi = {10.1145/2872518.2889370},
	abstract = {Oﬄine evaluation for information retrieval aims to compare the performance of retrieval systems based on relevance judgments for a set of test queries. Since manual judgments are expensive, selective labeling has been developed to semiautomatically label documents, in the wake of the similarity relationship among retrieved documents. Intuitively, the agreement w.r.t the cluster hypothesis can directly determine the amount of manual judgments that can be saved by creating labels with a semi-automatic method. Meanwhile, in representing documents, certain information is lost. We argue that better document representation can lead to better agreement with the cluster hypothesis. To this end, we investigate diﬀerent document representations on established benchmarks in the context of low-cost evaluation, showing that diﬀerent document representations vary in how well they capture document similarity relative to a query.},
	language = {en},
	urldate = {2018-12-18},
	booktitle = {Proceedings of the 25th {International} {Conference} {Companion} on {World} {Wide} {Web} - {WWW} '16 {Companion}},
	publisher = {ACM Press},
	author = {Hui, Kai and Berberich, Klaus},
	year = {2016},
	pages = {47--48}
}

@inproceedings{wu-methods-2003,
	address = {Melbourne, Florida},
	title = {Methods for ranking information retrieval systems without relevance judgments},
	isbn = {978-1-58113-624-1},
	doi = {10.1145/952532.952693},
	abstract = {In this paper we present some new methods of ranking information retrieval systems without relevance judgement. The common ground of these methods is using a measure we called reference count . An extensive experimentation was conducted to evaluate the eﬀectiveness of the proposed methods using various diﬀerent standards Information Retrieval evaluation measures for the ranking, like average precision, R-precision, and precision and diﬀerent document levels. We also compared the eﬀectiveness of the proposed methods with the method proposed by Soboroﬀ et al. The experimental results showed that the proposed methods are eﬀective, and in many cases are more eﬀective than Soboroﬀ at al.’s method.},
	language = {en},
	urldate = {2019-01-06},
	booktitle = {Proceedings of the 2003 {ACM} symposium on {Applied} computing  - {SAC} '03},
	publisher = {ACM Press},
	author = {Wu, Shengli and Crestani, Fabio},
	year = {2003},
	pages = {811}
}

@article{aslam-effectiveness-2003,
 author = {Aslam, Javed A. and Savell, Robert},
 title = {On the Effectiveness of Evaluating Retrieval Systems in the Absence of Relevance Judgments},
 booktitle = {Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval},
 series = {SIGIR '03},
 year = {2003},
 isbn = {1-58113-646-3},
 location = {Toronto, Canada},
 pages = {361--362},
 numpages = {2},
 doi = {10.1145/860435.860501},
 acmid = {860501},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ranking retrieval systems},
} 

@inproceedings{ETRA20,
author = {Bhattacharya, Nilavra and Rakshit, Somnath and Gwizdka, Jacek},
title = {Towards Real-time Webpage Relevance Prediction Using Convex Hull Based Eye-tracking Features},
year = {2020},
booktitle = {Symposium on Eye Tracking Research \& Applications (ETRA '20)}
}

@inproceedings{CHIIR20,
author = {Bhattacharya, Nilavra and Rakshit, Somnath and Gwizdka, Jacek and Kogut, Paul},
title = {Relevance Prediction from Eye-Movements Using Semi-Interpretable Convolutional Neural Networks},
year = {2020},
booktitle = {Conference on Human Information Interaction and Retrieval (CHIIR'20)}
}
  

@inproceedings{CHIIR19,
author = {Bhattacharya, Nilavra and Gwizdka, Jacek},
title = {Measuring Learning During Search: Differences in Interactions, Eye-Gaze, and Semantic Similarity to Expert Knowledge},
year = {2019},
booktitle = {CHIIR'19}
}
  


@inproceedings{ETRA18,
author = {Bhattacharya, Nilavra and Gwizdka, Jacek},
title = {Relating Eye-Tracking Measures with Changes in Knowledge on Search Tasks},
year = {2018},
booktitle = {Symposium on Eye Tracking Research \& Applications (ETRA'18)}
}
  




@article{1,
  title = {An Electrophysiological Investigation of Semantic Priming with Pictures of Real Objects},
  author = {{McPherson W. Brian} and {Holcomb Phillip J.}},
  year = {2003},
  month = mar,
  volume = {36},
  pages = {53--65},
  issn = {0048-5772},
  doi = {10.1017/S0048577299971196},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\2SHXEADB\\McPherson W. Brian-Holcomb Phillip J.-2003-An electrophysiological investigation of semantic priming with pictures of real.pdf;C\:\\Users\\nilav\\Zotero\\storage\\KVJT239C\\S0048577299971196.html},
  journal = {Psychophysiology},
  note = {1},
  number = {1}
}

@inproceedings{10,
  title = {Using {{Facial Expressions}} and {{Peripheral Physiological Signals As Implicit Indicators}} of {{Topical Relevance}}},
  booktitle = {Proceedings of the 17th {{ACM International Conference}} on {{Multimedia}}},
  author = {Arapakis, Ioannis and Konstas, Ioannis and Jose, Joemon M.},
  year = {2009},
  pages = {461--470},
  publisher = {{ACM}},
  doi = {10.1145/1631272.1631336},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\MBMN8PBG\\2009-Arapakis et al-Using Facial Expressions and Peripheral.pdf},
  isbn = {978-1-60558-608-3},
  note = {10},
  series = {{{MM}} '09}
}

@incollection{11,
  title = {Enhancing {{Text}}-{{Based Analysis Using Neurophysiological Measures}}},
  booktitle = {Foundations of {{Augmented Cognition}}. {{Neuroergonomics}} and {{Operational Neuroscience}}},
  author = {Behneman, Adrienne and Kintz, Natalie and Johnson, Robin and Berka, Chris and Hale, Kelly and Fuchs, Sven and Axelsson, Par and Baskin, Angela},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Schmorrow, Dylan D. and Estabrooke, Ivy V. and Grootjen, Marc},
  year = {2009},
  volume = {5638},
  pages = {449--458},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  url = {http://www.springerlink.com.proxy.libraries.rutgers.edu/content/66325w1077m2r35p/},
  urldate = {2011-12-24},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\YWWT463N\\Behneman et al-2009-Enhancing Text-Based Analysis Using Neurophysiological Measures.pdf;C\:\\Users\\nilav\\Zotero\\storage\\A7XL7ADR\\66325w1077m2r35p.html},
  isbn = {978-3-642-02811-3 978-3-642-02812-0},
  keywords = {IS Construct - Interest,IS Construct - Relevance,Method - EEG,Moshfeghi},
  note = {11}
}

@incollection{12,
  title = {Are {{You Really Looking}}? {{Finding}} the {{Answer}} through {{Fixation Patterns}} and {{EEG}}},
  shorttitle = {Are {{You Really Looking}}?},
  booktitle = {Foundations of {{Augmented Cognition}}. {{Neuroergonomics}} and {{Operational Neuroscience}}},
  author = {Brouwer, Anne-Marie and Hogervorst, Maarten A. and Herman, Pawel and Kooi, Frank},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Schmorrow, Dylan D. and Estabrooke, Ivy V. and Grootjen, Marc},
  year = {2009},
  volume = {5638},
  pages = {329--338},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  url = {http://www.springerlink.com.proxy.libraries.rutgers.edu/content/ptn8lr42u3k6vj52/},
  urldate = {2011-12-24},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\UXT4TYL9\\Brouwer et al-2009-Are You Really Looking.pdf;C\:\\Users\\nilav\\Zotero\\storage\\VQSQV9U7\\ptn8lr42u3k6vj52.html},
  isbn = {978-3-642-02811-3 978-3-642-02812-0},
  keywords = {IS Construct - Mind Wandering,Method - EEG,Method - Eye Tracking,Moshfeghi},
  note = {00000}
}

@inproceedings{13,
  title = {Discriminating the Relevance of Web Search Results with Measures of Pupil Size},
  booktitle = {Proceedings of the 27th International Conference on {{Human}} Factors in Computing Systems},
  author = {Oliveira, Flavio T.P. and Aula, Anne and Russell, Daniel M.},
  year = {2009},
  pages = {2209--2212},
  publisher = {{ACM}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/1518701.1519038},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\2HNBQA7N\\Oliveira et al-2009-Discriminating the relevance of web search results with measures of pupil size.pdf;C\:\\Users\\nilav\\Zotero\\storage\\P9WMLX5H\\citation.html},
  isbn = {978-1-60558-246-7},
  keywords = {IS Construct - Relevance,Method - Pupil Dilation,Stimuli - Image,Stimuli - SERP},
  note = {00047}
}

@inproceedings{14,
  title = {Using {{fNIRS Brain Sensing}} in {{Realistic HCI Settings}}: {{Experiments}} and {{Guidelines}}},
  booktitle = {Proceedings of the {{22Nd Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Solovey, Erin Treacy and Girouard, Audrey and Chauncey, Krysta and Hirshfield, Leanne M. and Sassaroli, Angelo and Zheng, Feng and Fantini, Sergio and Jacob, Robert J.K.},
  year = {2009},
  pages = {157--166},
  publisher = {{ACM}},
  address = {{Victoria, BC, Canada}},
  doi = {10.1145/1622176.1622207},
  acmid = {1622207},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\EBHD7SUR\\2009-Solovey et al-Using fNIRS Brain Sensing in Realistic HCI.pdf},
  isbn = {978-1-60558-745-5},
  numpages = {10},
  series = {{{UIST}} '09}
}

@inproceedings{15,
  title = {Finding the User's Interest Level from Their Eyes},
  booktitle = {Proceedings of the 2010 {{ACM}} Workshop on {{Social}}, Adaptive and Personalized Multimedia Interaction and Access},
  author = {Haji Mirza, Seyed Navid and Izquierdo, Ebroul},
  year = {2010},
  pages = {25--28},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1878061.1878070},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9AYEFU98\\Haji Mirza-Izquierdo-2010-Finding the user's interest level from their eyes.pdf},
  isbn = {978-1-4503-0171-8},
  keywords = {IS Construct - Interest,Method - Eye Tracking},
  lccn = {0000},
  note = {00001 
Cited by 0000},
  series = {{{SAPMIA}} '10}
}

@inproceedings{16,
  title = {Image Ranking with Implicit Feedback from Eye Movements},
  booktitle = {Proceedings of the 2010 {{Symposium}} on {{Eye}}-{{Tracking Research}} \&\#38; {{Applications}}},
  author = {Hardoon, David R. and Pasupa, Kitsuchart},
  year = {2010},
  pages = {291--298},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1743666.1743734},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\8J9MKJIH\\Hardoon-Pasupa-2010-Image ranking with implicit feedback from eye movements.pdf},
  isbn = {978-1-60558-994-7},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {00039},
  series = {{{ETRA}} '10}
}

@inproceedings{17,
  title = {Using {{Galvanic Skin Response Measures}} to {{Identify Areas}} of {{Frustration}} for {{Older Web}} 2.0 {{Users}}},
  booktitle = {Proceedings of the 2010 {{International Cross Disciplinary Conference}} on {{Web Accessibility}} ({{W4A}})},
  author = {Lunn, Darren and Harper, Simon},
  year = {2010},
  pages = {34:1-34:10},
  publisher = {{ACM}},
  address = {{Raleigh, North Carolina}},
  doi = {10.1145/1805986.1806032},
  acmid = {1806032},
  articleno = {34},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\IDQ3LQZ9\\2010-Lunn-Harper-Using Galvanic Skin Response Measures to Identify.pdf},
  isbn = {978-1-4503-0045-2},
  numpages = {10},
  series = {{{W4A}} '10}
}

@inproceedings{18,
  title = {Eye {{Movement As}} an {{Interaction Mechanism}} for {{Relevance Feedback}} in a {{Content}}-Based {{Image Retrieval System}}},
  booktitle = {Proceedings of the 2010 {{Symposium}} on {{Eye}}-{{Tracking Research}} \& {{Applications}}},
  author = {Zhang, Yun and Fu, Hong and Liang, Zhen and Chi, Zheru and Feng, Dagan},
  year = {2010},
  pages = {37--40},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1743666.1743674},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\AMQAS2UT\\Zhang et al-2010-Eye Movement As an Interaction Mechanism for Relevance Feedback in a.pdf},
  isbn = {978-1-60558-994-7},
  keywords = {IS Construct - Relevance,IS Construct - Retrieval,Method - Eye Tracking,Stimuli - Image},
  note = {00023},
  series = {{{ETRA}} '10}
}

@incollection{19,
  title = {Using {{Eye}}-{{Tracking}} for the {{Evaluation}} of {{Interactive Information Retrieval}}},
  booktitle = {Comparative {{Evaluation}} of {{Focused Retrieval}}},
  author = {Beckers, Thomas and Korbar, Dennis},
  editor = {Geva, Shlomo and Kamps, Jaap and Schenkel, Ralf and Trotman, Andrew},
  year = {2011},
  month = jan,
  pages = {236--240},
  publisher = {{Springer Berlin Heidelberg}},
  url = {http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-23577-1-21},
  urldate = {2013-04-27},
  copyright = {\textcopyright{}2011 Springer-Verlag GmbH Berlin Heidelberg},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\TXW8CRNJ\\Beckers-Korbar-2011-Using Eye-Tracking for the Evaluation of Interactive Information Retrieval.pdf;C\:\\Users\\nilav\\Zotero\\storage\\3FZXA7U5\\10.html},
  isbn = {978-3-642-23576-4 978-3-642-23577-1},
  lccn = {0002},
  number = {6932},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{2,
  title = {Implicit {{Relevance Feedback}} from {{Eye Movements}}},
  booktitle = {Artificial {{Neural Networks}}: {{Biological Inspirations}} \textendash{} {{ICANN}} 2005},
  author = {Saloj{\"a}rvi, Jarkko and Puolam{\"a}ki, Kai and Kaski, Samuel},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.z}ny, S{\l}awomir},
  year = {2005},
  volume = {3696},
  pages = {513--518},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  url = {http://www.springerlink.com/content/58k718c7q2g5rkq5/},
  urldate = {2011-12-16},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\7PXBT8A2\\Salojärvi et al-2005-Implicit Relevance Feedback from Eye Movements.pdf;C\:\\Users\\nilav\\Zotero\\storage\\9UHVRMCJ\\58k718c7q2g5rkq5.html},
  isbn = {978-3-540-28752-0 978-3-540-28754-4},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {2}
}

@incollection{20,
  title = {Document {{Classification}} on {{Relevance}}: {{A Study}} on {{Eye Gaze Patterns}} for {{Reading}}},
  shorttitle = {Document {{Classification}} on {{Relevance}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Fahey, Daniel and Gedeon, Tom and Zhu, Dingyun},
  editor = {Lu, Bao-Liang and Zhang, Liqing and Kwok, James},
  year = {2011},
  month = jan,
  pages = {143--150},
  publisher = {{Springer Berlin Heidelberg}},
  url = {http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-24958-7-17},
  urldate = {2013-08-13},
  copyright = {\textcopyright{}2011 Springer-Verlag GmbH Berlin Heidelberg},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\W27GXA2M\\Fahey et al-2011-Document Classification on Relevance.pdf;C\:\\Users\\nilav\\Zotero\\storage\\5UX6ZR94\\10.html},
  isbn = {978-3-642-24957-0 978-3-642-24958-7},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {00001 
Cited by 0000},
  number = {7063},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{21,
  title = {Emotional {{Text Tagging}}},
  booktitle = {2nd {{Workshop}} on {{Eye Gaze}} in {{Intelligent Human Machine Interaction}} at {{IUI}} 2011},
  author = {Ismail, Farida and Biedert, Ralf and Dengel, Andreas and Buscher, Georg},
  year = {2011},
  address = {{Palo Alto, CA}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\46ANRI2M\\Ismail et al-2011-Emotional Text Tagging.pdf},
  keywords = {IS Construct - Emotion,Method - EEG,Method - Eye Tracking},
  lccn = {0000},
  note = {00002}
}

@inproceedings{22,
  title = {Inferring Word Relevance from Eye-Movements of Readers},
  booktitle = {Proceedings of the 16th International Conference on {{Intelligent}} User Interfaces},
  author = {Loboda, Tomasz D. and Brusilovsky, Peter and Brunstein, J{\"o}erg},
  year = {2011},
  pages = {175--184},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1943403.1943431},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\C7KBDYZ8\\Loboda et al-2011-Inferring word relevance from eye-movements of readers.pdf},
  isbn = {978-1-4503-0419-1},
  keywords = {IS Construct - Relevance,Method - Eye Tracking,Stimuli - Docs Medium: Paragraphs / News Articles,Stimuli - Text},
  note = {00029 
Cited by 0009},
  series = {{{IUI}} '11}
}

@inproceedings{23,
  title = {Emotional Correlates of Information Retrieval Behaviors},
  booktitle = {2011 {{IEEE Workshop}} on {{Affective Computational Intelligence}} ({{WACI}})},
  author = {Lopatovska, I.},
  year = {2011},
  month = apr,
  pages = {1--7},
  doi = {10.1109/WACI.2011.5953145},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\2B63ZEWK\\2011-Lopatovska-Emotional correlates of information retrieval.pdf}
}

@article{24,
  title = {Closing the Loop in Cortically-Coupled Computer Vision: A Brain\textendash{}Computer Interface for Searching Image Databases},
  shorttitle = {Closing the Loop in Cortically-Coupled Computer Vision},
  author = {Pohlmeyer, Eric A. and Wang, Jun and Jangraw, David C. and Lou, Bin and Chang, Shih-Fu and Sajda, Paul},
  year = {2011},
  month = jun,
  volume = {8},
  pages = {036025},
  issn = {1741-2552},
  doi = {10.1088/1741-2560/8/3/036025},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\NGHPCV7K\\Pohlmeyer et al-2011-Closing the loop in cortically-coupled computer vision.pdf;C\:\\Users\\nilav\\Zotero\\storage\\D8JE238S\\036025.html},
  journal = {Journal of Neural Engineering},
  keywords = {IS Construct - Interest,Method - EEG},
  language = {en},
  note = {00054},
  number = {3}
}

@incollection{25,
  title = {Reading {{Your Mind}}: {{EEG}} during {{Reading Task}}},
  shorttitle = {Reading {{Your Mind}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Vo, Tan and Gedeon, Tom},
  editor = {Lu, Bao-Liang and Zhang, Liqing and Kwok, James},
  year = {2011},
  month = jan,
  pages = {396--403},
  publisher = {{Springer Berlin Heidelberg}},
  url = {http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-24955-6-48},
  urldate = {2013-08-13},
  copyright = {\textcopyright{}2011 Springer-Verlag GmbH Berlin Heidelberg},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\SZCH5F8P\\Vo-Gedeon-2011-Reading Your Mind.pdf;C\:\\Users\\nilav\\Zotero\\storage\\WNTSK6T6\\978-3-642-24955-6-48.html},
  isbn = {978-3-642-24954-9 978-3-642-24955-6},
  keywords = {IS Construct - Relevance,Method - EEG,Method - Eye Tracking},
  note = {00003 
Cited by 0001},
  number = {7062},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{26,
  title = {Attentive Documents: {{Eye}} Tracking as Implicit Feedback for Information Retrieval and Beyond},
  shorttitle = {Attentive Documents},
  author = {Buscher, Georg and Dengel, Andreas and Biedert, Ralf and Elst, Ludger V.},
  year = {2012},
  volume = {1},
  pages = {9:1--9:30},
  issn = {2160-6455},
  doi = {10.1145/2070719.2070722},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6XI9K7VV\\Buscher et al-2012-Attentive documents.pdf},
  journal = {ACM Trans. Interact. Intell. Syst.},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {00059},
  number = {2}
}

@inproceedings{27,
  title = {Continue or {{Stop Reading}}? {{Modeling Decisions}} in {{Information Search}}},
  shorttitle = {Continue or {{Stop Reading}}?},
  booktitle = {Proceedings of the 9th {{International Workshop}} on {{NLPCS}} 2012},
  author = {Lopez Orozco, Francisco and {Gu{\'e}rin-Dugu{\'e}}, Anne and Lemaire, Beno{\^i}t},
  editor = {B. Sharp, M. Zock},
  year = {2012},
  pages = {96--105},
  address = {{Wroclaw, Pologne}},
  url = {http://hal.archives-ouvertes.fr/hal-00790210},
  urldate = {2013-11-30},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\MFYTRHE2\\Lopez Orozco et al-2012-Continue or Stop Reading.pdf;C\:\\Users\\nilav\\Zotero\\storage\\FS5ERCX5\\hal-00790210.html},
  keywords = {IS Construct - Decision,IS Construct - Need Satisfaction,IS Construct - Relevance,Method - Eye Tracking},
  language = {Anglais},
  note = {00001}
}

@inproceedings{28,
  title = {Using Eye-Tracking with Dynamic Areas of Interest for Analyzing Interactive Information Retrieval},
  booktitle = {Conference on {{Research}} and Development in Information Retrieval (SIGIR'12)},
  author = {Tran, Vu Tuan and Fuhr, Norbert},
  year = {2012}
}

@inproceedings{29,
  title = {Search {{Intent Estimation}} from {{User}}'s {{Eye Movements}} for {{Supporting Information Seeking}}},
  booktitle = {Proceedings of the {{International Working Conference}} on {{Advanced Visual Interfaces}}},
  author = {Umemoto, Kazutoshi and Yamamoto, Takehiro and Nakamura, Satoshi and Tanaka, Katsumi},
  year = {2012},
  pages = {349--356},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2254556.2254624},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\XSGHLAK4\\Umemoto et al-2012-Search Intent Estimation from User's Eye Movements for Supporting Information.pdf},
  isbn = {978-1-4503-1287-5},
  note = {00014},
  series = {{{AVI}} '12}
}

@inproceedings{3,
  title = {Investigating {{Biometric Response}} for {{Information Retrieval Applications}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Mooney, Colum and Scully, Miche{\'a}l and Jones, Gareth J. F. and Smeaton, Alan F.},
  editor = {Lalmas, Mounia and MacFarlane, Andy and R{\"u}ger, Stefan and Tombros, Anastasios and Tsikrika, Theodora and Yavlinsky, Alexei},
  year = {2006},
  pages = {570--574},
  publisher = {{Springer Berlin Heidelberg}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\TS44W9L4\\2006-Mooney et al-Investigating Biometric Response for Information.pdf},
  isbn = {978-3-540-33348-7},
  note = {3}
}

@incollection{30,
  title = {Search {{Results Pages}} and {{Competition}} for {{Attention Theory}}: {{An Exploratory Eye}}-{{Tracking Study}}},
  shorttitle = {Search {{Results Pages}} and {{Competition}} for {{Attention Theory}}},
  booktitle = {Human {{Interface}} and the {{Management}} of {{Information}}. {{Information}} and {{Interaction Design}}},
  author = {Djamasbi, Soussan and {Hall-Phillips}, Adrienne and Yang, Ruijiao (Rachel)},
  editor = {Yamamoto, Sakae},
  year = {2013},
  month = jan,
  pages = {576--583},
  publisher = {{Springer Berlin Heidelberg}},
  url = {http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-39209-2-64},
  urldate = {2013-10-17},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\PW4WVX75\\Djamasbi et al-2013-Search Results Pages and Competition for Attention Theory.pdf;C\:\\Users\\nilav\\Zotero\\storage\\NBJEZ73N\\978-3-642-39209-2-64.html},
  isbn = {978-3-642-39208-5 978-3-642-39209-2},
  keywords = {IS Construct - Attention,IS Construct - Relevance,Method - Eye Tracking},
  note = {00002},
  number = {8016},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{31,
  title = {Decision-Making in Information Seeking on Texts: An {{Eye}}-{{Fixation}}-{{Related Potentials}} Investigation},
  author = {Frey, Aline and Ionescu, Gelu and Lemaire, Benoit and {Lopez-Orozco}, Francisco and Baccino, Thierry and {Guerin-Dugue}, Anne},
  year = {2013},
  volume = {7},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2013.00039},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\UJT7G47X\\Frey et al-2013-Decision-making in information seeking on texts.pdf},
  journal = {Frontiers in Systems Neuroscience},
  keywords = {IS Construct - Decision,IS Construct - Need Satisfaction,IS Construct - Relevance,Method - EEG,Method - ERP,Method - Eye Tracking},
  note = {00020},
  number = {39}
}

@inproceedings{32,
  title = {Looking for {{Information Relevance In}} the {{Brain}}},
  booktitle = {Gmunden {{Retreat}} on {{NeuroIS}} 2013},
  author = {Gwizdka, Jacek},
  year = {2013},
  month = jun,
  pages = {14},
  address = {{Gmunden, Austria}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9HDDZ9FL\\Gwizdka-2013-Looking for Information Relevance In the Brain.pdf},
  keywords = {Gwizdka,IS Construct - Relevance,Method - Eye Tracking,Method - fMRI,Method - Pupil Dilation,Stimuli - Docs Medium: Paragraphs / News Articles,Stimuli - Text},
  note = {00010}
}

@article{33,
  title = {Applications of Neuroimaging in Information Science: {{Challenges}} and Opportunities},
  shorttitle = {Applications of Neuroimaging in Information Science},
  author = {Gwizdka, Jacek and Mostafa, Javed and Moshfeghi, Yashar and Bergman, Ofer and Pollick, Frank E.},
  year = {2013},
  month = jan,
  volume = {50},
  pages = {1--4},
  issn = {1550-8390},
  doi = {10.1002/meet.14505001016},
  copyright = {Copyright \textcopyright{} 2013 by American Society for Information Science and Technology},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\XLGYBUKP\\Gwizdka et al-2013-Applications of neuroimaging in information science.pdf;C\:\\Users\\nilav\\Zotero\\storage\\WHMVVKDY\\abstract.html},
  journal = {Proceedings of the American Society for Information Science and Technology},
  keywords = {Gwizdka,Method - fMRI,Moshfeghi},
  language = {en},
  note = {00003},
  number = {1}
}

@inproceedings{34,
  title = {An Effective Implicit Relevance Feedback Technique Using Affective, Physiological and Behavioural Features},
  booktitle = {Proceedings of the 36th International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Moshfeghi, Yashar and Jose, Joemon M.},
  year = {2013},
  pages = {133--142},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2484028.2484074},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\BXESJT4P\\Moshfeghi-Jose-2013-An effective implicit relevance feedback technique using affective,.pdf;C\:\\Users\\nilav\\Zotero\\storage\\VJYEFAPF\\Moshfeghi-Jose-2013-An-effective-implicit-relevance-feedback-technique-using-affective,-physiological-and.pdf},
  isbn = {978-1-4503-2034-4},
  keywords = {IS Construct - Relevance,Method - Emotion,Method - GSR / EDA,Method - Heart Rate,Moshfeghi},
  note = {00029 
Cited by 0000},
  series = {{{SIGIR}} '13}
}

@incollection{35,
  title = {Understanding {{Relevance}}: {{An fMRI Study}}},
  shorttitle = {Understanding {{Relevance}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Moshfeghi, Yashar and Pinto, Luisa R. and Pollick, Frank E. and Jose, Joemon M.},
  editor = {Serdyukov, Pavel and Braslavski, Pavel and Kuznetsov, Sergei O. and Kamps, Jaap and R{\"u}ger, Stefan and Agichtein, Eugene and Segalovich, Ilya and Yilmaz, Emine},
  year = {2013},
  month = jan,
  pages = {14--25},
  publisher = {{Springer Berlin Heidelberg}},
  url = {http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-36973-5-2},
  urldate = {2013-10-09},
  copyright = {\textcopyright{}2013 Springer-Verlag Berlin Heidelberg},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\GH93GMAS\\Moshfeghi et al-2013-Understanding Relevance.pdf;C\:\\Users\\nilav\\Zotero\\storage\\JK2VWGYS\\10.html},
  isbn = {978-3-642-36972-8 978-3-642-36973-5},
  keywords = {IS Construct - Relevance,Method - fMRI,Moshfeghi,Stimuli - Image},
  note = {00026},
  number = {7814},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{36,
  title = {Predicting {{Term}}-Relevance from {{Brain Signals}}},
  booktitle = {Proceedings of the 37th {{International ACM SIGIR Conference}} on {{Research}} \&\#38; {{Development}} in {{Information Retrieval}}},
  author = {Eugster, Manuel J.A. and Ruotsalo, Tuukka and Spap{\'e}, Michiel M. and Kosunen, Ilkka and Barral, Oswald and Ravaja, Niklas and Jacucci, Giulio and Kaski, Samuel},
  year = {2014},
  pages = {425--434},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2600428.2609594},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\YHHBIWES\\Eugster et al-2014-Predicting Term-relevance from Brain Signals.pdf},
  isbn = {978-1-4503-2257-7},
  keywords = {IS Construct - Relevance,Method - EEG,Stimuli - Terms (Words / Phrases),Stimuli - Text},
  note = {00039},
  series = {{{SIGIR}} '14}
}

@inproceedings{37,
  title = {Characterizing {{Relevance}} with {{Eye}}-Tracking {{Measures}}},
  booktitle = {Proceedings of the 5th {{Information Interaction}} in {{Context Symposium}}},
  author = {Gwizdka, Jacek},
  year = {2014},
  pages = {58--67},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2637002.2637011},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\73UH3KWS\\2014-Gwizdka-Characterizing Relevance with Eye-tracking.pdf},
  isbn = {978-1-4503-2976-7},
  keywords = {Gwizdka,IS Construct - Relevance,Method - Eye Tracking,Method - Pupil Dilation,Stimuli - Docs Medium: Paragraphs / News Articles,Stimuli - Text},
  note = {00028},
  series = {{{IIiX}} '14}
}

@inproceedings{38,
  title = {News {{Stories Relevance Effects}} on {{Eye}}-Movements},
  booktitle = {Proceedings of the {{Symposium}} on {{Eye Tracking Research}} and {{Applications}}},
  author = {Gwizdka, Jacek},
  year = {2014},
  pages = {283--286},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2578153.2578198},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6ZBC8B3W\\Gwizdka-2014-News Stories Relevance Effects on Eye-movements.pdf},
  isbn = {978-1-4503-2751-0},
  keywords = {Gwizdka,IS Construct - Relevance,Method - Eye Tracking,Stimuli - Docs Medium: Paragraphs / News Articles,Stimuli - Text},
  note = {00008},
  series = {{{ETRA}} '14}
}

@incollection{39,
  title = {Using {{fNIRS}} to {{Measure Mental Workload}} in the {{Real World}}},
  booktitle = {Advances in {{Physiological Computing}}},
  author = {Peck, Evan M. and Afergan, Daniel and Yuksel, Beste F. and Lalooses, Francine and Jacob, Robert J. K.},
  editor = {Fairclough, Stephen H. and Gilleade, Kiel},
  year = {2014},
  pages = {117--139},
  publisher = {{Springer London}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6392-3-6},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\5398ACDE\\2014-Peck et al-Using fNIRS to Measure Mental Workload in the.pdf},
  isbn = {978-1-4471-6392-3}
}

@inproceedings{4,
  title = {Information {{Retrieval}} by {{Inferring Implicit Queries}} from {{Eye Movements}}},
  booktitle = {Eleventh {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hardoon, DR and {Shawe-Taylor}, J. and Ajanki, A. and Puolam{\"a}ki, K. and Kaski, S.},
  year = {2007},
  volume = {2},
  pages = {179--186},
  address = {{San Juan, Puerto Rico}},
  url = {http://jmlr.org/proceedings/papers/v2/hardoon07a.html},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\W4CPN9XP\\Hardoon et al-2007-Information Retrieval by Inferring Implicit Queries from Eye Movements.pdf},
  keywords = {IS Construct - Interest,IS Construct - Need,Method - Eye Tracking},
  note = {4}
}

@inproceedings{40,
  title = {When {{Relevance Judgement}} Is {{Happening}}?: {{An EEG}}-Based {{Study}}},
  shorttitle = {When {{Relevance Judgement}} Is {{Happening}}?},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Allegretti, Marco and Moshfeghi, Yashar and Hadjigeorgieva, Maria and Pollick, Frank E. and Jose, Joemon M. and Pasi, Gabriella},
  year = {2015},
  pages = {719--722},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2766462.2767811},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\UM7XISSI\\Allegretti et al-2015-When Relevance Judgement is Happening.pdf},
  isbn = {978-1-4503-3621-5},
  keywords = {IS Construct - Relevance,Method - EEG,Moshfeghi,Stimuli - Image},
  note = {00000},
  series = {{{SIGIR}} '15}
}

@inproceedings{41,
  title = {Exploring {{Peripheral Physiology As}} a {{Predictor}} of {{Perceived Relevance}} in {{Information Retrieval}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Barral, Oswald and Eugster, Manuel J.A. and Ruotsalo, Tuukka and Spap{\'e}, Michiel M. and Kosunen, Ilkka and Ravaja, Niklas and Kaski, Samuel and Jacucci, Giulio},
  year = {2015},
  pages = {389--399},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2678025.2701389},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\2K76NHSV\\Barral et al-2015-Exploring Peripheral Physiology As a Predictor of Perceived Relevance in.pdf},
  isbn = {978-1-4503-3306-1},
  keywords = {IS Construct - Relevance,IS Construct - Relevance Feedback,Method - EMG,Method - GSR / EDA,Stimuli - Docs Short: Snippets / Sentences,Stimuli - Text},
  note = {00017},
  series = {{{IUI}} '15}
}

@inproceedings{42,
  title = {Unconscious {{Physiological Effects}} of {{Search Latency}} on {{Users}} and {{Their Click Behaviour}}},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {{Barreda-{\'A}ngeles}, Miguel and Arapakis, Ioannis and Bai, Xiao and Cambazoglu, B. Barla and {Pereda-Ba{\~n}os}, Alexandre},
  year = {2015},
  pages = {203--212},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2766462.2767719},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\7I96H497\\Barreda-Ángeles et al-2015-Unconscious Physiological Effects of Search Latency on Users and Their Click.pdf},
  isbn = {978-1-4503-3621-5},
  keywords = {IS Construct - Emotion,Method - EMG,Method - GSR / EDA},
  note = {00013},
  series = {{{SIGIR}} '15}
}

@article{43,
  title = {Navigating through Digital Folders Uses the Same Brain Structures as Real World Navigation},
  author = {Benn, Yael and Bergman, Ofer and Glazer, Liv and Arent, Paris and Wilkinson, Iain D. and Varley, Rosemary and Whittaker, Steve},
  year = {2015},
  month = oct,
  volume = {5},
  pages = {14719},
  issn = {2045-2322},
  doi = {10.1038/srep14719},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\CEZRUR6I\\Benn et al-2015-Navigating through digital folders uses the same brain structures as real world.pdf;C\:\\Users\\nilav\\Zotero\\storage\\25QWPTDU\\srep14719.html},
  journal = {Scientific Reports},
  keywords = {IS Construct - Retrieval,Method - fMRI},
  language = {en},
  note = {00006}
}

@inproceedings{44,
  title = {A {{Real}}-{{Time Eye Tracking Based Query Expansion Approach}} via {{Latent Topic Modeling}}},
  booktitle = {Proceedings of the 24th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Chen, Yongqiang and Zhang, Peng and Song, Dawei and Wang, Benyou},
  year = {2015},
  pages = {1719--1722},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2806416.2806602},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4IVVL8B4\\Chen-et-al-2015-A-Real-Time-Eye-Tracking-Based-Query-Expansion-Approach-via-Latent-Topic-Modeling.pdf},
  isbn = {978-1-4503-3794-6},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {00000},
  series = {{{CIKM}} '15}
}

@inproceedings{45,
  title = {Classifying Document Categories Based on Physiological Measures of Analyst Responses},
  booktitle = {2015 6th {{IEEE International Conference}} on {{Cognitive Infocommunications}} ({{CogInfoCom}})},
  author = {Chow, C. and Gedeon, T.},
  year = {2015},
  month = oct,
  pages = {421--425},
  doi = {10.1109/CogInfoCom.2015.7390631},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\8IVH4NFW\\Chow-Gedeon-2015-Classifying document categories based on physiological measures of analyst.pdf;C\:\\Users\\nilav\\Zotero\\storage\\SFSY5N2V\\references.html},
  keywords = {IS Construct - Interest,IS Construct - Relevance,Method - ECG,Method - Eye Tracking,Method - GSR / EDA},
  note = {00004}
}

@inproceedings{46,
  title = {Live {{Demonstrator}} of {{EEG}} and {{Eye}}-{{Tracking Input}} for {{Disambiguation}} of {{Image Search Results}}},
  booktitle = {Symbiotic {{Interaction}}},
  author = {Golenia, Jan-Eike and Wenzel, Markus and Blankertz, Benjamin},
  editor = {Blankertz, Benjamin and Jacucci, Giulio and Gamberini, Luciano and Spagnolli, Anna and Freeman, Jonathan},
  year = {2015},
  pages = {81--86},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\Y9AXGNC8\\2015-Golenia et al-Live Demonstrator of EEG and Eye-Tracking Input.pdf},
  isbn = {978-3-319-24917-9}
}

@inproceedings{47,
  title = {Differences in {{Eye}}-{{Tracking Measures Between Visits}} and {{Revisits}} to {{Relevant}} and {{Irrelevant Web Pages}}},
  booktitle = {Proceedings of the 38th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Gwizdka, Jacek and Zhang, Yinglong},
  year = {2015},
  pages = {811--814},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2766462.2767795},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9TAPX9CH\\Gwizdka-Zhang-2015-Differences in Eye-Tracking Measures Between Visits and Revisits to Relevant.pdf},
  isbn = {978-1-4503-3621-5},
  keywords = {Gwizdka,IS Construct - Relevance,Method - Eye Tracking,Method - Pupil Dilation,Stimuli - Web Pages (free navigation from SERP),Stimuli - Wikipedia},
  note = {00007},
  series = {{{SIGIR}} '15}
}

@inproceedings{48,
  title = {Towards {{Inferring Web Page Relevance}} \textendash{} {{An Eye}}-{{Tracking Study}}},
  booktitle = {Proceedings of {{iConference}}'2015},
  author = {Gwizdka, Jacek and Zhang, Yinglong},
  year = {2015},
  month = mar,
  pages = {5},
  url = {https://www.ideals.illinois.edu/handle/2142/73709},
  urldate = {2015-04-01},
  copyright = {Copyright 2015 is held by the authors. Copyright permissions, when appropriate, must be obtained directly from the authors.},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\EU9X4KYT\\Gwizdka-Zhang-2015-Towards Inferring Web Page Relevance – An Eye-Tracking Study.pdf;C\:\\Users\\nilav\\Zotero\\storage\\A9MHCAYD\\73709.html},
  keywords = {Gwizdka,IS Construct - Relevance,Method - Eye Tracking,Stimuli - Web Pages (free navigation from SERP),Stimuli - Wikipedia},
  language = {English},
  note = {00004}
}

@article{49,
  title = {Neural {{Patterns}} of the {{Implicit Association Test}}},
  author = {Healy, Graham F. and Boran, Lorraine and Smeaton, Alan F.},
  year = {2015},
  pages = {605},
  doi = {10.3389/fnhum.2015.00605},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6HQ3CF9D\\Healy et al-2015-Neural Patterns of the Implicit Association Test.pdf},
  journal = {Frontiers in Human Neuroscience},
  keywords = {IS Construct - Relevance,Method - EEG,Method - ERP},
  note = {00001}
}

@inproceedings{5,
  title = {Interactive {{Neurotechnology Platform}}: {{A Real}}-Time {{Window}} on {{Human Information Processing}} at the {{Millisecond Level}}},
  shorttitle = {Interactive {{Neurotechnology Platform}}},
  booktitle = {Applied {{Human Factors}} and {{Ergonomics}}},
  author = {Berka, Chris and Hale, Kelly and Cowell, A and Fuchs, Sven},
  year = {2008},
  address = {{Las Vegas, NV}},
  url = {https://www.researchgate.net/publication/236610577-Interactive-Neurotechnology-Platform-A-Real-time-Window-on-Human-Information-Processing-at-the-Millisecond-Level},
  urldate = {2013-07-17},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\5UTTPANJ\\Berka et al-2008-Interactive Neurotechnology Platform.pdf;C\:\\Users\\nilav\\Zotero\\storage\\FE7ETBX3\\Interactive-Neurotechnology-Platform-A-Real-time-Window-on-Human-Information-Processing-at-the-.html},
  keywords = {IS Construct - Relevance,Method - EEG,Method - ERP},
  note = {5}
}

@article{50,
  title = {How Are Icons Processed by the Brain? {{Neuroimaging}} Measures of Four Types of Visual Stimuli Used in Information Systems},
  shorttitle = {How Are Icons Processed by the Brain?},
  author = {Huang, Sheng-Cheng and Bias, Randolph G. and Schnyer, David},
  year = {2015},
  month = apr,
  volume = {66},
  pages = {702--720},
  issn = {2330-1643},
  doi = {10.1002/asi.23210},
  copyright = {\textcopyright{} 2014 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\V9D4EWWR\\2015-Huang et al-How are icons processed by the brain.pdf;C\:\\Users\\nilav\\Zotero\\storage\\IA3MS8J7\\abstract.html},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {IS Construct - Meaning Making,Method - fMRI},
  language = {en},
  note = {00008},
  number = {4}
}

@article{51,
  title = {Towards Brain-Activity-Controlled Information Retrieval: {{Decoding}} Image Relevance from {{MEG}} Signals},
  shorttitle = {Towards Brain-Activity-Controlled Information Retrieval},
  author = {Kauppi, Jukka-Pekka and Kandemir, Melih and Saarinen, Veli-Matti and Hirvenkari, Lotta and Parkkonen, Lauri and Klami, Arto and Hari, Riitta and Kaski, Samuel},
  year = {2015},
  month = may,
  volume = {112},
  pages = {288--298},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.12.079},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\68Z69U7Z\\Kauppi et al-2015-Towards brain-activity-controlled information retrieval.pdf;C\:\\Users\\nilav\\Zotero\\storage\\4BZE2YJV\\S1053811915000026.html},
  journal = {NeuroImage},
  keywords = {IS Construct - Relevance,Method - Eye Tracking,Method - MEG},
  note = {00017}
}

@article{52,
  title = {Combining Eye Tracking and Pupillary Dilation Analysis to Identify {{Website Key Objects}}},
  author = {Loyola, Pablo and Martinez, Gustavo and Mu{\~n}oz, Kristofher and Vel{\'a}squez, Juan D. and Maldonado, Pedro and Couve, Andr{\'e}s},
  year = {2015},
  month = nov,
  volume = {168},
  pages = {179--189},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.05.108},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\E4FNLGDZ\\Loyola et al-2015-Combining eye tracking and pupillary dilation analysis to identify Website Key.pdf;C\:\\Users\\nilav\\Zotero\\storage\\DGBVR9F8\\S0925231215008061.html},
  journal = {Neurocomputing},
  keywords = {IS Construct - Attention,IS Construct - Relevance,Method - Eye Tracking,Method - Pupil Dilation},
  note = {00008}
}

@inproceedings{53,
  title = {Examining the {{Reliability}} of {{Using fNIRS}} in {{Realistic HCI Settings}} for {{Spatial}} and {{Verbal Tasks}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Maior, Horia A. and Pike, Matthew and Sharples, Sarah and Wilson, Max L.},
  year = {2015},
  pages = {3039--3042},
  publisher = {{ACM}},
  address = {{Seoul, Republic of Korea}},
  doi = {10.1145/2702123.2702315},
  acmid = {2702315},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\AGFVT8I8\\2015-Maior et al-Examining the Reliability of Using fNIRS in.pdf},
  isbn = {978-1-4503-3145-6},
  numpages = {4},
  series = {{{CHI}} '15}
}

@incollection{54,
  title = {Identifying {{Neurological Patterns Associated}} with {{Information Seeking}}: {{A Pilot fMRI Study}}},
  shorttitle = {Identifying {{Neurological Patterns Associated}} with {{Information Seeking}}},
  booktitle = {Information {{Systems}} and {{Neuroscience}}},
  author = {Mostafa, Javed and Carrasco, Vincent and Foster, Chris and Giovenallo, Kelly},
  editor = {Davis, Fred D. and Riedl, Ren{\'e} and vom Brocke, Jan and L{\'e}ger, Pierre-Majorique and Randolph, Adriane B.},
  year = {2015},
  pages = {167--173},
  publisher = {{Springer International Publishing}},
  url = {http://link.springer.com/chapter/10.1007/978-3-319-18702-0-22},
  urldate = {2015-06-03},
  copyright = {\textcopyright{}2015 Springer International Publishing Switzerland},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4VS36GPF\\Mostafa et al-2015-Identifying Neurological Patterns Associated with Information Seeking.pdf;C\:\\Users\\nilav\\Zotero\\storage\\329VDIGZ\\978-3-319-18702-0-22.html},
  isbn = {978-3-319-18701-3 978-3-319-18702-0},
  keywords = {IS Construct - Seeking,Method - fMRI},
  language = {en},
  note = {00002},
  number = {10},
  series = {Lecture {{Notes}} in {{Information Systems}} and {{Organisation}}}
}

@article{55,
  title = {Towards the Bio-Personalization of Music Recommendation Systems: {{A}} Single-Sensor {{EEG}} Biomarker of Subjective Music Preference},
  shorttitle = {Towards the Bio-Personalization of Music Recommendation Systems},
  author = {Adamos, Dimitrios A. and Dimitriadis, Stavros I. and Laskaris, Nikolaos A.},
  year = {2016},
  month = may,
  volume = {343\textendash{}344},
  pages = {94--108},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2016.01.005},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\3B4WQPG8\\Adamos et al-2016-Towards the bio-personalization of music recommendation systems.pdf;C\:\\Users\\nilav\\Zotero\\storage\\ZS3W6RMG\\S0020025516000074.html},
  journal = {Information Sciences},
  keywords = {IS Construct - Interest,IS Construct - Relevance,Method - EEG},
  note = {00007}
}

@article{56,
  title = {Extracting Relevance and Affect Information from Physiological Text Annotation},
  author = {Barral, Oswald and Kosunen, Ilkka and Ruotsalo, Tuukka and Spap{\'e}, Michiel M. and Eugster, Manuel J. A. and Ravaja, Niklas and Kaski, Samuel and Jacucci, Giulio},
  year = {2016},
  month = dec,
  volume = {26},
  pages = {493--520},
  issn = {0924-1868, 1573-1391},
  doi = {10.1007/s11257-016-9184-8},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\349EXX3P\\Barral et al-2016-Extracting relevance and affect information from physiological text annotation.pdf;C\:\\Users\\nilav\\Zotero\\storage\\KJXCKJUW\\s11257-016-9184-8.html},
  journal = {User Modeling and User-Adapted Interaction},
  keywords = {IS Construct - Emotion,IS Construct - Relevance,Method - EMG,Method - GSR / EDA,Stimuli - Docs Short: Snippets / Sentences,Stimuli - Text,Stimuli - Web Pages (free navigation from SERP)},
  language = {en},
  note = {00001},
  number = {5}
}

@article{57,
  title = {Natural Brain-Information Interfaces: {{Recommending}} Information by Relevance Inferred from Human Brain Signals},
  shorttitle = {Natural Brain-Information Interfaces},
  author = {Eugster, Manuel J. A. and Ruotsalo, Tuukka and Spap{\'e}, Michiel M. and Barral, Oswald and Ravaja, Niklas and Jacucci, Giulio and Kaski, Samuel},
  year = {2016},
  month = dec,
  volume = {6},
  pages = {38580},
  issn = {2045-2322},
  doi = {10.1038/srep38580},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\BSGZKU28\\Eugster-et-al-2016-srep38580-s1.pdf.pdf;C\:\\Users\\nilav\\Zotero\\storage\\EDDPFFA2\\Eugster-et-al-2016-Natural-brain-information-interfaces.pdf;C\:\\Users\\nilav\\Zotero\\storage\\UXEN25Y3\\srep38580.html},
  journal = {Scientific Reports},
  keywords = {IS Construct - Relevance,Method - EEG,Method - ERP,Stimuli - Docs Short: Snippets / Sentences,Stimuli - Text},
  language = {en},
  note = {00005}
}

@inproceedings{58,
  title = {Using {{Low}}-Cost {{Electroencephalography}} ({{EEG}}) {{Sensor}} to {{Identify Perceived Relevance}} on {{Web Search}}},
  booktitle = {Proceedings of the 79th {{ASIS}}\&{{T Annual Meeting}}: {{Creating Knowledge}}, {{Enhancing Lives Through Information}} \& {{Technology}}},
  author = {{Gonz{\'a}lez-Ib{\'a}{\~n}ez}, Roberto and {Escobar-Macaya}, Ma{\'r}\i{}a and Manriquez, Manuel},
  year = {2016},
  pages = {146:1-146:5},
  publisher = {{American Society for Information Science}},
  address = {{Copenhagen, Denmark}},
  url = {http://dl.acm.org.ezproxy.lib.utexas.edu/citation.cfm?id=3017447.3017593},
  acmid = {3017593},
  articleno = {146},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\H4RQLG3H\\2016-González-Ibáñez et al-Using Low-cost Electroencephalography (EEG).pdf},
  numpages = {5},
  series = {{{ASIST}} '16}
}

@article{59,
  title = {{{NeuroIR}} 2015: {{SIGIR}} 2015 {{Workshop}} on {{Neuro}}-{{Physiological Methods}} in {{IR Research}}},
  shorttitle = {{{NeuroIR}} 2015},
  author = {Gwizdka, Jacek and Mostafa, Javed},
  year = {2016},
  month = jan,
  volume = {49},
  pages = {83--88},
  doi = {10.1145/2888422.2888435},
  copyright = {All rights reserved},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\B97QPPVE\\Gwizdka-Mostafa-2016-NeuroIR 2015.pdf},
  journal = {SIGIR Forum},
  keywords = {Gwizdka},
  note = {00000}
}

@inproceedings{6,
  title = {Eye Movements as Implicit Relevance Feedback},
  booktitle = {{{CHI}} '08 Extended Abstracts on {{Human}} Factors in Computing Systems},
  author = {Buscher, Georg and Dengel, Andreas and {van Elst}, Ludger},
  year = {2008},
  pages = {2991--2996},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1358628.1358796},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\3RPVP6YM\\Buscher et al-2008-Eye movements as implicit relevance feedback.pdf},
  isbn = {978-1-60558-012-8},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {6},
  series = {{{CHI EA}} '08}
}

@incollection{60,
  title = {Leveraging {{Neurodata}} to {{Support Web User Behavior Analysis}}},
  booktitle = {Wisdom {{Web}} of {{Things}}},
  author = {Loyola, Pablo and Brunetti, Enzo and Martinez, Gustavo and Vel{\'a}squez, Juan D. and Maldonado, Pedro},
  editor = {Zhong, Ning and Ma, Jianhua and Liu, Jiming and Huang, Runhe and Tao, Xiaohui},
  year = {2016},
  pages = {181--207},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-44198-6-8},
  copyright = {\textcopyright{}2016 Springer International Publishing Switzerland},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9URDD74Q\\Loyola et al-2016-Leveraging Neurodata to Support Web User Behavior Analysis.pdf;C\:\\Users\\nilav\\Zotero\\storage\\KIXUH6X9\\978-3-319-44198-6-8.html},
  isbn = {978-3-319-44196-2 978-3-319-44198-6},
  keywords = {IS Construct - Behaviour,Method - EEG,Method - Eye Tracking,Review Paper},
  language = {en},
  note = {00000},
  series = {Web {{Information Systems Engineering}} and {{Internet Technologies Book Series}}}
}

@inproceedings{61,
  title = {Understanding {{Information Need}}: {{An fMRI Study}}},
  shorttitle = {Understanding {{Information Need}}},
  booktitle = {Proceedings of the 39th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Moshfeghi, Yashar and Triantafillou, Peter and Pollick, Frank E.},
  year = {2016},
  pages = {335--344},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2911451.2911534},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\7BGB88V6\\Moshfeghi et al-2016-Understanding Information Need.pdf},
  isbn = {978-1-4503-4069-4},
  keywords = {IS Construct - Need,Method - fMRI,Moshfeghi},
  note = {00008},
  series = {{{SIGIR}} '16}
}

@inproceedings{62,
  title = {Deepening the {{Role}} of the {{User}}: {{Neuro}}-{{Physiological Evidence As}} a {{Basis}} for {{Studying}} and {{Improving Search}}},
  shorttitle = {Deepening the {{Role}} of the {{User}}},
  booktitle = {Proceedings of the 2016 {{ACM}} on {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Mostafa, Javed and Gwizdka, Jacek},
  year = {2016},
  pages = {63--70},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2854946.2854979},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\UL9MCT8N\\Mostafa-Gwizdka-2016-Deepening the Role of the User.pdf},
  isbn = {978-1-4503-3751-9},
  keywords = {Gwizdka,IS Construct - Retrieval,Method - EEG,Method - Eye Tracking,Method - fMRI,nb-star},
  note = {00002},
  series = {{{CHIIR}} '16}
}

@inproceedings{63,
  title = {Fixation-{{Related EEG Frequency Band Power Analysis}}: {{A Promising Neuro}}-{{Cognitive Methodology}} to {{Evaluate}} the {{Matching}}-{{Quality}} of {{Web Search Results}}?},
  shorttitle = {Fixation-{{Related EEG Frequency Band Power Analysis}}},
  booktitle = {{{HCI International}} 2016 \textendash{} {{Posters}}' {{Extended Abstracts}}},
  author = {Scharinger, Christian and Kammerer, Yvonne and Gerjets, Peter},
  year = {2016},
  month = jul,
  pages = {245--250},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-40548-3-41},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\T2SYF9KE\\Scharinger et al-2016-Fixation-Related EEG Frequency Band Power Analysis.pdf;C\:\\Users\\nilav\\Zotero\\storage\\F57GE95M\\978-3-319-40548-3-41.html},
  keywords = {IS Construct - Relevance,Method - EEG,Method - EOG,Method - ERP,Method - Eye Tracking,Stimuli - SERP,Stimuli - Text},
  language = {en},
  note = {00001}
}

@article{64,
  title = {Classification of {{Eye Fixation Related Potentials}} for {{Variable Stimulus Saliency}}},
  author = {Wenzel, Markus A. and Golenia, Jan-Eike and Blankertz, Benjamin},
  year = {2016},
  volume = {10},
  pages = {23},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00023},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\Y5V3HNC4\\2016-Wenzel et al-Classification of Eye Fixation Related Potentials.pdf},
  journal = {Frontiers in Neuroscience}
}

@article{65,
  title = {Is {{Neural Activity Detected}} by {{ERP}}-{{Based Brain}}-{{Computer Interfaces Task Specific}}?},
  author = {{Wenzel, Markus A. AND Almeida, In{\^e}s AND Blankertz, Benjamin}},
  year = {2016},
  month = oct,
  volume = {11},
  pages = {1--16},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0165556},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\MNZ2989X\\2016-Wenzel, Markus A. AND Almeida, Inês AND Blankertz, Benjamin-Is Neural Activity Detected by ERP-Based.PDF},
  journal = {PLOS ONE},
  number = {10}
}

@article{66,
  title = {Risk and {{Ambiguity}} in {{Information Seeking}}: {{Eye Gaze Patterns Reveal Contextual Behavior}} in {{Dealing}} with {{Uncertainty}}},
  shorttitle = {Risk and {{Ambiguity}} in {{Information Seeking}}},
  author = {Wittek, Peter and Liu, Ying-Hsang and Dar{\'a}nyi, S{\'a}ndor and Gedeon, Tom and Lim, Ik Soo},
  year = {2016},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01790},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\F3R7KESV\\Wittek et al-2016-Risk and Ambiguity in Information Seeking.pdf},
  journal = {Frontiers in Psychology},
  keywords = {IS Construct - Behaviour,IS Construct - Seeking,Method - Eye Tracking},
  language = {English},
  note = {00003}
}

@inproceedings{67,
  title = {Differences in {{Reading Between Word Search}} and {{Information Relevance Decisions}}: {{Evidence}} from {{Eye}}-{{Tracking}}},
  booktitle = {Information {{Systems}} and {{Neuroscience}}},
  author = {Gwizdka, Jacek},
  editor = {Davis, Fred D. and Riedl, Ren{\'e} and {vom Brocke}, Jan and L{\'e}ger, Pierre-Majorique and Randolph, Adriane B.},
  year = {2017},
  pages = {141--147},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\7GJYVYPH\\2017-Gwizdka-Differences in Reading Between Word Search and.pdf},
  isbn = {978-3-319-41402-7},
  keywords = {Gwizdka}
}

@article{68,
  title = {Temporal Dynamics of Eye-Tracking and {{EEG}} during Reading and Relevance Decisions},
  author = {Gwizdka, Jacek and Hosseini, Rahilsadat and Cole, Michael and Wang, Shouyi},
  year = {2017},
  journal = {Journal of the Association for Information Science and Technology}
}

@article{69,
  title = {Combining Eye Tracking, Pupil Dilation and {{EEG}} Analysis for Predicting Web Users Click Intention},
  author = {Slanzi, Gino and Balazs, Jorge A. and Vel{\'a}squez, Juan D.},
  year = {2017},
  month = may,
  volume = {35},
  pages = {51--57},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2016.09.003},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\5779JHHR\\Slanzi et al-2017-Combining eye tracking, pupil dilation and EEG analysis for predicting web.pdf;C\:\\Users\\nilav\\Zotero\\storage\\NJJ7R8V3\\S1566253516300756.html},
  journal = {Information Fusion},
  keywords = {IS Construct - Decision,IS Construct - Relevance,Method - EEG,Method - Eye Tracking,Method - Pupil Dilation},
  note = {00006}
}

@inproceedings{7,
  title = {Can Relevance of Images Be Inferred from Eye Movements?},
  booktitle = {Proceedings of the 1st {{ACM}} International Conference on {{Multimedia}} Information Retrieval},
  author = {Klami, Arto and Saunders, Craig and {de Campos}, Te{\'o}filo E. and Kaski, Samuel},
  year = {2008},
  pages = {134--140},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1460096.1460120},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\93YHY389\\Klami et al-2008-Can relevance of images be inferred from eye movements.pdf},
  isbn = {978-1-60558-312-9},
  keywords = {IS Construct - Relevance,Method - Eye Tracking},
  note = {7},
  series = {{{MIR}} '08}
}

@article{70,
  title = {Biometric {{Information Fusion}} for {{Web User Navigation}} and {{Preferences Analysis}}: {{An Overview}}},
  shorttitle = {Biometric {{Information Fusion}} for {{Web User Navigation}} and {{Preferences Analysis}}},
  author = {Slanzi, Gino and Pizarro, Gaspar and Vel{\'a}squez, Juan D.},
  year = {2017},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2017.02.006},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\GAM29SZF\\2017-Slanzi et al-Biometric Information Fusion for Web User.pdf},
  journal = {Information Fusion},
  keywords = {Review Paper},
  note = {00001}
}

@article{71,
  title = {Real-Time Inference of Word Relevance from Electroencephalogram and Eye Gaze},
  author = {Wenzel, M. A. and Bogojeski, M. and Blankertz, B.},
  year = {2017},
  volume = {14},
  pages = {056007},
  issn = {1741-2552},
  doi = {10.1088/1741-2552/aa7590},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\ZGAZ5UYR\\2017-Wenzel et al-Real-time inference of word relevance from.pdf},
  journal = {Journal of Neural Engineering},
  keywords = {IS Construct - Relevance,Method - EEG,Method - Eye Tracking},
  language = {en},
  note = {00001},
  number = {5}
}

@inproceedings{72,
  title = {Improving Search Engines via Large-Scale Physiological Sensing},
  author = {White, Ryen and Ma, Ryan},
  booktitle={Conference on Research and Development in Information Retrieval (SIGIR'17)},
  year = {2017}
}

@article{73,
  title = {Implicit Relevance Feedback from Electroencephalography and Eye Tracking in Image Search},
  author = {Golenia, Jan-Eike and Wenzel, Markus A and Bogojeski, Mihail and Blankertz, Benjamin},
  year = {2018},
  month = jan,
  volume = {15},
  pages = {026002},
  publisher = {{IOP Publishing}},
  doi = {10.1088/1741-2552/aa9999},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\LXV3EL3P\\2018-Golenia et al-Implicit relevance feedback from.pdf},
  journal = {Journal of Neural Engineering},
  number = {2}
}

@inproceedings{74,
  title = {Inferring {{Web Page Relevance Using Pupillometry}} and {{Single Channel EEG}}},
  booktitle = {Information {{Systems}} and {{Neuroscience}}},
  author = {Gwizdka, Jacek},
  editor = {Davis, Fred D. and Riedl, Ren{\'e} and {vom Brocke}, Jan and L{\'e}ger, Pierre-Majorique and Randolph, Adriane B.},
  year = {2018},
  pages = {175--183},
  publisher = {{Springer International Publishing}},
  address = {{Cham, Switzerland}},
  doi = {10.1007/978-3-319-67431-5-20},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\X3MURDU3\\Gwizdka-2018-Inferring Web Page Relevance Using Pupillometry and Single Channel EEG.pdf},
  isbn = {978-3-319-67431-5},
  keywords = {Gwizdka,IS Construct - Relevance,Method - EEG,Method - Pupil Dilation,Stimuli - Web Pages (free navigation from SERP)},
  language = {en},
  series = {Lecture {{Notes}} in {{Information Systems}} and {{Organisation}}}
}

@article{75,
  title = {Using {{Psychophysiological Sensors}} to {{Assess Mental Workload During Web Browsing}}},
  author = {{Jimenez-Molina}, Angel and Retamal, Cristian and Lira, Hernan},
  year = {2018},
  month = feb,
  volume = {18},
  pages = {458},
  doi = {10.3390/s18020458},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\LSBQD2YE\\Jimenez-Molina et al-2018-Using Psychophysiological Sensors to Assess Mental Workload During Web Browsing.pdf;C\:\\Users\\nilav\\Zotero\\storage\\55CACINZ\\458.html},
  journal = {Sensors},
  language = {en},
  number = {2}
}

@article{76,
  title = {{{ERP}}/{{MMR Algorithm}} for {{Classifying Topic}}-{{Relevant}} and {{Topic}}-{{Irrelevant Visual Shots}} of {{Documentary Videos}}},
  author = {Kim, Hyun Hee and Kim, Yong Ho},
  year = {2019},
  volume = {70},
  pages = {931--941},
  issn = {2330-1643},
  doi = {10.1002/asi.24179},
  copyright = {\textcopyright{} 2018 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\FF6EJ7JY\\2018-Kim-Kim-ERP-MMR Algorithm for Classifying Topic-Relevant.pdf;C\:\\Users\\nilav\\Zotero\\storage\\XBHXBGMY\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {IS Construct - Relevance,Method - EEG,Method - ERP},
  language = {en},
  number = {9}
}

@article{77,
  title = {Learning {{From News}} on {{Different Media Platforms}}: {{An Eye}}-{{Tracking Experiment}}},
  shorttitle = {Learning {{From News}} on {{Different Media Platforms}}},
  author = {Kruikemeier, Sanne and Lecheler, Sophie and Boyer, Ming M.},
  year = {2018},
  month = jan,
  volume = {35},
  pages = {75--96},
  issn = {1058-4609, 1091-7675},
  journal = {Political Communication},
  keywords = {NP-used},
  language = {en},
  number = {1}
}

@inproceedings{78,
  title = {Understanding {{Reading Attention Distribution During Relevance Judgement}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Li, Xiangsheng and Liu, Yiqun and Mao, Jiaxin and He, Zexue and Zhang, Min and Ma, Shaoping},
  year = {2018},
  pages = {733--742},
  publisher = {{ACM}},
  address = {{Torino, Italy}},
  doi = {10.1145/3269206.3271764},
  acmid = {3271764},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9S3BG9JA\\2018-Li et al-Understanding Reading Attention Distribution.pdf},
  isbn = {978-1-4503-6014-2},
  numpages = {10},
  series = {{{CIKM}} '18}
}

@inproceedings{79,
  title = {Search {{Process As Transitions Between Neural States}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Moshfeghi, Yashar and Pollick, Frank E.},
  year = {2018},
  pages = {1683--1692},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Lyon, France}},
  doi = {10.1145/3178876.3186080},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\EHJ8WMZ6\\2018-Moshfeghi-Pollick-Search Process As Transitions Between Neural.pdf},
  isbn = {978-1-4503-5639-8},
  keywords = {IS Construct - Retrieval,IS Construct - Search,IS Construct - Seeking,Method - fMRI,Moshfeghi},
  series = {{{WWW}} '18}
}

@inproceedings{8,
  title = {Learning to Learn Implicit Queries from Gaze Patterns},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning},
  author = {Puolam{\"a}ki, Kai and Ajanki, Antti and Kaski, Samuel},
  year = {2008},
  pages = {760--767},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1390156.1390252},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\DJ4HHQCH\\Puolamäki et al-2008-Learning to learn implicit queries from gaze patterns.pdf},
  isbn = {978-1-60558-205-4},
  keywords = {IS Construct - Intent,IS Construct - Interest,Method - Eye Tracking},
  note = {8},
  series = {{{ICML}} '08}
}

@article{80,
  title = {The Dominant Factor of Social Tags for Users' Decision Behavior on E-commerce Websites: {{Color}} or Text},
  shorttitle = {The Dominant Factor of Social Tags for Users' Decision Behavior on E-commerce Websites},
  author = {Xu, Chen and Zhang, Qin},
  year = {2018},
  month = nov,
  issn = {2330-1643},
  doi = {10.1002/asi.24118},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\XLYX3H3A\\2018-Xu-Zhang-The dominant factor of social tags for users’.pdf;C\:\\Users\\nilav\\Zotero\\storage\\XA66S47Q\\login.html},
  journal = {Journal of the Association for Information Science and Technology},
  language = {en}
}

@article{81,
  title = {A Comparison of Unimodal and Multimodal Models for Implicit Detection of Relevance in Interactive {{IR}}},
  author = {{Gonz{\'a}lez-Ib{\'a}{\~n}ez}, Roberto and {Esparza-Villam{\'a}n}, Aileen and {Vargas-Godoy}, Juan Carlos and Shah, Chirag},
  year = {2019},
  volume = {0},
  doi = {10.1002/asi.24202},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24202},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\BQYMSJM4\\2019-González-Ibáñez et al-A comparison of unimodal and multimodal models.pdf},
  journal = {Journal of the Association for Information Science and Technology},
  number = {0}
}

@inproceedings{82,
  title = {Detecting {{Cognitive Bias}} in a {{Relevance Assessment Task Using}} an {{Eye Tracker}}},
  booktitle = {Proceedings of the 11th {{ACM Symposium}} on {{Eye Tracking Research}} \& {{Applications}}},
  author = {Harris, Christopher G.},
  year = {2019},
  pages = {36:1-36:5},
  publisher = {{ACM}},
  address = {{Denver, Colorado}},
  doi = {10.1145/3314111.3319824},
  acmid = {3319824},
  articleno = {36},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\MQJTUPUG\\2019-Harris-Detecting Cognitive Bias in a Relevance.pdf},
  isbn = {978-1-4503-6709-7},
  keywords = {IS Construct - Decision,IS Construct - Relevance,Method - Eye Tracking,Stimuli - Docs Short: Snippets / Sentences,Stimuli - Text},
  numpages = {5},
  series = {{{ETRA}} '19}
}

@article{83,
  title = {Integrating Neurophysiologic Relevance Feedback in Intent Modeling for Information Retrieval},
  author = {Jacucci, Giulio and Barral, Oswald and Daee, Pedram and Wenzel, Markus and Serim, Baris and Ruotsalo, Tuukka and Pluchino, Patrik and Freeman, Jonathan and Gamberini, Luciano and Kaski, Samuel and Blankertz, Benjamin},
  year = {2019},
  volume = {0},
  issn = {2330-1643},
  doi = {10.1002/asi.24161},
  copyright = {\textcopyright{} 2019 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals, Inc. on behalf of ASIS\&T.},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\PCHVB2SW\\Jacucci et al-Integrating neurophysiologic relevance feedback in intent modeling for.pdf;C\:\\Users\\nilav\\Zotero\\storage\\S3VM8TQI\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {IS Construct - Intent,IS Construct - Retrieval,Method - EEG,Method - Eye Tracking},
  language = {en},
  number = {0}
}

@inproceedings{84,
  title = {Biometric {{Tools}} in {{Information Science}}. {{The Example}} of an {{Information Literacy Study}} \textendash{} {{A Holiday Planning Experiment}}},
  booktitle = {Information {{Literacy}} in {{Everyday Life}}},
  author = {Jasiewicz, Justyna and Kisilowska, Ma{\l}gorzata and {Jupowicz-Ginalska}, Anna},
  editor = {Kurbano{\u g}lu, Serap and {\v S}piranec, Sonja and {\"U}nal, Yurdag{\"u}l and Boustany, Joumana and Huotari, Maija Leena and Grassian, Esther and Mizrachi, Diane and Roy, Loriene},
  year = {2019},
  pages = {23--32},
  publisher = {{Springer International Publishing}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\A4S895Y8\\Jasiewicz et al-2019-Biometric Tools in Information Science.pdf},
  isbn = {978-3-030-13472-3},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{85,
  title = {The Relationships between Health Information Behavior and Neural Processing in African Americans with Prehypertension},
  author = {Jones, Lenette M. and Wright, Kathy D. and Jack, Anthony I. and Friedman, Jared P. and Fresco, David M. and Veinot, Tiffany and Lu, Wei and Moore, Shirley M.},
  year = {2019},
  volume = {0},
  issn = {2330-1643},
  doi = {10.1002/asi.24098},
  copyright = {\textcopyright{} 2018 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4F6VVQ9E\\2019-Jones et al-The relationships between health information.pdf;C\:\\Users\\nilav\\Zotero\\storage\\69HILJKK\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  language = {en},
  number = {0}
}

@article{86,
  title = {Video Summarization Using Event-Related Potential Responses to Shot Boundaries in Real-Time Video Watching},
  author = {Kim, Hyun Hee and Kim, Yong Ho},
  year = {2019},
  volume = {0},
  issn = {2330-1643},
  doi = {10.1002/asi.24103},
  copyright = {\textcopyright{} 2018 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6L7TQKYC\\2019-Kim-Kim-Video summarization using event-related potential.pdf;C\:\\Users\\nilav\\Zotero\\storage\\5T6XPYUF\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  language = {en},
  number = {0}
}

@article{87,
  title = {Personalization in Text Information Retrieval: {{A}} Survey},
  author = {Liu, Jingjing and Liu, Chang and Belkin, Nicholas J.},
  year = {2019},
  volume = {0},
  doi = {10.1002/asi.24234},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.24234},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\RXCFXKEE\\2019-Liu et al-Personalization in text information retrieval.pdf},
  journal = {Journal of the Association for Information Science and Technology},
  number = {0}
}

@inproceedings{88,
  title = {Towards Predicting a Realisation of an Information Need Based on Brain Signals},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Moshfeghi, Yashar and Pollick, Frank},
  year = {2019},
  pages = {1300--1309},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\INWKRL6E\\2019-Moshfeghi-Pollick-Towards predicting a realisation of an.pdf},
  keywords = {Moshfeghi},
  organization = {{ACM}}
}

@article{89,
  title = {Neuropsychological Model of the Realization of Information Need},
  author = {Moshfeghi, Yashar and Pollick, Frank E.},
  year = {2019},
  volume = {0},
  issn = {2330-1643},
  doi = {10.1002/asi.24242},
  copyright = {\textcopyright{} 2019 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\H2EY3IRZ\\2019-Moshfeghi-Pollick-Neuropsychological model of the realization of.pdf;C\:\\Users\\nilav\\Zotero\\storage\\T5HDLANK\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {IS Construct - Need,Method - fMRI,Moshfeghi},
  language = {en},
  number = {0}
}

@article{9,
  title = {Using Hidden {{Markov}} Model to Uncover Processing States from Eye Movements in Information Search Tasks},
  author = {Simola, Jaana and Saloj{\"a}rvi, Jarkko and Kojo, Ilpo},
  year = {2008},
  month = oct,
  volume = {9},
  pages = {237--251},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2008.01.002},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4IVZU3VW\\Simola et al-2008-Using hidden Markov model to uncover processing states from eye movements in.pdf;C\:\\Users\\nilav\\Zotero\\storage\\EAG858UU\\S1389041708000132.html},
  journal = {Cognitive Systems Research},
  keywords = {IS Construct - Interest,IS Construct - Relevance,Method - Eye Tracking},
  lccn = {0016},
  note = {9},
  number = {4}
}

@article{90,
  title = {Investigating the Role of Eye Movements and Physiological Signals in Search Satisfaction Prediction Using Geometric Analysis},
  author = {Wu, Yingying and Liu, Yiqun and Tsai, Yen-Hsi Richard and Yau, Shing-Tung},
  year = {2019},
  volume = {0},
  issn = {2330-1643},
  doi = {10.1002/asi.24240},
  copyright = {\textcopyright{} 2019 ASIS\&T},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\N5W7VFQ3\\2019-Wu et al-Investigating the role of eye movements and.pdf;C\:\\Users\\nilav\\Zotero\\storage\\MW9HVFDC\\asi.html},
  journal = {Journal of the Association for Information Science and Technology},
  keywords = {IS Construct - Behaviour,IS Construct - Need Satisfaction,IS Construct - Seeking,Method - Eye Tracking,Method - GSR / EDA},
  language = {en},
  number = {0}
}

@inproceedings{91,
  title = {Investigating {{Human Visual Behavior}} by {{Hidden Markov Models}} in the {{Design}} of {{Marketing Information}}},
  booktitle = {Advances in {{Human Factors}} and {{Simulation}}},
  author = {Grobelny, Jerzy and Michalski, Rafa{\l}},
  editor = {Cassenti, Daniel N.},
  year = {2020},
  pages = {234--245},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6SIGJ4N5\\2020-Grobelny-Michalski-Investigating Human Visual Behavior by Hidden.pdf},
  isbn = {978-3-030-20148-7}
}



@inproceedings{101,
  title = {Eye-Tracking Analysis of User Behavior in {{WWW}} Search},
  booktitle = {Proceedings of the 27th Annual International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Granka, Laura A. and Joachims, Thorsten and Gay, Geri},
  year = {2004},
  pages = {478--479},
  publisher = {{Association for Computing Machinery}},
  address = {{Sheffield, United Kingdom}},
  doi = {10.1145/1008992.1009079},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\93F3GUQM\\2004-Granka et al-Eye-tracking analysis of user behavior in WWW.pdf},
  isbn = {1-58113-881-4},
  keywords = {[int-L],eye-tracking,implicit feedback,WWW search},
  numpages = {2},
  series = {{{SIGIR}} '04}
}

@inproceedings{102,
  title = {Eye-Tracking Reveals the Personal Styles for Search Result Evaluation},
  booktitle = {Human-Computer Interaction - {{INTERACT}} 2005},
  author = {Aula, Anne and Majaranta, P{\"a}ivi and R{\"a}ih{\"a}, Kari-Jouko},
  editor = {Costabile, Maria Francesca and Patern{\`o}, Fabio},
  year = {2005},
  pages = {1058--1061},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\KH986QLR\\2005-Aula et al-Eye-tracking reveals the personal styles for.pdf},
  isbn = {978-3-540-31722-7},
  keywords = {[int-L]}
}

@inproceedings{103,
  title = {Combining Eye Movements and Collaborative Filtering for Proactive Information Retrieval},
  booktitle = {Proceedings of the 28th Annual International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Puolam{\"a}ki, Kai and Saloj{\"a}rvi, Jarkko and Savia, Eerika and Simola, Jaana and Kaski, Samuel},
  year = {2005},
  pages = {146--153},
  publisher = {{Association for Computing Machinery}},
  address = {{Salvador, Brazil}},
  doi = {10.1145/1076034.1076062},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\7T6YC78I\\2005-Puolamäki et al-Combining eye movements and collaborative.pdf},
  isbn = {1-59593-034-5},
  keywords = {[int-L],collaborative filtering,eye movements,hidden Markov model,latent variable model,mixture model,proactive information retrieval,relevance feedback},
  numpages = {8},
  series = {{{SIGIR}} '05}
}

@inproceedings{104,
  title = {What Are You Looking for? {{An}} Eye-Tracking Study of Information Usage in Web Search},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Cutrell, Edward and Guan, Zhiwei},
  year = {2007},
  pages = {407--416},
  publisher = {{Association for Computing Machinery}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1240624.1240690},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\9BKG6GG8\\2007-Cutrell-Guan-What are you looking for.pdf},
  isbn = {978-1-59593-593-9},
  keywords = {[int-L],contextual snippets,eye tracking,user studies,web search},
  numpages = {10},
  series = {{{CHI}} '07}
}

@inproceedings{105,
  title = {An Eye Tracking Study of the Effect of Target Rank on Web Search},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Guan, Zhiwei and Cutrell, Edward},
  year = {2007},
  pages = {417--420},
  publisher = {{Association for Computing Machinery}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1240624.1240691},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\FI3XRWUS\\2007-Guan-Cutrell-An eye tracking study of the effect of target.pdf},
  isbn = {978-1-59593-593-9},
  keywords = {[int-L],eye tracking,target position,trust,web search},
  numpages = {4},
  series = {{{CHI}} '07}
}

@inproceedings{106,
  title = {A Qualitative Look at Eye-Tracking for Implicit Relevance Feedback},
  booktitle = {Proceedings of the Workshop on Context-Based Information Retrieval},
  author = {Moe, Kirsten Kirkegaard and Jensen, Jeanette M and Larsen, Birger},
  year = {2007},
  volume = {326},
  pages = {36--47},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\THRSXDJ9\\2007-Moe et al-A qualitative look at eye-tracking for implicit.pdf},
  keywords = {[int-I]}
}

@inproceedings{107,
  title = {Query Expansion Using Gaze-Based Feedback on the Subdocument Level},
  booktitle = {Proceedings of the 31st Annual International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Buscher, Georg and Dengel, Andreas and {van Elst}, Ludger},
  year = {2008},
  pages = {387--394},
  publisher = {{Association for Computing Machinery}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/1390334.1390401},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\AZTA833F\\2008-Buscher et al-Query expansion using gaze-based feedback on the.pdf},
  isbn = {978-1-60558-164-4},
  keywords = {[int-I],[int-L],[int-Q],eye tracking,implicit feedback,personalization,reading},
  numpages = {8},
  series = {{{SIGIR}} '08}
}

@article{108,
  title = {Eye Tracking and Online Search: {{Lessons}} Learned and Challenges Ahead},
  author = {Lorigo, Lori and Haridasan, Maya and Brynjarsd{\'o}ttir, Hr{\"o}nn and Xia, Ling and Joachims, Thorsten and Gay, Geri and Granka, Laura and Pellacini, Fabio and Pan, Bing},
  year = {2008},
  volume = {59},
  pages = {1041--1052},
  doi = {10.1002/asi.20794},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20794},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\BWHFYMF9\\2008-Lorigo et al-Eye tracking and online search.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {[int-L],[int-Q]},
  number = {7}
}

@article{109,
  title = {Can Eyes Reveal Interest? {{Implicit}} Queries from Gaze Patterns},
  author = {Ajanki, Antti and Hardoon, David R and Kaski, Samuel and Puolam{\"a}ki, Kai and {Shawe-Taylor}, John},
  year = {2009},
  volume = {19},
  pages = {307--339},
  publisher = {{Springer}},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\DR79LLFF\\2009-Ajanki et al-Can eyes reveal interest.pdf},
  journal = {User Modeling and User-Adapted Interaction},
  keywords = {[int-I]},
  number = {4}
}

@inproceedings{110,
  title = {What {{Do You See When You}}'re {{Surfing}}? {{Using Eye Tracking}} to {{Predict Salient Regions}} of {{Web Pages}}},
  author = {Buscher, Georg and Cutrell, Edward and Morris, Meredith Ringel},
  year = {2009},
  pages = {10},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\RAXLERIM\\Buscher et al. - 2009 - What Do You See When You’re Surfing Using Eye Tra.pdf},
  journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  keywords = {[int-I]},
  language = {en},
  series = {CHI ’09}
}

@inproceedings{111,
  title = {Segment-Level Display Time as Implicit Feedback: {{A}} Comparison to Eye Tracking},
  booktitle = {Proceedings of the 32nd International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Buscher, Georg and {van Elst}, Ludger and Dengel, Andreas},
  year = {2009},
  pages = {67--74},
  publisher = {{Association for Computing Machinery}},
  address = {{Boston, MA, USA}},
  doi = {10.1145/1571941.1571955},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\WH95L68D\\2009-Buscher et al-Segment-level display time as implicit feedback.pdf},
  isbn = {978-1-60558-483-6},
  keywords = {[int-I],[int-L],[int-Q],display time,eye tracking,implicit feedback,personalization},
  numpages = {8},
  series = {{{SIGIR}} '09}
}

@inproceedings{112,
  title = {{{EEG}} Analysis for Implicit Tagging of Video Data},
  booktitle = {2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops},
  author = {Koelstra, S. and M{\"u}hl, C. and Patras, I.},
  year = {2009},
  month = sep,
  pages = {1--6},
  doi = {10.1109/ACII.2009.5349482},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\34KN2NMD\\2009-Koelstra et al-EEG analysis for implicit tagging of video data.pdf},
  keywords = {Analysis of variance,ANOVA,classification accuracy,computer vision,EEG analysis,electroencephalography,Electroencephalography,Enterprise resource planning,feedback mechanism,Humans,Image analysis,image classification,implicit tagging,independent component analysis,Independent component analysis,indexing,Indexing,multimedia computing,multimedia indexing,multimedia retrieval,Multimedia systems,N400 event-related potential,nb-star,neuro-physiological indicators,neurophysiology,tag validation,Tagging,Target tracking,video data,video retrieval,vision-based recognition system}
}

@inproceedings{113,
  title = {A {{Comparison}} of {{General}} vs {{Personalised Affective Models}} for the {{Prediction}} of {{Topical Relevance}}},
  booktitle = {Proceedings of the 33rd International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval},
  author = {Arapakis, Ioannis and Athanasakos, Konstantinos and Jose, Joemon M},
  year = {2010},
  pages = {371--378},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\BQSXQTKX\\2010-Arapakis et al-A comparison of general vs personalised affective.pdf},
  keywords = {nb-star},
  organization = {{ACM}}
}

@article{114,
  title = {The Use of Relevance Criteria during Predictive Judgment: {{An}} Eye Tracking Approach},
  author = {Balatsoukas, Panos and Ruthven, Ian},
  year = {2010},
  volume = {47},
  pages = {1--10},
  doi = {10.1002/meet.14504701145},
  eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/meet.14504701145},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\FN6TBXTI\\2010-Balatsoukas-Ruthven-The use of relevance criteria during predictive.pdf},
  journal = {Proceedings of the American Society for Information Science and Technology},
  keywords = {[int-L]},
  number = {1}
}

@inproceedings{115,
  title = {The Good, the Bad, and the Random: {{An}} Eye-Tracking Study of Ad Quality in Web Search},
  booktitle = {Proceedings of the 33rd International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Buscher, Georg and Dumais, Susan T. and Cutrell, Edward},
  year = {2010},
  pages = {42--49},
  publisher = {{Association for Computing Machinery}},
  address = {{Geneva, Switzerland}},
  doi = {10.1145/1835449.1835459},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6FNQTLEJ\\2010-Buscher et al-The good, the bad, and the random.pdf},
  isbn = {978-1-4503-0153-4},
  keywords = {[int-L],gaze tracking,search engine results pages,user study},
  numpages = {8},
  series = {{{SIGIR}} '10}
}

@inproceedings{116,
  title = {Linking Search Tasks with Low-Level Eye Movement Patterns},
  booktitle = {Proceedings of the 28th Annual European Conference on Cognitive Ergonomics},
  author = {Cole, Michael J. and Gwizdka, Jacek and Bierig, Ralf and Belkin, Nicholas J. and Liu, Jingjing and Liu, Chang and Zhang, Xiangmin},
  year = {2010},
  pages = {109--116},
  publisher = {{Association for Computing Machinery}},
  address = {{Delft, Netherlands}},
  doi = {10.1145/1962300.1962323},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\6RVI8N3Y\\2010-Cole et al-Linking search tasks with low-level eye movement.pdf},
  isbn = {978-1-60558-946-6},
  keywords = {[int-I],cognitive task,eye movements,information search,interactive information retrieval,personalization,user models,user study},
  numpages = {8},
  series = {{{ECCE}} '10}
}

@inproceedings{117,
  title = {Individual Differences in Gaze Patterns for Web Search},
  booktitle = {Proceedings of the Third Symposium on Information Interaction in Context},
  author = {Dumais, Susan T. and Buscher, Georg and Cutrell, Edward},
  year = {2010},
  pages = {185--194},
  publisher = {{Association for Computing Machinery}},
  address = {{New Brunswick, New Jersey, USA}},
  doi = {10.1145/1840784.1840812},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\JSLGJWZ9\\2010-Dumais et al-Individual differences in gaze patterns for web.pdf},
  isbn = {978-1-4503-0247-0},
  keywords = {[int-L],eye-tracking,individual differences},
  numpages = {10},
  series = {{{IIiX}} '10}
}

@inproceedings{118,
  title = {An Eye-Tracking-Based Approach to Facilitate Interactive Video Search},
  booktitle = {Proceedings of the 1st {{ACM}} International Conference on Multimedia Retrieval},
  author = {Vrochidis, Stefanos and Patras, Ioannis and Kompatsiaris, Ioannis},
  year = {2011},
  publisher = {{Association for Computing Machinery}},
  address = {{Trento, Italy}},
  doi = {10.1145/1991996.1992039},
  articleno = {Article 43},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\Q3LV4BCL\\2011-Vrochidis et al-An eye-tracking-based approach to facilitate.pdf},
  isbn = {978-1-4503-0336-1},
  keywords = {-video-,[int-I],[int-L],eye-tracking,implicit feedback,interactive,machine learning,search engine,support vector machines,video retrieval},
  numpages = {8},
  series = {{{ICMR}} '11}
}

@article{119,
  title = {An Eye-Tracking Approach to the Analysis of Relevance Judgments on the {{Web}}: {{The}} Case of {{Google}} Search Engine},
  author = {Balatsoukas, Panos and Ruthven, Ian},
  year = {2012},
  volume = {63},
  pages = {1728--1746},
  doi = {10.1002/asi.22707},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.22707},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\DK5WJRVY\\2012-Balatsoukas-Ruthven-An eye-tracking approach to the analysis of.pdf},
  journal = {Journal of the American Society for Information Science and Technology},
  keywords = {[int-L],information seeking,judgment,user studies},
  number = {9}
}

@article{120,
  title = {Window to the Wandering Mind: {{Pupillometry}} of Spontaneous Thought While Reading},
  author = {Franklin, Michael S. and Broadway, James M. and Mrazek, Michael D. and Smallwood, Jonathan and Schooler, Jonathan W.},
  year = {2013},
  volume = {66},
  pages = {2289--2294},
  journal = {Quarterly Journal of Experimental Psychology},
  number = {12}
}

@inproceedings{121,
  title = {Looking Ahead: {{Query}} Preview in Exploratory Search},
  booktitle = {Proceedings of the 36th International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Qvarfordt, Pernilla and Golovchinsky, Gene and Dunnigan, Tony and Agapie, Elena},
  year = {2013},
  pages = {243--252},
  publisher = {{Association for Computing Machinery}},
  address = {{Dublin, Ireland}},
  doi = {10.1145/2484028.2484084},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4Y9PBCNG\\2013-Qvarfordt et al-Looking ahead.pdf},
  isbn = {978-1-4503-2034-4},
  keywords = {[int-Q],exploratory search,hcir,information retrieval,information seeking},
  numpages = {10},
  series = {{{SIGIR}} '13}
}

@inproceedings{122,
  title = {Predicting Query Reformulation Type from User Behavior},
  booktitle = {Proceedings of the 28th Annual {{ACM}} Symposium on Applied Computing},
  author = {Umemoto, Kazutoshi and Yamamoto, Takehiro and Nakamura, Satoshi and Tanaka, Katsumi},
  year = {2013},
  pages = {894--901},
  publisher = {{Association for Computing Machinery}},
  address = {{Coimbra, Portugal}},
  doi = {10.1145/2480362.2480534},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\5MT4PB5R\\2013-Umemoto et al-Predicting query reformulation type from user.pdf},
  isbn = {978-1-4503-1656-9},
  keywords = {[int-Q],log analysis,search intent,user behavior},
  numpages = {8},
  series = {{{SAC}} '13}
}

@article{123,
  title = {Search Result List Evaluation versus Document Evaluation: Similarities and Differences},
  shorttitle = {Search Result List Evaluation versus Document Evaluation},
  author = {Xie, Iris and Benoit, Edward},
  year = {2013},
  month = jan,
  volume = {69},
  pages = {49--80},
  issn = {0022-0418},
  doi = {10.1108/00220411311295324},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\QVLVLDMJ\\Xie and Benoit - 2013 - Search result list evaluation versus document eval.pdf},
  journal = {Journal of Documentation},
  keywords = {[int-I],[int-L],nb-star},
  language = {en},
  number = {1}
}

@inproceedings{124,
  title = {A Comparative Study about Children's and Adults' Perception of Targeted Web Search Engines},
  booktitle = {Proceedings of the {{SIGCHI}} Conference on Human Factors in Computing Systems},
  author = {Gossen, Tatiana and H{\"o}bel, Juliane and N{\"u}rnberger, Andreas},
  year = {2014},
  pages = {1821--1824},
  publisher = {{Association for Computing Machinery}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/2556288.2557031},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\SJZ8UVTI\\2014-Gossen et al-A comparative study about children’s and adults’.pdf},
  isbn = {978-1-4503-2473-1},
  keywords = {[int-L],children,eye-tracker,search engine,user study},
  numpages = {4},
  series = {{{CHI}} '14}
}

@inproceedings{125,
  title = {An Eye-Tracking Study of User Interactions with Query Auto Completion},
  booktitle = {Proceedings of the 23rd {{ACM}} International Conference on Conference on Information and Knowledge Management},
  author = {Hofmann, Kajta and Mitra, Bhaskar and Radlinski, Filip and Shokouhi, Milad},
  year = {2014},
  pages = {549--558},
  publisher = {{Association for Computing Machinery}},
  address = {{Shanghai, China}},
  doi = {10.1145/2661829.2661922},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\CXYNP4FS\\2014-Hofmann et al-An eye-tracking study of user interactions with.pdf},
  isbn = {978-1-4503-2598-1},
  keywords = {[int-L],[int-Q],evaluation,eye tracking,query auto completion},
  numpages = {10},
  series = {{{CIKM}} '14}
}

@inproceedings{126,
  title = {Searching, Browsing, and Clicking in a Search Session: {{Changes}} in User Behavior by Task and over Time},
  booktitle = {Proceedings of the 37th International {{ACM SIGIR}} Conference on Research \& Development in Information Retrieval},
  author = {Jiang, Jiepu and He, Daqing and Allan, James},
  year = {2014},
  pages = {607--616},
  publisher = {{Association for Computing Machinery}},
  address = {{Gold Coast, Queensland, Australia}},
  doi = {10.1145/2600428.2609633},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\MKK55TRE\\2014-Jiang et al-Searching, browsing, and clicking in a search.pdf},
  isbn = {978-1-4503-2257-7},
  keywords = {[int-L],[int-Q],browsing,clicking,eye-tracking,search behavior,session,task},
  numpages = {10},
  series = {{{SIGIR}} '14}
}

@inproceedings{127,
  title = {An Eye-Tracking Study of Query Reformulation},
  booktitle = {Proceedings of the 38th International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Eickhoff, Carsten and Dungs, Sebastian and Tran, Vu},
  year = {2015},
  pages = {13--22},
  publisher = {{Association for Computing Machinery}},
  address = {{Santiago, Chile}},
  doi = {10.1145/2766462.2767703},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\ZNMWHUZ4\\2015-Eickhoff et al-An eye-tracking study of query reformulation.pdf},
  isbn = {978-1-4503-3621-5},
  keywords = {[int-Q],domain expertise,eye-gaze tracking,knowledge acquisition,mouse cursor tracking,query refinement,query reformulation,query suggestion},
  numpages = {10},
  series = {{{SIGIR}} '15}
}

@inproceedings{128,
  title = {Influence of Vertical Result in Web Search Examination},
  booktitle = {Proceedings of the 38th International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Liu, Zeyang and Liu, Yiqun and Zhou, Ke and Zhang, Min and Ma, Shaoping},
  year = {2015},
  pages = {193--202},
  publisher = {{Association for Computing Machinery}},
  address = {{Santiago, Chile}},
  doi = {10.1145/2766462.2767714},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\XJBNMWXL\\2015-Liu et al-Influence of vertical result in web search.pdf},
  isbn = {978-1-4503-3621-5},
  keywords = {[int-L],eye tracking,federated search,user behavior analysis},
  numpages = {10},
  series = {{{SIGIR}} '15}
}

@inproceedings{129,
  title = {Exploring the Use of Query Auto Completion: {{Search}} Behavior and Query Entry Profiles},
  booktitle = {Proceedings of the 2016 {{ACM}} on Conference on Human Information Interaction and Retrieval},
  author = {Smith, Catherine L. and Gwizdka, Jacek and Feild, Henry},
  year = {2016},
  pages = {101--110},
  publisher = {{Association for Computing Machinery}},
  address = {{Carrboro, North Carolina, USA}},
  doi = {10.1145/2854946.2854975},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\XCIFICWV\\2016-Smith et al-Exploring the use of query auto completion.pdf},
  isbn = {978-1-4503-3751-9},
  keywords = {[int-Q],eye-tracking.,interactive information retrieval,query auto completion,query suggestion,search behavior},
  numpages = {10},
  series = {{{CHIIR}} '16}
}

@incollection{130,
  title = {Differences in {{Reading Between Word Search}} and {{Information Relevance Decisions}}: {{Evidence}} from {{Eye}}-{{Tracking}}},
  shorttitle = {Differences in {{Reading Between Word Search}} and {{Information Relevance Decisions}}},
  booktitle = {Information {{Systems}} and {{Neuroscience}}},
  author = {Gwizdka, Jacek},
  editor = {Davis, Fred D. and Riedl, Ren{\'e} and {vom Brocke}, Jan and L{\'e}ger, Pierre-Majorique and Randolph, Adriane B.},
  year = {2017},
  volume = {16},
  pages = {141--147},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-41402-7-18},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\SUKVQ7YJ\\2017-Gwizdka-Differences in Reading Between Word Search and.pdf},
  isbn = {978-3-319-41401-0 978-3-319-41402-7},
  keywords = {nb-star},
  language = {en}
}

@article{131,
  title = {Natural Search User Interfaces for Complex Biomedical Search: {{An}} Eye Tracking Study},
  author = {Liu, Ying-Hsang and Thomas, Paul and Bacic, Marijana and Gedeon, Tom and Li, Xindi},
  year = {2017},
  volume = {66},
  pages = {364--381},
  publisher = {{Routledge}},
  doi = {10.1080/24750158.2017.1357915},
  eprint = {https://doi.org/10.1080/24750158.2017.1357915},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\P7Q2ZKHL\\2017-Liu et al-Natural search user interfaces for complex.pdf},
  journal = {Journal of the Australian Library and Information Association},
  keywords = {-66-,[int-L]},
  number = {4}
}

@inproceedings{132,
  title = {A Comparative User Study of Interactive Multilingual Search Interfaces},
  booktitle = {Proceedings of the 2018 Conference on Human Information Interaction \& Retrieval},
  author = {Ling, Chenjun and Steichen, Ben and Choulos, Alexander G.},
  year = {2018},
  pages = {211--220},
  publisher = {{Association for Computing Machinery}},
  address = {{New Brunswick, NJ, USA}},
  doi = {10.1145/3176349.3176383},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\EZSYKBFY\\2018-Ling et al-A comparative user study of interactive.pdf},
  isbn = {978-1-4503-4925-3},
  keywords = {[int-L],[int-Q],eye tracking,human-computer information retrieval,multilingual interfaces,multilingual search},
  numpages = {10},
  series = {{{CHIIR}} '18}
}

@article{133,
  title = {How Does Domain Expertise Affect Users' Search Interaction and Outcome in Exploratory Search?},
  author = {Mao, Jiaxin and Liu, Yiqun and Kando, Noriko and Zhang, Min and Ma, Shaoping},
  year = {2018},
  month = jul,
  volume = {36},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  journal = {ACM Transactions on Information Systems}
}

@article{134,
  title = {Examining the Impact of Domain and Cognitive Complexity on Query Formulation and Reformulation},
  author = {Wildemuth, Barbara M. and Kelly, Diane and Boettcher, Emma and Moore, Erin and Dimitrova, Gergana},
  year = {2018},
  month = may,
  volume = {54},
  pages = {433--450},
  issn = {03064573},
  doi = {10.1016/j.ipm.2018.01.009},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\F53Y4NUJ\\Wildemuth et al. - 2018 - Examining the impact of domain and cognitive compl.pdf},
  journal = {Information Processing \& Management},
  keywords = {nb-star},
  language = {en},
  number = {3}
}

@inproceedings{135,
  title = {Patterns of Search Result Examination: {{Query}} to First Action},
  booktitle = {Proceedings of the 28th {{ACM}} International Conference on Information and Knowledge Management},
  author = {Abualsaud, Mustafa and Smucker, Mark D.},
  year = {2019},
  pages = {1833--1842},
  publisher = {{Association for Computing Machinery}},
  address = {{Beijing, China}},
  doi = {10.1145/3357384.3358041},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\CDWCK4GF\\2019-Abualsaud-Smucker-Patterns of search result examination.pdf},
  isbn = {978-1-4503-6976-3},
  keywords = {[int-Q],query abandonment,requery behavior,user behavior,user study},
  numpages = {10},
  series = {{{CIKM}} '19}
}

@inproceedings{136,
  title = {Understanding the {{Influence}} of {{Topic Familiarity}} on {{Search Behavior}} in {{Digital Libraries}}},
  booktitle = {Proceedings of {{SIGIR}} 2019 {{Workshop}} on {{ExplainAble Recommendation}} and {{Search}} ({{EARS}}'19)},
  author = {Davari, Masoud and Yu, Ran and Dietze, Stefan},
  year = {2019},
  pages = {4},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\573LPSRM\\Davari et al. - 2019 - Understanding the Influence of Topic Familiarity o.pdf},
  keywords = {[int-Q]},
  language = {en}
}

@inproceedings{137,
  title = {Why Do {{Users Issue Good Queries}}?: {{Neural Correlates}} of {{Term Specificity}}},
  booktitle = {Proceedings of the {{42Nd}} International {{ACM SIGIR}} Conference on Research and Development in Information Retrieval},
  author = {Kangassalo, Lauri and Spap{\'e}, Michiel and Jacucci, Giulio and Ruotsalo, Tuukka},
  year = {2019},
  pages = {375--384},
  publisher = {{ACM}},
  address = {{Paris, France}},
  doi = {10.1145/3331184.3331243},
  acmid = {3331243},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\DFRX496D\\2019-Kangassalo et al-Why do users issue good queries.pdf},
  isbn = {978-1-4503-6172-9},
  keywords = {[int-Q],human neurophysiology,neural correlates,term specificity},
  numpages = {10},
  series = {{{SIGIR}}'19}
}

@inproceedings{138,
  title = {The {{Role}} of {{Word}}-{{Eye}}-{{Fixations}} for {{Query Term Prediction}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Davari, Masoud and Hienert, Daniel and Kern, Dagmar and Dietze, Stefan},
  year = {2020},
  month = mar,
  pages = {422--426},
  publisher = {{ACM}},
  address = {{Vancouver BC Canada}},
  doi = {10.1145/3343413.3378010},
  file = {C\:\\Users\\nilav\\Zotero\\storage\\4Z6847RL\\2020-Davari et al-The Role of Word-Eye-Fixations for Query Term.pdf},
  isbn = {978-1-4503-6892-6},
  keywords = {[int-Q]},
  language = {en}
}

@inproceedings{139,
  title = {Children's {{Eye}}-Fixations on {{Google Search Results}}},
  shorttitle = {Proc. {{Assoc}}. {{Info}}. {{Sci}}. {{Tech}}.},
  booktitle = {Proceedings of the 79th {{ASIS}}\&{{T Annual Meeting}}},
  author = {Bilal, Dania and Gwizdka, Jacek},
  year = {2016},
  volume = {79},
  pages = {89:1--89:6},
  publisher = {{American Society for Information Science}},
  address = {{Silver Springs, MD, USA}},
  doi = {10.1002/pra2.2016.14505301089},
  copyright = {All rights reserved},
  isbn = {2373-9231},
  keywords = {-tablet,[int-L],Children,Eye-tracking,Google,Information search,mypub,Reading,search engines,search tasks,SERPs,Snippets,UTK-Google-Project},
  series = {{{ASIST}} '16}
}

@inproceedings{140,
  title = {Analysis of {{Children}}'s {{Queries}} and {{Click Behavior}} on {{Ranked Results}} and {{Their Thought Processes}} in {{Google Search}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Conference Human Information Interaction}} and {{Retrieval}}},
  author = {Gwizdka, Jacek and Bilal, Dania},
  year = {2017},
  pages = {377--380},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3020165.3022157},
  isbn = {978-1-4503-4677-1},
  keywords = {[int-L],children,click-through,google,information search,query analysis,search engines,serps,thought processes.},
  series = {{{CHIIR}} '17}
}







@inproceedings{roy2016representing,
  title={Representing documents and queries as sets of word embedded vectors for information retrieval},
  author={Roy, Dwaipayan and Ganguly, Debasis and Mitra, Mandar and Jones, Gareth JF},
  booktitle={ACM SIGIR workshop on neural information retrieval (Neu-IR)},
  year={2016}
}

@article{chen2018visual,
  title={Visual exploration and comparison of word embeddings},
  author={Chen, Juntian and Tao, Yubo and Lin, Hai},
  journal={Journal of Visual Languages \& Computing},
  volume={48},
  pages={178--186},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{kenter2015short,
  title={Short text similarity with word embeddings},
  author={Kenter, Tom and De Rijke, Maarten},
  booktitle={Conference on Information and Knowledge Management (CIKM)},
  pages={1411--1420},
  year={2015}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{maaten2008visualizing,
  title={Visualizing data using t-SNE},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@article{zhang2016process,
  title={Process patterns and conceptual changes in knowledge representations during information seeking and sensemaking: A qualitative user study},
  author={Zhang, Pengyi and Soergel, Dagobert},
  journal={Journal of Information Science},
  volume={42},
  number={1},
  pages={59--78},
  year={2016},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{kruikemeier2018learning,
  title={Learning from news on different media platforms: An eye-tracking experiment},
  author={Kruikemeier, Sanne and Lecheler, Sophie and Boyer, Ming M},
  journal={Political Communication},
  volume={35},
  number={1},
  pages={75--96},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{gadiraju2018AnalyzingKnowledgeGain,
  title={Analyzing knowledge gain of users in informational search sessions on the web},
  author={Gadiraju, Ujwal and Yu, Ran and Dietze, Stefan and Holtz, Peter},
  booktitle={Conference on Human Information Interaction \& Retrieval (CHIIR)},
  year={2018}
}

@inproceedings{ghosh2018SearchingLearningExploring,
  title={Searching as learning: Exploring search behavior and learning outcomes in learning-related tasks},
  author={Ghosh, Souvick and Rath, Manasa and Shah, Chirag},
  booktitle={Conference on Human Information Interaction \& Retrieval (CHIIR)},
  year={2018}
}

@inproceedings{yu2018PredictingUserKnowledgea,
  address = {New York, NY, USA},
  series = {SIGIR '18},
  title = {Predicting {{User Knowledge Gain}} in {{Informational Search Sessions}}},
  isbn = {978-1-4503-5657-2},
  doi = {10.1145/3209978.3210064},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  publisher = {{ACM}},
  author = {Yu, Ran and Gadiraju, Ujwal and Holtz, Peter and Rokicki, Markus and Kemkes, Philipp and Dietze, Stefan},
  year = {2018},
  keywords = {user modeling,-tablet,web search,knowledge gain,search as learning},
  pages = {75--84}
}



@article{chen2020understanding,
  title={Understanding online health information consumers' search as a learning process},
  author={Chen, Yijin and Zhao, Yiming and Wang, Ziyun},
  journal={Library Hi Tech},
  year={2020},
  publisher={Emerald Publishing Limited}
}

@inproceedings{o2020role,
  title={The Role of Domain Knowledge in Search as Learning},
  author={O'Brien, Heather L and Kampen, Andrea and Cole, Amelia W and Brennan, Kathleen},
  booktitle={Conference on Human Information Interaction and Retrieval (CHIIR)},
  year={2020}
}

@inproceedings{urgo2019anderson,
  title={Anderson and krathwohl's two-dimensional taxonomy applied to task creation and learning assessment},
  author={Urgo, Kelsey and Arguello, Jaime and Capra, Robert},
  booktitle={International Conference on Theory of Information Retrieval (ICTIR)},
  year={2019}
}

@inproceedings{xu2020does,
  title={How Does Team Composition Affect Knowledge Gain of Users in Collaborative Web Search?},
  author={Xu, Luyan and Zhou, Xuan and Gadiraju, Ujwal},
  booktitle={Conference on Hypertext and Social Media (HT)},
  year={2020}
}

@inproceedings{roy2020exploring,
  title={Exploring Users' Learning Gains within Search Sessions},
  author={Roy, Nirmal and Moraes, Felipe and Hauff, Claudia},
  booktitle={Conference on Human Information Interaction and Retrieval (CHIIR)},
  year={2020}
}




@inproceedings{burdick2018factors,
  title={Factors Influencing the Surprising Instability of Word Embeddings},
  author={Burdick, Laura and Kummerfeld, Jonathan K and Mihalcea, Rada},
  booktitle={Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2092--2102},
  year={2018}
}

  
@inproceedings{bhattacharya2018relating,
  title={Relating eye-tracking measures with changes in knowledge on search tasks},
  author={Bhattacharya, Nilavra and Gwizdka, Jacek},
  booktitle={Symposium on Eye Tracking Research \& Applications (ETRA)},
  year={2018}
}
  

  


@book{Darwin1859,
address = {London},
author = {Darwin, Charles},
publisher = {John Murray},
title = {{On the Origin of Species by Means of Natural Selection or the Preservation of Favoured Races in the Struggle for Life}},
year = {1859}
}

@book{von-goethe-wilhelm-1829,
	title = {Wilhelm {Meisters} {Wanderjahre} oder die {Entsagenden}},
	shorttitle = {Werke},
	language = {de},
	publisher = {Cotta},
	author = {von Goethe, Johann Wolfgang},
	sortname={Goethe},
	year = {1829}
}

@article{Satchell2009,
  author =        {Satchell, Christine and Dourish, Paul},
  journal =       {Proceedings of the Annual Conference of the
                   Australian Computer-Human Interaction Special
                   Interest Group (OZCHI '09)},
  number =        {November},
  pages =         {9--16},
  title =         {{Beyond The User: Use And Non-Use in HCI}},
  year =          {2009},
  doi =           {10.1145/1738826.1738829},
  isbn =          {9781605588544},
  issn =          {14712970},
  url =           {http://douri.sh/publications/2009/nonuse-ozchi.pdf},
}

@book{Wu2016,
  author =        {Wu, Tim},
  publisher =     {Knopf Publishing Group},
  title =         {{The Attention Merchants: The Epic Scramble to Get
                   Inside Our Heads}},
  year =          {2016},
}

@article{Shea2014,
  author =        {Shea, Nicholas and Boldt, Annika and Bang, Dan and
                   Yeung, Nick and Heyes, Cecilia and Frith, Chris D},
  journal =       {Trends in Cognitive Sciences},
  number =        {4},
  pages =         {186--193},
  title =         {{Supra-personal cognitive control and metacognition}},
  volume =        {18},
  year =          {2014},
  doi =           {10.1016/j.tics.2014.01.006},
  issn =          {1364-6613},
  url =           {http://dx.doi.org/10.1016/j.tics.2014.01.006},
}

@inproceedings{Lottridge2012,
  author =        {Lottridge, Danielle and Marschner, Eli and
                   Wang, Ellen and Romanovsky, Maria and Nass, Clifford},
  booktitle =     {Proceedings of the Human Factors and Ergonomics
                   Society 56th Annual Meeting},
  title =         {{Browser design impacts multitasking}},
  year =          {2012},
  doi =           {10.1177/1071181312561289},
  isbn =          {9780945289418},
  issn =          {10711813},
}

@book{Mill1965,
  author = {Mill, John Stuart},
  Location = {London},
  Publisher = {Longmans},
  Title = {A System of Logic, Ratiocinative and Inductive},
  Subtitle = {Being a Connected View of the Principles of Evidence and the Methods of Scientific Investigation},
  Year = {1965 [1843]}}