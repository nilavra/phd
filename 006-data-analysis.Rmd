# Data Analysis

Note about pronouns:
all participants are referred to using gender-neutral they/them pronouns.

Merge `SES2a` and `SES2b` into `SES2ab`

<!--
Analysis steps:
- Check analysis ideas for more ideas
- get data for IMI, MAI, etc
-->


## Data Cleaning and Processing

see crescenzi thesis

Session: `task_id`, 30 minutes of inactivity
(Google Analytics: a session lasts until there's 30 minutes of inactivity)

Session duration is considered 30 minutes (!) (or maybe 1 hour?)
as per Google Analytics^[https://www.hotjar.com/google-analytics/glossary/session-duration] and (TODO: find reference)

## URL Categorization

<!-- ALL IN CAPS BECAUSE I WAS DOING URL CATEGORIZATION -->

- peer-reviewed publications are `PUB`s
- others are `ARTICLE`s (e.g. Wikipedia)
- if no other info, then `WEB`
- fuzzy between `WEB` and `ARTICLE` (when classified manually)
- `ARTICLE` if there is a clear author
  - except `WIKIPEDIA`, due to common parlance
  - encyclopedias
- journal homepages are `WEB`
- list of chapters in a book are `L.PUB`.
  - e.g. in detail view of
- book chapter is `PUB`

## User characteristics: Latent Profiles

- feature sets for profiling (toggle on and off)
  - `IMI`
  - `MAI`
  - `SRQ`
  - `WMC` / memory span:
    - scaling by dividing by 10 (?) (that's the max Coglab would show)

<!-- 
  - PLO: perceived learning outcome (/100): 
  - PSO: perceived search outcome (/100)
  - others?

PLO and PSO can't be used as there are multiple of them in SES1 and SES3 
-->

Memory span values normalized by 10, because "*The maximum memory span measurable with this experiment is ten*" as per CogLab output

- ~~Use LIME / SHAP and counterfactual explanations to understand which components contribute to change in Profile Membership~~
- no no; a simple examination of what feature values changed between timepoints will be enough
- use 2 groups! 2 groups is always better. easier to explain; easier to write.



## Search Behaviour Data Analysis Framework

From @white2016interactions, Table 2.1 (adapted from Bates, 1989):

- **Level 1: Move**
  - Atomic search event – for example, a query or click
  (*An identifiable thought or action that is part of information searching.*)
- **Level 2: Tactic**
  - Goal or task, including query or click chain
  (*One or several moves made to further a search*)
- **Level 3: Statagem**
  - Mission or session
  (*A larger, more complex set of thoughts and/or actions than the tactic; a stratagem consists of multiple tactics and/or moves, all of which are designed to exploit a particular search domain that is thought to contain the desired information*)
- **Level 4: Strategy**
  - Session or cross-session search task
  (*A plan, which may contain moves, tactics, and/or stratagems, for an entire information search.*)

## Level 1: Moves - Query Reformulation

- Yung Sheng's Dissertation
- [@hassan2014struggling] Table 1

Measures:

- Number of terms per Query
- Query length (characters?)
- Number of (unique) queries per search
- Number of reformulated query types
- Abandoned Queries (Percentage of queries with no clicks)
- query similarity [@hassan2014struggling]
  - average similarity between all queries to the first query in every session
  - exact match, approx match, lemma match, semantic match




## Level 1: Moves - Dwell Time (DT) - combined with URL types

URL type categories

- `Q`
- `L`
- `I`
- `NOTETAKING`
- `LONGSAL`
- `?` (`UNCLASSIFIED`)

Dwell Time categories:

- `SHORT`: 1 - 5 seconds
  - "*A time span of less than 5 seconds is a too short period for being able to read a summary and extract information*" [@he2016beyond].
- `MEDIUM`: 5 - 30 seconds
- `LONG`: > 30 seconds
  - from Handehawa's thesis, and related Shah references


Other Ideas

- do analysis of `LIB` and `LIBGUIDE` type URLs - library websites
- Dwell Time:
  - overall
  - per-task
  - per timepoint?
- sig diff in DT between
  - `SES1` and `SES3`
  - `SES2a` and `SES2d`?
- checkout: `sns.pairplot()` -- pairwise relationships in a dataset
  - https://seaborn.pydata.org/generated/seaborn.pairplot.html
- checkout seaborn's plotting capabilites!


## Level 1: Moves - Interactions

Events

- `TAB`: Parallel Browsing Events
  - `OPEN`
  - `SWITCH`
  - `CLOSE`
- `TASK`: from YASBIL events and `task_id`
  - `START`
  - `END`
- `IDLE`: user stays idle for >=1 minute [@taramigkou2018leveraging]
  - `SHORT`: 1 - 5mins
  - `MEDIUM`: 5 - 30 mins
  - `LONG`: >= 30 mins -- treat as new session

`IDLE` could be PDF reading, or hand note-taking, or truly idle.

<!-- - not to be confused with SES1, SES2, SES3 etc.
    - think of a better name?
    - treat IDLE.LONG as new session? -->


## Level 2: Implicit Features for Exploratory Search Process

From @hendahewa2016implicit and related papers.

- **Creativity** --> Information Novelty
  - **Unique Coverage:** Unique web pages visited
  - **Likelihood of Discovery:** Measurement of difficulty to find certain information
- **Exploration**
  - **Total Coverage:** Total number of content pages visited
  - **Distinct Queries:** Total number of different queries issued
  - **Query diversity:** Measurement of similarity between queries issued
- **Knowledge Discovery** --> Finding useful and relevant information
  - **Useful Coverage:** Number of pages where users spend a considerable amount of time
  - **Relevant Coverage:** Number of pages that users denote as relevant to the task

https://rucore.libraries.rutgers.edu/rutgers-lib/49207/

<!-- - low priority
- chirag sha's student; has intersting metrics like coverage, unique pages visited etc.
- https://rucore.libraries.rutgers.edu/rutgers-lib/49207/
- maybe future work? -->





## Level 2+: Search Tactics and Strategies

Pernilla Q et al's paper: Jiyin He, Pernilla Qvarfort, Martin Halvey, Gene Golovchinsky. Beyond actions: Exploring the discovery of tactics from user logs. In Information Processing & Management, vol. 52, issue 6, Nov. 2016, pp. 1200–1226.

- http://dx.doi.org/10.1016/j.ipm.2016.05.007
- https://www.pernillaq.com/exploratory-search
- [checkout the forward citations of this paper on automated log analyses](https://scholar.google.com/scholar?cites=16001618163231710088&as_sdt=5,44&sciodt=0,44&hl=en)
- like process mining!
- "*Since modern search systems may allow user interactions beyond the tactics defined in the literature the tactics may need to be extended*"


Behaviours from *Search Patterns* book

- quit
- narrow
- expand
- pearl growing / citation mining / snowballing
- pogo sticking
- thrashing




### Strategies from webpage L2 categories only

No sense of time. Only events

Tactics (per task)

- TASK.{START / END}
- Q.{types of reformulations}
- L.{`PUB` / `WEB` / `X`}
- L.click (only hyperlink clicks (?))
- I.{`PUB` / `WEB` / `ARTICLE` / X}
- I.click
- OTHER.{`X`}
- TAB.{tab events}
- IDLE.{DT categories}
  - IDLE.LONG = change of episode (or session)

<!-- - SESSION.{session events} -->


### Strategies from webpage L1 categories and Dwell Time (DT)

<!-- 
`SES2a` and `SES2b` have to be combined
(no: we are calculating per user-task, and taking care of episodes through IDLE time) 
-->

Tactics (per task)

- TASK.{START / END}
- Q.{types of reformulations}
- L.{DT categories}
- L.click (only hyperlink clicks (?))
- I.{DT categories}
- I.click
- OTHER.{DT categories}
- TAB.{tab events}
- IDLE.{DT categories}
  - IDLE.LONG = change of episode (or session)


<!-- 
- L.HOVER (?) (hover = reading?)
- SESSION.{session events}
- -->


### Strategies from webpage L2 categories and Dwell Time (DT)

Tactics (per task)

- TASK.{START / END}
- Q.{types of reformulations}
- L.{`PUB` / `WEB` / `X`}.{DT categories}
- L.click (only hyperlink clicks (?))
- I.{`PUB` / `WEB` / `ARTICLE` / X}.{DT categories}
- I.click
- OTHER.{`X`}.{DT categories}
- TAB.{tab events}
- IDLE.{DT categories}
  - IDLE.LONG =  change of episode (or session)




### Strategies from @lam2007session

- As in [@lam2007session] Sec 6 (`S` to denote a Search event and `X` to denote a non-search engine event):
  - Short Navigation: S(Start) --> X (End), with the S event limits to the first session events and the X event to the last events.
  - Topic Exploration: S --> X --> X --> X --> X --> ...
  - Methodical Results Exploration: S --> X --> S --> X --> S ...
  - Query Refinement: S --> S --> S --> S ...
- In our coding:
  - Short Navigation: QUERY --> non-Query (NQ: L / I / ?)
  - Topic Exploration: QUERY --> NQ --> NQ --> NQ --> NQ --> ...
  - etc.


- Using `WebNavigation` events and tab switches
- can do sequential pattern mining as in [@ibanez2022comparison]
  - maximal sequential pattern
  - etc.
- Other search patters:
  - `SS`: Search-engine Searches
  - `TS`: Third-party Searches using third-party online sites as search engines
  - `TE`: True Explorations of search results

### Struggling vs. Exploring

Indicators predictive of struggling [@hassan2014struggling]:

- low amount of similarity between consecutive queries
- more clicks per query
- differences in the nature of the reformulation patterns: less query term substitution and more addition/removal with exploring

<!-- Hassan et al. (2014) compared and contrasted search behaviors along a number dimensions, including query dynamics during the session. They found that there are some features that may be predictive of struggling, such as a low amount of similarity between consecutive queries, more clicks per query, as well as differences in the nature of the reformulation patterns (i.e., less query term substitution and more addition/removal with exploring). Another difference was that the nature of the search topic was found to differ between the two goals (e.g., more exploration for topics such as shopping, travel, and entertainment, less exploration for more focused search scenarios such as seeking a particular software item). Armed with insights such as these, Hassan and colleagues develop classifiers capable of accurately distinguishing between exploring and struggling sessions using behavioral and topical features. -->

### Navigators vs. Explorers

From IWSS book
(later, low priority)


<!-- 
### Original Idea

for LongSAL, tactics / states:

- Q
  - subdivided by Q reform types?
- L
- I\_short (DT <= 10s) -- rethink these durs
- I\_med ( 10 < DT <= 30s)
- I\_long (> 30s) 
  - 30s from Handehawa's thesis, and related Shah references
- opening a new tab: parallel mission?
- closing ta tab
- staying Idle (?) -- user stays idle for 1 minute -- as in taramigkou 2018 ip&m paper
- beginning and end of sessions?
- clicks?
  - clicks on L pages
  - clicks on I pages

or maybe I.type of webpage visited?
or combine DT with I.PUB / I.X? -->



### Transition analysis of Search Tactics / Strategies

Transition analysis can be applied to:

- search tactics
- tabs: parallel browsing behaviour (think hard... do we need a constant number of tabs for this to work?)
- combined (opening tab and closing tabs are tactics / events in Markov process)

Analyze / compare entropies and transition matrices across different tasks and sessions?
- e.g. SES1 tasks --> SES2 --> SES3

Help with analyses:

- Chen and Cooper (2002) used a Chi-square test to compare the distribution of transitions in search tactic transition matrices.
- [@he2016beyond p. 1220 sec 6.1] for calculation formulas for entropy of transitions
- [@krejtz2014entropy] sec 4 for calculation formulas
- [@he2016beyond p. 1220 sec 6.2] for hypotheses


## Correlation analysis

Correlation between user profiles and search tactics [Table 9 @taramigkou2018leveraging]



<!-- 
https://www.google.com/search?q=nilavra&oq=nilavra&aqs=edge..69i57j0i512j69i60l3j69i61j69i65j69i60j69i65.4522j0j4&sourceid=chrome&ie=UTF-8 
-->