[["index.html", "Welcome Abstract Acknowledgments", " LongSAL: A Longitudinal Study to Understand University Students’ Learning During Search Nilavra Bhattacharya Abstract This R Markdown template is for writing an Oxford University thesis. The template is built using Yihui Xie’s bookdown package, with heavy inspiration from Chester Ismay’s thesisdown and the OxThesis  template (most recently adapted by John McManigle). This template’s sample content include illustrations of how to write a thesis in R Markdown, and largely follows the structure from this R Markdown workshop. Congratulations for taking a step further into the lands of open, reproducible science by writing your thesis using a tool that allows you to transparently include tables and dynamically generated plots directly from the underlying data. Hip hooray! Welcome Welcome to the bs4 output from the oxforddown thesis template for R Markdown. To view/download the PDF output, click here (or click the cover image). Enjoy! Ulrik Lyngs 4 January 2021 Abstract This R Markdown template is for writing an Oxford University thesis. The template is built using Yihui Xie’s bookdown package, with heavy inspiration from Chester Ismay’s thesisdown and the OxThesis  template (most recently adapted by John McManigle). This template’s sample content include illustrations of how to write a thesis in R Markdown, and largely follows the structure from this R Markdown workshop. Congratulations for taking a step further into the lands of open, reproducible science by writing your thesis using a tool that allows you to transparently include tables and dynamically generated plots directly from the underlying data. Hip hooray! Acknowledgments This is where you will normally thank your advisor, colleagues, family and friends, as well as funding and institutional support. In our case, we will give our praises to the people who developed the ideas and tools that allow us to push open science a little step forward by writing plain-text, transparent, and reproducible theses in R Markdown. We must be grateful to John Gruber for inventing the original version of Markdown, to John MacFarlane for creating Pandoc (http://pandoc.org) which converts Markdown to a large number of output formats, and to Yihui Xie for creating knitr which introduced R Markdown as a way of embedding code in Markdown documents, and bookdown which added tools for technical and longer-form writing. Special thanks to Chester Ismay, who created the thesisdown package that helped many a PhD student write their theses in R Markdown. And a very special thanks to John McManigle, whose adaption of Sam Evans’ adaptation of Keith Gillow’s original maths template for writing an Oxford University DPhil thesis in LaTeX provided the template that I in turn adapted for R Markdown. Finally, profuse thanks to JJ Allaire, the founder and CEO of RStudio, and Hadley Wickham, the mastermind of the tidyverse without whom we’d all just given up and done data science in Python instead. Thanks for making data science easier, more accessible, and more fun for us all. "],["introduction.html", "1 Introduction 1.1 Searching as Learning: Overview 1.2 Problem Statement 1.3 Purpose of this Dissertation Proposal 1.4 Outline", " 1 Introduction 1.1 Searching as Learning: Overview Searching for information is a fundamental human activity. In the modern world, it is frequently conducted by users interacting with online search systems (e.g., web search engines), or more formally, Information Retrieval (IR) systems. As early as in 1980, Bertam Brookes, in his ‘fundamental equation’ of information and knowledge, had stated that an information searcher’s current state of knowledge is changed to a new knowledge structure by exposure to information (Brookes, 1980, p. 131). This indicates that searchers acquire new knowledge in the search process, and the same information will have different effects on different searchers’ knowledge states. Fifteen years later, (Marchionini, 1995) described information seeking as “a process, in which humans purposefully engage in order to change their state of knowledge”. Thus, we have known for quite a while that search is driven by higher-level human needs, and IR systems are a means to an end, and not the end in itself. Interactive information retrieval (IIR), a.k.a. human-computer information retrieval (HCIR) (Marchionini, 2006) refers to the study and evaluation of users’ interaction with IR systems and users’ satisfaction with the retrieved information (Borlund, 2013). Despite their technological marvels, modern IR systems falls short in several aspects of fully satisfying the higher level human need for information. In essence, IR systems are software that take, as input, some query, and return as output some ranked list of resources. “Within the context of information seeking, (search engines and IR systems) feel like they play a prominent role in our lives, when in actuality, they only play a small role: the retrieval part of \\[information\\] … Search engines don’t help us identify what we need – that’s up to us; search engines don’t question what we ask for, though they do recommend queries that use similar words. Search engines don’t help us choose a source – though they are themselves a source, and a heavily marketed one, so we are certainly compelled to choose search engines over other sources, even when other sources might have better information. Search engines don’t help us express our query accurately or precisely – though they will help with minor spelling corrections. Search engines do help retrieve information—this is the primary part that they automate. Search engines don’t help us evaluate the answers we retrieve – it’s up to us to decide whether the results are relevant, credible, true; Google doesn’t view those as their responsibility. Search engines don’t help us sensemake – we have to use our minds to integrate what we’ve found into our knowledge.” – (Ko, 2021) In recent years, the IIR research community has been actively promoting the Search as Learning (SAL) research direction. This fast-growing community of researchers propose that search environments should be augmented and reconfigured to foster learning, sensemaking, and long-term knowledge-gain. Various workshops and seminars have been organized to develop research agendas at the interaction of IIR and the Learning Sciences (Agosti et al., 2014; Allan et al., 2012; Collins-Thompson et al., 2017; Freund et al., 2013, 2014; Gwizdka et al., 2016) Additionally, special issues on Search as Learning have also been published in the Journal of Information Science (Hansen &amp; Rieh, 2016) and in the Information Retrieval Journal (Eickhoff et al., 2017). Articles in these special issued presented landmark literature reviews (Rieh et al., 2016; Vakkari, 2016), research agendas, and ideas in this direction. Overall, these works generally advocate that future research in this domain should aim to: understand the contexts in which people search to learn understand factors that can influence learning outcomes understand how search behaviours can predict learning outcomes develop search systems to better support learning and sensemaking help searchers be more critical consumers of information understand the cognitive biases fostered by existing search systems develop search engine ranking algorithms and interface tools that foster long term knowledge gain Parallelly, the Educational Science and the Learning Science research communities have also been organizing workshops and formulating research agendas to conceptualize forms of ‘new learning’ (Cope &amp; Kalantzis, 2013; Kalantzis &amp; Cope, 2012; New London Group, 1996) that are afforded by innovations in digital technologies and e-learning ecologies (Cope &amp; Kalantzis, 2017). Higher education researchers have been increasingly studying how students’ information search and information use behaviour affect and support their learning (Weber et al., 2019, 2018; Zlatkin-Troitschanskaia et al., 2021). Efforts are underway to conceptualize a theoretical framework around new forms of e-Learning that is aided and afforded by digital technologies (Amina, 2017; Cope &amp; Kalantzis, 2017). In the community’s own words: “learning today is more about navigation, discernment, induction, and synthesis” of the wide body of information present ubiquitously at every student’s fingertips (Amina, 2017). Therefore “knowing the source, finding the source, and using the information aptly is important to learn and know now more than ever before” (Cope &amp; Kalantzis, 2013). All of these interests in the intersection of searching and learning goes to emphasise that understanding learning during search is critical to improve human-information interaction. 1.2 Problem Statement A major limitation in the area of Search as Learning, Interactive IR (IIR), and more broadly, in Human-Computer Interaction (HCI) research is that, the user is examined in the short-term, typically over the course of a single experimental session in a lab (Karapanos et al., 2021; Kelly et al., 2009; Koeman, 2020; Zlatkin-Troitschanskaia et al., 2021). Very few studies exist in the search-as-learning domain that have observed the same participant over a longer period of time than a single search session (Kelly, 2006a, 2006b; Kuhlthau, 2004; Vakkari, 2001b; White et al., 2009; Wildemuth, 2004). This ephemeral approach has acute implications in any domain where learning is involved because “learning is a process that leads to change in knowledge … (which) unfolds over time” (Ambrose et al., 2010), and “…does not happen all at once”(White, 2016b). To the best of the author’s knowledge, almost no new longitudinal studies were reported in major search-as-learning literature in the last five years, that systematically studied students’ information search behaviour and information-use over the long term, in their in-situ naturalistic environment and contexts, and linked those behaviours quantitatively to the students’ learning outcomes and individual differences. Higher education students are increasingly using the Internet as their main learning environment and source of information when studying. Yet, the short term nature of research in this domain creates significant gaps in our knowledge regarding how students’ information search behaviour and information use develop over time, and how it affects their learning (Zlatkin-Troitschanskaia et al., 2021). When research in this area “relies so heavily on (short-term) lab studies, can we realistically say we are comprehensively studying human-tech interactions – when many of those interactions take place over long periods of time in real-world contexts? …An over-reliance on short studies risks inaccurate findings, potentially resulting in prematurely embracing or disregarding new concepts.” (Koeman, 2020). Current search engines and information retrieval systems “do not help us know what we want to know, …do not help us know if what we’ve found is relevant or true; and they do not help us make sense of \\[the retrieved information\\]. All they do is quickly retrieve what other people on the internet have shared” (Ko, 2021). Unless we have more long-term understanding of the nature of knowledge gain during search, the limitations of current search systems will continue to persist. Increased knowledge and understanding of students’, and more broadly searchers’, information searching and learning behaviour over time will help us to overcome the limitations of current IR systems, and transform them into rich learning spaces where “search experiences and learning experiences are intertwined and even synergized” (Rieh, 2020). The internet and digital educational technologies offer great opportunities to transform learning and the education experience. Enabled by our increased comprehension of the longitudinal searching-as-learning process, improved and validated by empirical data, we can create a new wave of fundamentally transformative educational technologies and “e-learning ecologies, that will be more engaging for learners, more effective (than traditional classroom practices), more resource efficient, and more equitable in the face of learner diversity” (Cope &amp; Kalantzis, 2017). 1.3 Purpose of this Dissertation Proposal To address the gaps in our knowledge of how information searching influences students’ learning process over time, this dissertation proposal proposes to conduct a semester-long longitudinal study (approx. 16 weeks) with university student participants. The overarching research aim is to identify how students’ online searching behaviour correlate with their learning outcomes for a particular university course. Building upon principles from the Learning Sciences (Ambrose et al., 2010; National Research Council, 2000; Novak, 2010; Sawyer, 2005), and empirical evidences from the Information Sciences (Rieh et al., 2016; Vakkari, 2016; White, 2016a), this dissertation proposal aims to situate students as learners in their naturalistic contexts, and characterised by their individual differences, measure students’ information search and information use behaviour over time, and correlate the information search behaviour with the learning outcomes for the university course. Learning, or addressing a gap in one’s knowledge, has been well established as an important motivator behind information-seeking activities (Section 1.1). Therefore, search systems that support rapid learning across a number of searchers, and a range of tasks, can be considered as more effective search systems (White, 2016a, p. 310). This dissertation proposal takes a step in this direction. “It opens great expectations for many-sided, great contribution to our knowledge on the relations between search process and learning outcomes” (Bhattacharya, 2021 anonymous reviewer). 1.4 Outline This dissertation proposal document is structured as follows. First, principles of learning and relevant background from the domain of Educational Sciences are presented in Chapter 2. Next, relevant empirical evidences from the Information Searching Literature are discussed in Chapter 3. Chapter 4 presents the research questions, the overarching hypotheses, and discusses their rationale in the context of the existing research gaps. Chapter 5 describes the research methods, including the longitudinal study design, experimental procedures, data collection and analyses plans, anticipated limitations, and expected schedule to complete the dissertation. References Agosti, M., Fuhr, N., Toms, E., &amp; Vakkari, P. (2014). Evaluation methodologies in information retrieval dagstuhl seminar 13441. ACM SIGIR Forum, 48, 36–41. Allan, J., Croft, B., Moffat, A., &amp; Sanderson, M. (2012). Frontiers, challenges, and opportunities for information retrieval: Report from SWIRL 2012 the second strategic workshop on information retrieval in lorne. ACM SIGIR Forum, 46, 2–32. Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C., &amp; Norman, M. K. (2010). How Learning Works: Seven Research-Based Principles for Smart Teaching. John Wiley &amp; Sons. Amina, T. (2017). Active knowledge making: Epistemic dimensions of e-learning. In E-learning ecologies (pp. 65–87). Routledge. Bhattacharya, N. (2021). A longitudinal study to understand learning during search. Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, 363–366. Borlund, P. (2013). Interactive Information Retrieval: An Introduction. Journal of Information Science Theory and Practice, 1(3), 12–32. https://doi.org/10.1633/JISTAP.2013.1.3.2 Brookes, B. C. (1980). The foundations of information science. Part i. Philosophical aspects. Journal of Information Science, 2(3-4), 125–133. Collins-Thompson, K., Hansen, P., &amp; Hauff, C. (2017). Search as learning (dagstuhl seminar 17092). Dagstuhl Reports, 7. Cope, B., &amp; Kalantzis, M. (2017). E-Learning Ecologies: Principles for New Learning and Assessment. Taylor &amp; Francis. Cope, B., &amp; Kalantzis, M. (2013). Towards a New Learning: The Scholar Social Knowledge Workspace, in Theory and Practice. E-Learning and Digital Media, 10(4), 332–356. https://doi.org/10.2304/elea.2013.10.4.332 Eickhoff, C., Gwizdka, J., Hauff, C., &amp; He, J. (2017). Introduction to the special issue on search as learning. Information Retrieval Journal, 20(5), 399–402. Freund, L., Gwizdka, J., Hansen, P., Kando, N., &amp; Rieh, S. Y. (2013). From searching to learning. Evaluation Methodologies in Information Retrieval. Dagstuhl Reports, 13441, 102–105. Freund, L., He, J., Gwizdka, J., Kando, N., Hansen, P., &amp; Rieh, S. Y. (2014). Searching as learning (SAL) workshop 2014. Proceedings of the 5th Information Interaction in Context Symposium, 7–7. Gwizdka, J., Hansen, P., Hauff, C., He, J., &amp; Kando, N. (2016). Search as learning (SAL) workshop 2016. Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1249–1250. Hansen, P., &amp; Rieh, S. Y. (2016). Editorial: Recent advances on searching as learning: An introduction to the special issue. Journal of Information Science, 42(1), 3–6. https://doi.org/10.1177/0165551515614473 Kalantzis, M., &amp; Cope, B. (2012). New Learning: Elements of a Science of Education. Cambridge University Press. Karapanos, E., Gerken, J., Kjeldskov, J., &amp; Skov, M. B. (Eds.). (2021). Advances in Longitudinal HCI Research. Springer International Publishing. https://doi.org/10.1007/978-3-030-67322-2 Kelly, D. (2006a). Measuring online information seeking context, Part 1: Background and method. Journal of the American Society for Information Science and Technology, 57(13), 1729–1739. https://doi.org/10.1002/asi.20483 Kelly, D. (2006b). Measuring online information seeking context, Part 2: Findings and discussion. Journal of the American Society for Information Science and Technology, 57(14), 1862–1874. https://doi.org/10.1002/asi.20484 Kelly, D., Dumais, S., &amp; Pedersen, J. O. (2009). Evaluation challenges and directions for information-seeking support systems. IEEE Computer, 42(3). Ko, A. J. (2021). Seeking information. In Foundations of Information. https://faculty.washington.edu/ajko/books/foundations-of-information/#/seeking Koeman, L. (2020). HCI/UX research: What methods do we use? – lisa koeman – blog. https://lisakoeman.nl/blog/hci-ux-research-what-methods-do-we-use/. Kuhlthau, C. C. (2004). Seeking meaning: A process approach to library and information services (Vol. 2). Libraries Unlimited Westport, CT. Marchionini, G. (1995). Information Seeking in Electronic Environments. Cambridge University Press. Marchionini, G. (2006). Toward human-computer information retrieval. Bulletin of the American Society for Information Science and Technology, 32(5), 20–22. National Research Council. (2000). How people learn: Brain, mind, experience, and school: Expanded edition. The National Academies Press. https://doi.org/10.17226/9853 New London Group. (1996). A pedagogy of multiliteracies: Designing social futures. Harvard Educational Review, 66(1), 60–92. Novak, J. D. (2010). Learning, creating, and using knowledge: Concept maps as facilitative tools in schools and corporations (2nd ed). Routledge. Rieh, S. Y. (2020). Research area 1: Searching as learning. https://rieh.ischool.utexas.edu/research. Rieh, S. Y., Collins-Thompson, K., Hansen, P., &amp; Lee, H.-J. (2016). Towards searching as a learning process: A review of current perspectives and future directions. Journal of Information Science, 42(1), 19–34. https://doi.org/10.1177/0165551515615841 Sawyer, R. K. (2005). The Cambridge handbook of the learning sciences. Cambridge University Press. Vakkari, P. (2016). Searching as learning: A systematization based on literature. Journal of Information Science, 42(1), 7–18. https://doi.org/10.1177/0165551515615833 Vakkari, P. (2001b). Changes in search tactics and relevance judgements when preparing a research proposal a summary of the findings of a longitudinal study. Information Retrieval, 4(3), 295–310. Weber, H., Becker, D., &amp; Hillmert, S. (2019). Information-seeking behaviour and academic success in higher education: Which search strategies matter for grade differences among university students and how does this relevance differ by field of study? Higher Education, 77(4), 657–678. https://doi.org/10.1007/s10734-018-0296-4 Weber, H., Hillmert, S., &amp; Rott, K. J. (2018). Can digital information literacy among undergraduates be improved? Evidence from an experimental study. Teaching in Higher Education, 23(8), 909–926. https://doi.org/10.1080/13562517.2018.1449740 White, R. (2016a). Interactions with search systems. Cambridge University Press. White, R. (2016b). Learning and use. In Interactions with search systems (pp. 231–248). Cambridge University Press. https://doi.org/10.1017/CBO9781139525305.010 White, R., Dumais, S., &amp; Teevan, J. (2009). Characterizing the influence of domain expertise on web search behavior. Proceedings of the Second ACM International Conference on Web Search and Data Mining - WSDM ’09, 132. https://doi.org/10.1145/1498759.1498819 Wildemuth, B. M. (2004). The effects of domain knowledge on search tactic formulation. Journal of the American Society for Information Science and Technology, 55(3), 246–258. https://doi.org/10.1002/asi.10367 Zlatkin-Troitschanskaia, O., Hartig, J., Goldhammer, F., &amp; Krstev, J. (2021). Students’ online information use and learning progress in higher education A critical literature review. Studies in Higher Education, 1–26. https://doi.org/10.1080/03075079.2021.1953336 "],["ch_bg_learn.html", "2 Background: Knowledge and Learning 2.1 Terminology 2.2 Principles of Meaningful Learning 2.3 Meaningful Learning as Sensemaking 2.4 ‘New’ Learning as Online Information Searching 2.5 Promoting Better Learning 2.6 Summary and Implications for this Proposal", " 2 Background: Knowledge and Learning This first chapter on background literature discusses relevant concepts from the disciplines of Education and Learning Sciences. First, we introduce some relevant terminology, and the concepts of deep or meaningful learning. Then we discuss several research backed principles that have been shown to lead to meaningful learning. Next, we discuss how learning, sensemaking, and searching for information are related, and how modern technologies provide affordances for new forms of learning and knowledge work in the 21st century. We also discuss some concepts about individual differences of learners as well as techniques that can promote better learning. In the last section, we state what implications these findings have for shaping the proposed study in this dissertation proposal. 2.1 Terminology The Webster dictionary1 defines knowledge in two ways. The first definition is “the range of one’s information or understanding”. (Vakkari, 2016) says it is “the totality what a person knows, that is, a personal knowledge or belief system. It may include both justified, true beliefs and less justified, not so true beliefs, which the person more or less thinks hold true.” Webster’s second definition of knowledge is “the sum of what is known: the body of truth, information, and principles acquired by humankind”. We can regard this as universal knowledge. Learning is a process, that leads to a change in (personal) knowledge, beliefs, behaviours, and attitudes (Ambrose et al., 2010). Thus, learning always aims to increase one’s personal knowledge, and can often draw from the body of universal knowledge. In some cases, the change in personal knowledge can also lead to change in universal knowledge, such as when new discoveries are made, or new philosophies are proposed. Human learning is an innate capacity. It is longitudinal and unfolds over time. Learning is lifelong and life-wide, and has a lasting impact on how humans think and act (Ambrose et al., 2010; Kalantzis &amp; Cope, 2012). Learning can be informal or formal. Informal learning is the casual learning taking place in everyday life, and is incidental to the everyday life experience. Formal learning is the deliberate, conscious, systematic, and explicit acquiring of knowledge (Kalantzis &amp; Cope, 2012). Education is a form of formal learning. It is the systematic acquiring of knowledge. In today’s world, the institutions of education are formally constructed places (classrooms), times (of the day and of life) and social relations (teachers and students); for instance, schools, colleges, and universities. The scientific discipline of Education concerns itself with the systematic investigation of the ways in which humans know and learn. It is the science of “coming to know” (Kalantzis &amp; Cope, 2012). Pedagogy describes small sequences of learner activities that promote learning in educational settings (Kalantzis &amp; Cope, 2012). Traditional approaches to (classroom) pedagogy, especially the didactic pedagogy, primarily involves a teacher telling, and a learner listening. The teacher is in command of the knowledge, and their mission is to transmit this knowledge to the learners, in a one-way flow. It is hoped that the learners will dutifully absorb the knowledge laid before them by the teacher. The balance of agency weighs heavily towards the teacher. “There is a special focus on long-term memory, or retention, measurable by the ritual of closed-book, summative examination” (Cope &amp; Kalantzis, 2017). image Meaningful learning (ake deep learning) as explained by Novak (2010, fig. 5.3) (annotations our own). Cognitive scientists had discovered that learners retain material better, and are able to generalize and apply it to a broader range of contexts, when they learn deep knowledge rather than surface knowledge, and when they learn how to use that knowledge in real-world social and practical settings (Sawyer, 2005). Deep learning2 takes place when “the learner chooses conscientiously to integrate new knowledge to knowledge that the learner already possesses” and involves “substantive, non-arbitrary incorporations of concepts into cognitive structure” (Novak, 2002, p. 549) and may eventually lead to the development of transferable knowledge and skills. A parallel terminology for deep learning (Marton &amp; Säaljö, 1976; Marton &amp; Säljö, 1976) is meaningful learning (Ausubel et al., 1968; Novak, 2002), and they are often contrasted with surface learning or rote learning. Table \\[tab_deep_learning_surface_learning\\] discusses some more details on deep or meaningful learning, and the limitations of traditional classroom practices to promote deep learning. Figure 1.1 describes (using a concept map) how meaningful learning can be achieved and sustained, and our annotations highlight how Search-as-learning systems can foster the same. 2.2 Principles of Meaningful Learning (Ambrose et al., 2010) have proposed several principles of (student) learning that lead to creation of deeper knowledge in learners, and help educators understand why certain teaching approaches may help or hinder learning. These principles are based on research and literature from a range of disciplines in psychology, education, and anthropology, and the authors claim they are domain independent, experience independent, and cross-culturally relevant. Students’ prior knowledge can help or hinder learning. How students organize knowledge influences how they learn and apply what they know. Students’ motivation determines, directs, and sustains what they do to learn. Goal-directed practice coupled with targeted feedback enhances the quality of students’ learning. Students’ current level of development interacts with the social, emotional, and intellectual context around the student to impact learning. To become self-directed learners, students must learn to monitor and adjust their approaches to learning. In line with the above, the US National Research Council identified several key principles about experts’ knowledge (National Research Council, 2000), that illustrate the outcome of successful learning: Experts notice features and meaningful patterns of information that are not noticed by novices. Experts have acquired a great deal of content knowledge that is organized in ways that reflect a deep understanding of their subject matter. Experts’ knowledge cannot be reduced to sets of isolated facts or propositions but, instead, reflects contexts of applicability: that is, the knowledge is ‘conditionalized’ on a set of circumstances. Experts are able to flexibly retrieve important aspects of their knowledge with little attentional effort. Though experts know their disciplines thoroughly, this does not guarantee that they are able to teach others. Experts have varying levels of flexibility in their approach to new situations. The principles of learning illustrate that both the context of learning, and the individual differences of learners moderate the learning process. The findings about expert knowledge suggests that incorporating new information into existing knowledge structures in a meaningful manner is a key aspect of learning. We discuss these concepts in more detail in the following sections. 2.3 Meaningful Learning as Sensemaking In this section, we discuss how meaningful learning can be further qualified using the concepts of sensemaking. Sensemaking3 is a process that occurs when learners connect their previously developed knowledge, ideas, abilities, and experiences together to address the uncertainty presented by a newly introduced phenomenon, problem, or piece of information (Next Generation Science Standards, 2021). A significant portion of learning is sensemaking, especially those which use recorded information or systematic discovery to learn concepts, ideas, theories, and facts in a domain (such as science or history) (P. Zhang &amp; Soergel, 2014). The phrase “figure something out” is often synonymous with sensemaking. Sensemaking is generally about actively trying to figure out the way the world works, and/or exploring how to create or alter things to achieve desired goals (Next Generation Science Standards, 2021). (Dervin &amp; Naumer, 2010) distinguish work on sensemaking in four fields: “Human Computer Interaction (HCI) (Russell’s sensemaking); Cognitive Systems Engineering (Klein’s sensemaking); Organizational Communication (Weick’s sensemaking; Kurtz and Snowden’s sense-making); and Library and Information Science (Dervin’s sense-making)”. Many theories of learning and sensemaking revolve around the concept of fitting new information into an existing or adapted knowledge structure (P. Zhang &amp; Soergel, 2014). The central idea is that knowledge is stored in human memory as structures or schemas, which comprise interconnected concepts and relationships. When new information is encountered or acquired, the learner or sensemaker needs to actively construct a revised or entirely new knowledge structure. Examples of some such theories include: the assimilation theory (theory of meaningful learning) (Ausubel et al., 1968; Ausubel, 2012; Novak, 2002; Novak, 2010); the schema theory (Rumelhart &amp; Norman, 1981; Rumelhart &amp; Ortony, 1977); and the generative learning theory (Grabowski, 1996; Wittrock, 1989); all of which have their foundations in the Piagetian concepts of assimilation and accommodation (Piaget, 1936). Assimilation means addition of new information into an existing knowledge structure. A “synonym” (Vakkari, 2016) for assimilation is accretion, which is the gradual addition of factual information to an existing knowledge structure, without structural changes. Accretion does not change concepts and their relations in the structure, but may populate a concept with new instances or facts. Accommodation means modifying or changing existing knowledge structures, by adding or removing concepts and their connections in the knowledge structure. Accommodation is subdivided into tuning / weak-revision, and restructuring, based on the degree of structural changes (P. Zhang &amp; Soergel, 2014). Tuning or weak revision does not include replacing concepts or connections between concepts in the structure, but tuning of the scope and meaning of concepts and their connections. This may include, for example, generalizing or specifying a concept. Restructuring means radically changing and replacing concepts and their connections in the existing knowledge structure, or creating of new structures. Such radical changes often take place when prior knowledge conflicts with new information. New structures are constructed either to reinterpret old information or to account for new information (Vakkari, 2016; P. Zhang &amp; Soergel, 2014). A comparison of these types of conceptual changes can be found in (P. Zhang &amp; Soergel, 2014 Table 3). 2.3.1 Concept Maps to enhance Sensemaking Concept Map showing the key features of Concept Maps (Moon et al., 2011, fig. 1.1). As we saw in the previous section, deep learning / meaningful learning / sensemaking is a process in which new information is connected to a relevant area of a learner’s existing knowledge structure. However, the learner must choose to do this, and must actively seek a way to integrate the new information with existing relevant information in their cognitive structure (Ausubel et al., 1968; Novak, 2010). Learning facilitators (e.g., teachers) can encourage this choice by using the concept mapping technique. A concept-map is a two-dimensional, hierarchical node-link diagram (a graph in Computer Science parlance) that depicts the structure of knowledge within a discipline, as viewed by a student, an instructor, or an expert in a field or sub-field. The map is composed of concept labels, each enclosed in a box (graph nodes); a series of labelled linking lines (labelled edges); and an inclusive, general-to-specific organization (Halttunen &amp; Jarvelin, 2005). Concept-maps assess how well students see the ‘’big picture’’, and where there are knowledge-gaps and misconceptions. A mind map is a diagram similar to a concept map, comprising nodes and links between nodes. However, mind maps emerge from a single centre, and have a more hierarchical, tree like structure. Concept maps are more free-form, allowing multiple hubs and clusters. Also, mind-maps have unlabelled links, and are subjective to the creator. There are no “correct” relationships between nodes in a mind map. Figure  1.2 shows the key features of a concept map, with the help of a concept map. Concept maps are therefore, arguably the most suited mechanism to represent the cognitive knowledge structures, connections, and patterns in a learner’s mind. Conventional tests, such as multiple choice questions, are best at assessing students’ recall of facts and guessing skills. Their format treats information as distinct and separate items, rather than interconnected pieces of a bigger picture. Concept maps on the other hand, encourage learners to identify and make connections between concepts that they know, and concepts that are new to them. Concept maps have been used for over 50 years to provide a useful and visually appealing way of illustrating and assessing learners’ conceptual knowledge (Egusa et al., 2010, 2014a, 2014b, 2017; Halttunen &amp; Jarvelin, 2005; Novak, 2010; Novak &amp; Gowin, 1984). Analysis of concept maps can reveal interesting patterns of learning and thinking. Some of these measures that have been used by (Halttunen &amp; Jarvelin, 2005) are: addition, deletion, and differences in top-level concept-nodes; depths of hierarchy; and number of concepts that were ignored or changed fundamentally. In this regard, (Novak &amp; Gowin, 1984) have presented well-established scoring schemes to evaluate concept-maps: 1 point is awarded for each correct relationship (i.e. concept–concept linkage); 5 points for each valid level of hierarchy; 10 points for each valid and significant cross-link; and 1 point for each example. Having discussed how deep learning / meaningful learning / sensemaking involves creation of knowledge structures in the learner’s mind, and suitably adding new pieces of information in the knowledge structure, we now discuss how these processes are influenced in the 21st century with the presence of new media, digital technologies, and information retrieval systems. 2.4 ‘New’ Learning as Online Information Searching Digital media technologies and e-learning ‘ecologies’ can enable new forms and models of learning, that are fundamentally different from the traditional classroom practices of didactic pedagogy (Cope &amp; Kalantzis, 2017). Some key concepts associated with these forms of ‘new learning’ are described below. These concepts from the Educational Sciences domain tie back strongly to the issues, challenges, and research agenda being investigated by researchers in the Search as Learning and Information Retrieval domain (Section \\[sec_intro_overview\\]). 2.4.1 Active Knowledge Making The Internet and new forms of media provide us the opportunity to create learning environments where learners are no longer mainly consumers of knowledge, but also modifiers, producers, and exchangers of knowledge. In active knowledge making, learners can, and often need to, find information on their own using online resources. They are not restricted to the textbook alone. The Internet is often a definitive resource for information on any given topic. A learner can search the web (to learn) at any time, from anywhere, on any web-enabled device. As knowledge producers, learners search and analyze multiple sources with differing and contradictory perspectives, and develop their own observations and conclusions. In this process, they become researchers themselves and learn to collaborate with peers in knowledge production. Collaboration gives learners the opportunity to work with others as coauthors of knowledge, peer reviewers, and discussants to completed works. Because learners bring their own views, outlooks, and experiences, the knowledge artefact they create is often uniquely voiced instead of a templated “correct” response (Amina, 2017). Learners become active knowledge producers (for instance, project-based learning, using multiple knowledge sources, and research based knowledge making), and not merely knowledge consumers (as exemplified in the ‘transmission’ pedagogies of traditional textbook learning or e-learning focused on video or e-textbook delivery). Active knowledge making practices underpin contemporary emphases on innovation, creativity and problem solving, which are quintessential ‘knowledge economy’ and ‘knowledge society’ attributes. – (Cope &amp; Kalantzis, 2017) 2.4.2 Artefacts for Learning Assessment Traditionally, the focus of learning outcomes has been long term memory. Students and learners were expected to remember a collection of facts, definitions, proofs, equations, and other associated details. For a significant amount of modern knowledge-work today, memory is actually less important. Information is so readily accessible now that it is no longer necessary to remember the information. Because of the technological phenomenon, the mass of information is available ubiquitously4 to a learner (or a knowledge worker), in every moment of learning. Empirical details such as facts, definitions, proofs, or equations do not need to be remembered today, because they can always be looked up again (Amina, 2017; Cope &amp; Kalantzis, 2017). This creates an interesting shift in the focus of learning and knowledge work today: “if we are not going to measure and value long-term memory in education, what are we going to assess?” (Cope &amp; Kalantzis, 2017) suggest that we assess the knowledge artefacts that learners produce. In active knowledge making, the final work5 can be proof of the learning outcome and represent a learner’s ability to use the resources that are available (Amina, 2017). Measure of learning can be measure of information quality and information use in artefacts. This shows a shift in pedagogy and assessment and an increase in personalization and individualization of learning (Pea &amp; Jacks, 2014). Memorizing the information on a topic is less important, compared to the writing, synthesizing, analyzing, and sensemaking of the available information that has been referenced in the work. This shifts the focus of assessment to the quality of the artefacts and the processes of their construction. Moreover, as technology increases the ability to capture detailed data from formal and informal learning activities, it can give us a new view of how learners progress in acquiring knowledge, skills, and attributes (DiCerbo &amp; Behrens, 2014). Because learning is a continuous, longitudinal process, these advanced, technologically enhanced assessments are more useful in understanding the learning process and knowledge development (Amina, 2017). Assessing open-ended artefacts does come with its challenges and limitations. First, assessing and grading artefacts requires the development of detailed qualitative coding guides (M. J. Wilson &amp; Wilson, 2013). This process involves defining grading criteria and measuring inter-coder agreement to ensure that the coding guide is reliable. Prior studies have scored summaries along dimensions such as the inclusion of facts, relationships between facts, and evaluative statements (Lei et al., 2015; Roy et al., 2021; M. J. Wilson &amp; Wilson, 2013). Second, the quality of responses may be difficult to compare across learners. Since this type of assessment imposes very few constraints on the learners’ responses, it may cause some learners to satisfice, and not convey everything that was learned. Additionally, writing skills are likely to vary across learners, and some may not be able to effectively articulate everything that was learnt. 2.4.3 ‘Information Search and Evaluation’ as and for Learning Learning today is more about navigation, discernment, induction, and synthesis, and less about memory and deduction (Cope &amp; Kalantzis, 2013). However, knowing the source, finding the source, and using the information critically is important to learn and know now more than ever before (Amina, 2017). Learners must know the social sources of knowledge and understand and correctly use quotations, paraphrases, remixes, links, citations, and the like in the works that they develop. Searching and sourcing from the web entails a process of developing and completing a work that inevitably makes learners knowledge producers, as long as they can navigate and critically discern the value of multiple sources. This is a skill that must be learned, as many sources of information are not valid, reliable, or authentic (McGrew et al., 2018; Wineburg &amp; McGrew, 2016). Understanding the different sources and identifying the more reliable ones are essential for effective teaching and learning (McGrew et al., 2017; McGrew, 2021). This is a critical aspect because the inability to cite properly or to use reliable resources provides learners with misconstrued information and ideas (Amina, 2017; Breakstone et al., 2021; McGrew et al., 2017). The Stanford History Education Group (SHEG) conceptualised the Civic Online Reasoning (COR) curriculum6 to enable students to effectively search for and evaluate online information (Breakstone et al., 2021; Breakstone et al., 2018; McGrew, 2020). The curriculum centres on asking three questions of any digital content: (i) who is behind a piece of information? (ii) what is the evidence for a claim? (iii) what do other sources say? The curriculum has lessons and assessments for information evaluation skills such as lateral reading (Wineburg &amp; McGrew, 2017), identifying news versus opinions, checking domain names, identifying sponsored content, evaluating evidence, and practising click restraint (McGrew &amp; Glass, 2021). The lessons were developed and piloted by the Stanford History Education Group (McGrew et al., 2018; McGrew, 2020; McGrew &amp; Glass, 2021). Taken together, these strategies will allow academics and students to better evaluate digital content, from the perspectives of professional fact checkers. The purview of the Civic Online Reasoning curriculum is more targeted than the expansive fields of media and digital literacy7, (which can embrace topics ranging from cyberbullying to identity theft). Civic Online Reasoning focuses squarely on how to sort fact from fiction online, a prerequisite for responsible civic engagement in the twenty-first century (Breakstone et al., 2021; Kahne et al., 2012; Mihailidis &amp; Thevenin, 2013). 2.5 Promoting Better Learning “It is not the technology that makes a difference; it is the pedagogy.”   – (Cope &amp; Kalantzis, 2017) Having discussed how meaningful learning takes place, and how it is influenced by the presence of digital media and the mass of information on the Internet, let us now look deeper into the learners as persons themselves. In this section, we discuss how different cognitive and metacognitive practices and aspects of learners can promote better learning. These phenomena have important implications for any digital systems that aim to foster learning. 2.5.1 Externalization and Articulation The learning sciences have discovered that when learners externalize and articulate their developing knowledge, they learn more effectively (National Research Council, 2000). Best learning takes place when learners articulate their unformed and still developing understanding, and continue to articulate it throughout the process of learning. This phenomenon was first studied in the 1920s by Russian psychologist Lev Vygotsky. Articulating and learning go hand in hand, in a mutually reinforcing feedback loop. Often learners do not actually learn something until they start to articulate it. While thinking out loud, they learn more rapidly and deeply than while studying quietly (Sawyer, 2005). The learning sciences community is actively researching how to support students in their ongoing process of articulation, and which forms of articulation are the most beneficial to learning. Articulation is more effective if it is scaffolded – channelled so that certain kinds of knowledge are articulated, and in a certain form that is most likely to result in useful reflection (Sawyer, 2005). Students need help in articulating their developing understandings, as they do not yet know how to think about thinking, or talk about thinking; their knowledge state is anomalous (Belkin et al., 1982). 2.5.2 Metacognition and Reflection image One of the reasons that articulation is so helpful to learning is that it promotes reflection or metacognition. Metacognition, commonly referred to as thinking about thinking, involves thinking at a higher level of abstraction, which in turn improves thinking and learning (Blanken-Webb, 2017). It is “the process of reflecting on and directing one’s own thinking” (National Research Council, 2000, p. 78), and involves thinking about the process of learning, and thinking about knowledge. This ties forward to the self-regulation that effective learners exhibit (Section 1.5.4. Effective learners are aware of their learning process, and can measure how efficiently they are learning as they study. The literature on metacognition broadly identifies two fundamental components of metacognition: knowledge about cognition, and regulation of cognition. Knowledge about cognition includes three subprocesses that facilitate the reflective aspect of metacognition: declarative knowledge (knowledge about self and about strategies), procedural knowledge (knowledge about how to use strategies), and conditional knowledge (knowledge about when and why to use strategies). Regulation of cognition include a number of subprocesses that facilitate the control aspect of learning. Five component skills of regulation have been discussed extensively in the literature, including planning, information management strategies, comprehension monitoring, debugging strategies, and evaluation. The operational definitions of these components are described in Table \\[tab_metacognition_components\\]. (Schraw &amp; Dennison, 1994) developed the Metacognitive Awareness Inventory (MAI) survey and a scoring guide to measure these self-reported components and subprocesses of metacognition. The original survey consists of 52 true/false questions (Appendix \\[app_mai\\]), such as “I consider several alternatives to a problem before I answer”, “I understand my intellectual strengths and weaknesses”, “I have control over how well I learn”, and “I change strategies when I fail to understand”. The instrument has been widely used in research, and has its reliability and validity measures available. Later, (Terlecki &amp; McMahon, 2018) proposed a revised version of the MAI, using five-point Likert-scales, ranging from “I never do this” to “I do this always”. They argue that when measuring change in metacognition over time, the Likert-scale based ‘how often’ questions are more effective than dichotomous ‘Yes/No’ questions (Terlecki, 2020; Terlecki &amp; McMahon, 2018). 2.5.3 Motivation The motivation and self-determination continuum, as proposed by the Self-Determination Theory (SDT). Figure adapted from (Guyan, 2013; Ryan &amp; Deci, 2000a, 2000b). Motivation is the process that initiates, guides, and maintains goal-oriented behaviours (Cherry, 2020). The Self-Determination Theory (SDT) represents a broad framework for the study of human motivation and personality (Ryan &amp; Deci, 2017). SDT differentiates the types of motivation based on the reasons that give rise to behaviour: intrinsic motivation and extrinsic motivation. Intrinsic motivation is engaging in a task or behaviour for the rewards inside the task or behaviour, such the pleasure, enjoyment and satisfaction that the behaviour provides. It is a stable form of motivation. Extrinsic motivation is engaging in a task or behaviour for the rewards outside the task or behaviour, such as receiving rewards, avoidance of punishment, gaining social approval, or achievement of a valued result. Extrinsic motivation is on a continuum from less stable to more stable, as illustrated in Figure 1.3. Extrinsic motivation does not last unless the rewards and punishments are explicitly visible (Deci &amp; Ryan, 2013; Ryan &amp; Deci, 2000b; Tahamtan, 2019). (Ryan, 1982) proposed the Intrinsic Motivation Inventory (IMI) (Appendix \\[app_imi\\]), a multidimensional questionnaire intended to assess participants’ subjective experience related to a target activity in laboratory experiments. The instrument assesses participants’ interest/enjoyment, perceived competence, effort, value/usefulness, felt pressure and tension, and perceived choice while performing a given activity, yielding six subscale scores. The interest/enjoyment subscale is considered the most indicative self-report measure of intrinsic motivation. The perceived choice and perceived competence concepts are theorized to be positive predictors of both self-report and behavioral measures of intrinsic motivation. The pressure/tension is theorized to be a negative predictor of intrinsic motivation. Effort is a separate variable that is relevant to some motivation questions, so it is used if its relevant. The value/usefulness subscale is used to measure internalization, with the idea being that people internalize and become self-regulating with respect to activities that they experience as useful or valuable for themselves. 2.5.4 Self-regulation Self-regulation is the ability to develop, implement, and flexibly maintain planned behaviour in order to achieve one’s goals. Self-regulation, and more broadly, self-direction, are critical to being an effective “lifelong” learner. Self-regulation becomes increasingly important at higher levels of education and in professional life, as people take on more complex tasks and greater responsibilities for their own learning. However, these metacognitive skills tend to fall outside the content area of most courses, and therefore, often neglected in instruction (Ambrose et al., 2010, p. 191). Building on the foundational work of (Kanfer, 1970b, 1970a), Miller and Brown formulated a seven-step model of self-regulation (J. Brown, 1998; W. R. Miller &amp; Brown, 1991). In this model, behavioural self-regulation may falter because of failure or deficits at any of these seven steps: (i) receiving relevant information, (ii) evaluating the information and comparing it to norms, (iiii) triggering change, (iv) searching for options, (v) formulating a plan, (vi) implementing the plan, and (vii) assessing the plan’s effectiveness (which recycles to steps (i) and (ii)). Although this model was developed specifically to study addictive behaviours, the self-regulatory processes it describes are meant to be general principles of behavioural self-control. (J. M. Brown et al., 1999) developed the Self-Regulation Questionnaire (SRQ) (Appendix \\[app_srq\\]) to assess these self-regulatory processes through self-report. The items were developed to mark each of the seven sub-processes of the (W. R. Miller &amp; Brown, 1991) model, forming seven subscales of the SRQ. The 63-item scale elicits responses in the form of 5-point Likert scale, ranging from strongly disagree to strongly agree. Based on clinical and college samples, the authors tentatively recommend a score of 239 and above as high (intact) self-regulation capacity (top quartile), 214-238 as intermediate (moderate) self-regulation capacity (middle quartiles), and 213 and below as low (impaired) self-regulation capacity (bottom quartile). 2.5.4.1 Self-directed and Self-regulated Learning As we saw in the previous sections, self-regulation, motivation, and metacognition are key concepts that moderate the learning process. These terms are couched in the concepts of self-regulated learning and self-directed learning. Self-directed learning (SDL) is a “process in which individuals take the initiative, with or without the help from others, in diagnosing their learning needs, formulating goals, identifying human and material resources, choosing and implementing appropriate learning strategies, and evaluating learning outcomes”(Knowles, 1975, p. 18). Self-regulated learning (SRL) can be described as the degree to which students are “metacognitively, motivationally, and behaviourally active participants in their own learning process” (Zimmerman, 1989, p. 329). Self-directed learning vs. self-regulated learning, as illustrated by (Saks &amp; Leijen, 2014). Often used interchangeably, self-directed learning (SDL) and self-regulated learning (SRL) have some important similarities and differences (Figure 1.4) (Saks &amp; Leijen, 2014). SDL, originating from adult education, is a broader, macro-level construct, and is usually practised outside the traditional school environment. The self-directed learner is free to design their own learning environment, and free to plan and set their own learning goals. SRL, on the other hand, is a narrower, micro-level construct, originating from educational and cognitive psychology, and is mostly utilized in the school environment. Learners do not have as much freedom as in SDL. The instructor or facilitator often defines the learning task and the learning goals. Self-directed learning may include self-regulated learning, but the converse is not true (Jossberger et al., 2010; Loyens et al., 2008). In other words, “a self-directed learner is supposed to self-regulate, but a self-regulated learner may not self-direct” (Saks &amp; Leijen, 2014). Despite their differences, SDL and SRL share key similarities (Saks &amp; Leijen, 2014). First, both can be seen in two dimensions: (i) external to the learner, as a process or series of events, and (ii) internal to the learner, arising from the learner’s personality, aptitude, and individual differences. Second, both the learning processes have four key phases: (i) defining tasks, (ii) setting goals and planning, (iii) enacting strategies, and (iv) monitoring and reflecting. Third, both SDL and SRL require active participation, goal-directed behaviour, metacognition, and intrinsic motivation. In summary, metacognition is monitoring and controlling what is in the learner’s head; self-regulation is monitoring and controlling how the learner interacts with their environment; self-regulated learning is the application of metacognition and self-regulation to learning (Mannion, 2020); and the whole learning process is sustained by motivation, which is desirable to be intrinsic. 2.6 Summary and Implications for this Proposal In this first chapter of the background literature review, we discussed (i) what is meaningful learning, a.k.a. deep learning, or sensemaking; (ii) how meaningful learning updates the learner’s cognitive knowledge structure; (iii) how the learning process is influenced by digital technologies, mass of information on the Internet, and IR systems; and (iv) what principles and practices learners and educators must realize and follow to promote meaningful learning. These findings are from the domains of Educational Sciences, Learning Sciences and Cognitive Sciences. We argue that these are important aspects to be considered when designing future IR or educational information systems that aim to combine and improve the searching and learning experience. Guided by these findings, we make some important decision choices for the proposed longitudinal study in this dissertation proposal. We aim to situate learners in their context, and incorporate their individual differences using metacognition, motivation, and self-regulation characteristics. Additionally, we aim to assess learning using artefacts and concept maps. We choose not use traditional tests like question-answers, and multiple choice assignments, since they are often not the preferred choice of knowledge-work output in real world scenarios. Concept maps are better suited to represent the learning and sensemaking process, and artefacts are better able to demonstrate a learner’s knowledge work. In the next chapter, we look at relevant literature from the Information Sciences and Interactive Information Retrieval disciplines. References Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C., &amp; Norman, M. K. (2010). How Learning Works: Seven Research-Based Principles for Smart Teaching. John Wiley &amp; Sons. Amina, T. (2017). Active knowledge making: Epistemic dimensions of e-learning. In E-learning ecologies (pp. 65–87). Routledge. Ausubel, D. P. (2012). The acquisition and retention of knowledge: A cognitive view. Springer Science &amp; Business Media. Ausubel, D. P., Novak, J. D., Hanesian, H., et al. (1968). Educational psychology: A cognitive view (Vol. 6). Holt, Rinehart; Winston New York. Belkin, N. J., Oddy, R. N., &amp; Brooks, H. M. (1982). ASK for information retrieval: Part i. Background and theory. Journal of Documentation. Blanken-Webb, J. (2017). Metacognition: Cognitive dimensions of e-learning. In E-learning ecologies (pp. 163–182). Routledge. Breakstone, J., McGrew, S., Smith, M., Ortega, T., &amp; Wineburg, S. (2018). Why we need a new approach to teaching digital literacy. Phi Delta Kappan, 99(6), 27–32. Breakstone, J., Smith, M., Wineburg, S., Rapaport, A., Carle, J., Garland, M., &amp; Saavedra, A. (2021). Students’ Civic Online Reasoning: A National Portrait. Educational Researcher. https://doi.org/10.3102/0013189X211017495 Brown, J. (1998). Self-regulation and the addictive behaviours. New York: Plenum Press. Brown, J. M., Miller, W. R., &amp; Lawendowski, L. A. (1999). The self-regulation questionnaire. In V. L. &amp; J. T. L. (Eds.), Innovations in clinical practice: A sourcebook (Vol. 17, pp. 281–292). Professional Resource Press/Professional Resource Exchange. Cherry, K. (2020). What Is Motivation? In Verywell Mind. https://www.verywellmind.com/what-is-motivation-2795378 Collins, C. (2021). Reimagining Digital Literacy Education to Save Ourselves. Learning for Justice, Fall 2021. https://www.learningforjustice.org/magazine/fall-2021/reimagining-digital-literacy-education-to-save-ourselves Cope, B., &amp; Kalantzis, M. (2017). E-Learning Ecologies: Principles for New Learning and Assessment. Taylor &amp; Francis. Cope, B., &amp; Kalantzis, M. (2013). Towards a New Learning: The Scholar Social Knowledge Workspace, in Theory and Practice. E-Learning and Digital Media, 10(4), 332–356. https://doi.org/10.2304/elea.2013.10.4.332 Deci, E. L., &amp; Ryan, R. M. (2013). Intrinsic motivation and self-determination in human behavior. Springer Science &amp; Business Media. Dervin, B., &amp; Naumer, C. M. (2010). Sense-making. In M. J. Bates &amp; M. M. N. (Eds.), Encyclopedia of library and information sciences (3rd ed.) (pp. 4696--4707). Taylor; Francis. DiCerbo, K. E., &amp; Behrens, J. T. (2014). Impacts of the digital ocean on education. London: Pearson, 1. Egusa, Y., Saito, H., Takaku, M., Terai, H., Miwa, M., &amp; Kando, N. (2010). Using a Concept Map to Evaluate Exploratory Search. Proceedings of the Third Symposium on Information Interaction in Context, 175–184. https://doi.org/10.1145/1840784.1840810 Egusa, Y., Takaku, M., &amp; Saito, H. (2014a). How Concept Maps Change if a User Does Search or Not? Proceedings of the 5th Information Interaction in Context Symposium, 68–75. https://doi.org/10.1145/2637002.2637012 Egusa, Y., Takaku, M., &amp; Saito, H. (2014b). How to evaluate searching as learning. Searching as Learning Workshop (IIiX 2014 Workshop). http://www.diigubc.ca/IIIXSAL/program.html Egusa, Y., Takaku, M., &amp; Saito, H. (2017). Evaluating Complex Interactive Searches Using Concept Maps. SCST@ CHIIR, 15–17. Grabowski, B. L. (1996). Generative learning: Past, present, and future. Handbook of Research for Educational Communications and Technology, 897–918. Guyan, M. (2013). Improving Learner Motivation for eLearning. In Learning Snippets. https://learningsnippets.wordpress.com/2013/10/30/improving-learner-motivation-for-elearning/ Halttunen, K., &amp; Jarvelin, K. (2005). Assessing learning outcomes in two information retrieval learning environments. Information Processing &amp; Management, 41(4), 949–972. https://doi.org/10.1016/j.ipm.2004.02.004 Jossberger, H., Brand-Gruwel, S., Boshuizen, H., &amp; Van de Wiel, M. (2010). The challenge of self-directed and self-regulated learning in vocational education: A theoretical analysis and synthesis of requirements. Journal of Vocational Education and Training, 62(4), 415–440. Kahne, J., Lee, N.-J., &amp; Feezell, J. T. (2012). Digital media literacy education and online civic and political participation. International Journal of Communication, 6, 24. Kalantzis, M., &amp; Cope, B. (2012). New Learning: Elements of a Science of Education. Cambridge University Press. Kanfer, F. H. (1970a). Self-monitoring: Methodological limitations and clinical applications. Kanfer, F. H. (1970b). Self-regulation: Research, issues, and speculations. Behavior Modification in Clinical Psychology, 74, 178–220. Knowles, M. S. (1975). Self-directed learning: A guide for learners and teachers. New York: Association press. Lei, P.-L., Sun, C.-T., Lin, S. S., &amp; Huang, T.-K. (2015). Effect of metacognitive strategies and verbal-imagery cognitive style on biology-based video search and learning performance. Computers &amp; Education, 87, 326–339. Loyens, S. M. M., Magda, J., &amp; Rikers, R. M. J. P. (2008). Self-Directed Learning in Problem-Based Learning and its Relationships with Self-Regulated Learning. Educational Psychology Review, 20(4), 411–427. https://doi.org/10.1007/s10648-008-9082-7 Mannion, J. (2020). Metacognition, self-regulation and self-regulated learning: What’s the difference? In impact.chartered.college. https://impact.chartered.college/article/metacognition-self-regulation-regulated-learning-difference/ Marton, F., &amp; Säaljö, R. (1976). On qualitative differences in learning—ii outcome as a function of the learner’s conception of the task. British Journal of Educational Psychology, 46(2), 115–127. Marton, F., &amp; Säljö, R. (1976). On qualitative differences in learning: I—outcome and process. British Journal of Educational Psychology, 46(1), 4–11. McGrew, S. (2020). Learning to evaluate: An intervention in civic online reasoning. Computers &amp; Education, 145, 103711. McGrew, S. (2021). Skipping the source and checking the contents: An in-depth look at students’ approaches to web evaluation. Computers in the Schools, 38(2), 75–97. McGrew, S., Breakstone, J., Ortega, T., Smith, M., &amp; Wineburg, S. (2018). Can students evaluate online sources? Learning from assessments of civic online reasoning. Theory &amp; Research in Social Education, 46(2), 165–193. McGrew, S., &amp; Glass, A. C. (2021). Click Restraint: Teaching Students to Analyze Search Results. Proceedings of the 14th International Conference on Computer-Supported Collaborative Learning-CSCL 2021. McGrew, S., Ortega, T., Breakstone, J., &amp; Wineburg, S. (2017). The challenge that’s bigger than fake news: Civic reasoning in a social media environment. American Educator, 41(3), 4. Mihailidis, P., &amp; Thevenin, B. (2013). Media literacy as a core competency for engaged citizenship in participatory democracy. American Behavioral Scientist, 57(11), 1611–1622. Miller, W. R., &amp; Brown, J. M. (1991). Self-regulation as a conceptual basis for the prevention and treatment of addictive behaviours. Self-Control and the Addictive Behaviours, 3–79. Moon, B., Hoffman, R. R., Novak, J., &amp; Canas, A. (Eds.). (2011). Applied Concept Mapping: Capturing, Analyzing, and Organizing Knowledge (Zeroth). CRC Press. https://doi.org/10.1201/b10716 National Research Council. (2000). How people learn: Brain, mind, experience, and school: Expanded edition. The National Academies Press. https://doi.org/10.17226/9853 Next Generation Science Standards. (2021). Task annotation project in science | sense-making. https://www.nextgenscience.org/sites/default/files/TAPS%20Sense-making.pdf. Novak, J. D. (2002). Meaningful learning: The essential factor for conceptual change in limited or inappropriate propositional hierarchies leading to empowerment of learners. Science Education, 86(4), 548–571. Novak, J. D. (2010). Learning, creating, and using knowledge: Concept maps as facilitative tools in schools and corporations (2nd ed). Routledge. Novak, J. D., &amp; Gowin, D. B. (1984). Learning how to learn. Cambridge University Press. https://doi.org/10.1017/CBO9781139173469 Pea, R., &amp; Jacks, D. (2014). The learning analytics workgroup: A report on building the field of learning analytics for personalized learning at scale. https://ed.stanford.edu/sites/default/files/law_report_complete_09-02-2014.pdf; Stanford, CA: Stanford University. Piaget, J. (1936). Origins of intelligence in children. Roy, N., Torre, M. V., Gadiraju, U., Maxwell, D., &amp; Hauff, C. (2021). Note the highlight: Incorporating active reading tools in a search as learning environment. Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, 229–238. Rumelhart, D. E., &amp; Norman, D. A. (1981). Accretion, tuning and restructuring: Three modes of learning. In J. W. Cotton &amp; K. R. (Eds.), Semantic factors in cognition (pp. 37–90). Rumelhart, D. E., &amp; Ortony, A. (1977). The representation of knowledge in memory. In R. C. Anderson, S. R. J., &amp; M. W. E. (Eds.), Schooling and the acquisition of knowledge (pp. 99–135). Hillsdale, NJ: Erlbaum. Ryan, R. M. (1982). Control and information in the intrapersonal sphere: An extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43(3), 450. Ryan, R. M., &amp; Deci, E. L. (2000a). Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary Educational Psychology, 25(1), 54–67. Ryan, R. M., &amp; Deci, E. L. (2000b). Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist, 55(1), 68. Ryan, R. M., &amp; Deci, E. L. (2017). Self-determination theory: Basic psychological needs in motivation, development, and wellness. Guilford Publications. Saks, K., &amp; Leijen, Ä. (2014). Distinguishing Self-directed and Self-regulated Learning and Measuring them in the E-learning Context. Procedia - Social and Behavioral Sciences, 112, 190–198. https://doi.org/10.1016/j.sbspro.2014.01.1155 Sawyer, R. K. (2005). The Cambridge handbook of the learning sciences. Cambridge University Press. Schraw, G., &amp; Dennison, R. S. (1994). Assessing Metacognitive Awareness. Contemporary Educational Psychology, 19(4), 460–475. https://doi.org/10.1006/ceps.1994.1033 Tahamtan, I. (2019). The effect of motivation on web search behaviors of health consumers. Proceedings of the 2019 Conference on Human Information Interaction and Retrieval, 401–404. Terlecki, M. (2020). Revising the Metacognitive Awareness Inventory (MAI) to be More User-Friendly. In Improve with Metacognition. https://www.improvewithmetacognition.com/revising-the-metacognitive-awareness-inventory/ Terlecki, M., &amp; McMahon, A. (2018). A Call for Metacognitive Intervention: Improvements Due to Curricular Programming in Leadership. Journal of Leadership Education, 17(4), 130–145. https://doi.org/10.12806/V17/I4/R8 Vakkari, P. (2016). Searching as learning: A systematization based on literature. Journal of Information Science, 42(1), 7–18. https://doi.org/10.1177/0165551515615833 Wilson, M. J., &amp; Wilson, M. L. (2013). A comparison of techniques for measuring sensemaking and learning within participant-generated summaries. Journal of the American Society for Information Science and Technology, 64(2), 291–306. Wineburg, S., &amp; McGrew, S. (2016). Why students can’t google their way to the truth. Education Week, 36(11), 22–28. Wineburg, S., &amp; McGrew, S. (2017). Lateral reading: Reading less and learning more when evaluating digital information. Wittrock, M. C. (1989). Generative processes of comprehension. Educational Psychologist, 24(4), 345–376. Zhang, P., &amp; Soergel, D. (2014). Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking. Journal of the Association for Information Science and Technology, 65(9), 1733–1756. https://doi.org/10.1002/asi.23125 Zimmerman, B. J. (1989). A social cognitive view of self-regulated academic learning. Journal of Educational Psychology, 81(3), 329. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ https://youtu.be/RkBUZ4At8Qg↩︎ https://www.nngroup.com/articles/f-shaped-pattern-reading-web-content↩︎ and continue to link↩︎ be it a project report, poster, presentation, video, software, research paper, website, etc.↩︎ https://cor.stanford.edu↩︎ “Digital literacy describes a holistic approach to cultivating skills that allow people to participate meaningfully in online communities, interpret the changing digital landscape, understand the relationships between systemic -isms and information, and unlock the power of digital tools for good. This includes media literacy. Terms like critical media literacy, media literacy, news literacy, and more are not necessarily interchangeable.” – (Collins, 2021)↩︎ "],["ch_bg_search.html", "3 Background: Information Searching 3.1 Terminology 3.2 Three-stage Interactions with Online Search Systems 3.3 Effects of Expertise and Working Memory on Search Behaviour 3.4 Assessing Learning during Search 3.5 Limitations of Current Search Systems in Fostering Learning 3.6 Summary", " 3 Background: Information Searching This second chapter on background literature discusses relevant concepts from the disciplines of Information Sciences, and more specifically Interaction Information Retrieval. First, we introduce some terminology around information behaviour, information need, and information relevance. Then we discuss relevant findings various empirical studies, from the lens of three-stage interactions in the information search process. Then we discuss some overall generic characteristics of information search behaviour, and how they are linked to expertise and working memory. Next we discuss how learning has been assessed in recent search-as-learning studies. We also discuss some limitations of current search systems to foster learning, including the lack of sufficient number of longitudinal studies. In the last section, we state what implications these findings have for shaping the proposed study in this dissertation proposal. 3.1 Terminology Information retrieval (IR) is the process of obtaining information objects, that are relevant to an information need, from a collection of those objects (Wikipedia). Information objects are entities that can potentially convey information. They can take many forms, such as documents, webpages, facts, music, spoken words, images, videos, artefacts, and other forms of human expression. Areas where information retrieval techniques are employed include search engines, such as web search, social search, and desktop search; media search, as in image, music, video; digital libraries and recommender systems, as well as domain specific applications like geographical information systems, e-Commerce websites, legal information search, and others. Multiple perspectives exist around how users interact with information, and IR systems. In the Search Engine application view, the interactions are restricted to the search engine interface. In the Human-computer interaction (HCI) view, interactions are between a person and a system; but the system can go beyond supporting only retrieval, to supporting more complex tasks. In the cognitive view of IR, which is the broadest, the interactions for obtaining information can be between a person and a system, as well as between people, for retrieval of information. Nested model of information behaviour by (T. D. Wilson, 1999). People’s behaviour around information can be modelled as a nested Venn diagram as proposed by (T. D. Wilson, 1999) (Figure 1.1). Information behaviour is the more general field of investigation. Information-seeking behaviour can be seen as a sub-set of the field, particularly concerned with the variety of methods people employ to discover, and gain access to information objects. Information search behaviour is yet a sub-set of information-seeking, concerned with the interactions between the user and computer-based information systems. In this dissertation, we focus on information search rather than the other two higher hierarchical concepts. This is because online IR systems, such as search engines or digital libraries, have become the primary source for people to obtain information in modern times, and web search is becoming ever more pervasive and ubiquitious in our day to day lives. The field of interactive information retrieval (IIR) posits that IR systems should operate in the way that good libraries do. Good libraries provide both the information a visitor needs, as well as a partner in the learning process — the information professional — to navigate that information, make sense of it, preserve it, and turn it into knowledge. As early as in 1980, Bertram Brookes stated that searchers acquire new knowledge in the information seeking process (Brookes, 1980). Fifteen years later, Gary Marchionini described information seeking, as “a process, in which humans purposefully engage in order to change their state of knowledge” (Marchionini, 1995). So we have known for quite a while that search is driven by the higher-level human need to gain knowledge. Information Retrieval is thus a means to an end, and not the end in itself. Thus, the ideal IR system should not only help users to locate information, but also help them to bridge the gap between information and knowledge. This brings us to the concept of information need. Information Need is the desire to locate and obtain information to satisfy a conscious or unconscious human need. Most search systems of today assume that the search query is an accurate representation of a user’s information need. However, (Belkin et al., 1982) observed that in many cases, users of search systems are unable to precisely formulate what they need. They miss some vital knowledge to formulate their queries. As humans, we have difficulty in asking questions about what we do not know. Belkin called this phenomenon as Anomalous State of Knowledge, or ASK. Later, (X. Huang &amp; Soergel, 2013) identified an exhaustive set of criteria that should be considered in order to ideally represent a user’s information need. These criteria for information need are highly dependent on the user context: user attributes, tasks or goals, as well as the situation the user is embedded in. This brings us to another closely related concept: information relevance. Relevance is a fundamental concept of Information Science and Information Retrieval, and perhaps the most celebrated work in this area has been done by Tefko Saracevic (Saracevic, 1975, 2007a, 2007b, 2016). Webster dictionary define relevance as “a relation to the matter at hand”. In most circumstances, relevance is a “y’know” notion. People apply it effortlessly, without anybody having to define for them what “relevance” is. This creates one of the most fascinating challenges in the information field: humans understand relevance intuitively, while it is an open research problem to represent relevance effectively for use by algorithmic systems. The situation becomes more interesting because relevance always depends on context, and the context is ever dynamic, as the matter at hand changes. 3.2 Three-stage Interactions with Online Search Systems Models of information search process, with our coloured annotations identifying the three stages. As we saw in the previous section, information search behaviour is the (study of) interactions between a user, and digital Information Retrieval (IR) systems. The field of Information Science/Studies has developed multiple models explaining how information search works (T. D. Wilson, 1999). A few of them are presented in Figure 1.2. Across many of these models, we observe that most major Information Retrieval (IR) systems have three fundamental ways of letting users interact with the system, and the underlying information: (1) an interface for entering search queries; (2) an interface for viewing and evaluating a list of retrieved information-objects, or search results; (3) an interface for viewing and evaluating individual information-objects. For instance, (Marchionini, 1995)’s ISP model hints at these three interfaces in the fourth, sixth and seventh stages, namely “formulate query”, “examine results”, and “extract info”. (Spink, 1997)’s model of the IR interaction process consists of sequential steps or cycles, and each cycle comprises one or more interactive feedback occurrences of user input (query), IR system output (list), and user interpretation and judgement (of individual information-objects). Consequently, findings from the large body of empirical research in interactive IR (especially those with web based search systems) can be grouped around these thee stages of interactions with search systems: Stage 1: search query formulation / reformulation Stage 2: search results evaluation (or source selection) Stage 3: content page evaluation (or, interacting with sources) The discussions in the following subsections are based around these three stages of interactions. The empirical studies discussed below generally follow some common principles of user studies in Interactive IR (IIR) (Borlund, 2013; Kelly, 2009): participants are presented with a search task or search topic, and then they are asked to search the internet (or a simulation of the open web) for information. During the search, the various interactions (queries, clicks, webpages opened etc.) are recorded, and these are analysed and correlated with other sources of data to answer research questions. 3.2.1 Stage 1: Query (Re)formulation How do users behave when submitting search queries (to an IR system)? Query formulation is the process of composing a search query that describes the information need of a searcher. Query reformulation refers to the act of either modifying a previous query, or creating a new query. Query reformulation typically occurs due to a searcher’s improved understanding of how to better translate their information need into a search query. The relationship between two successively issued queries have been classified in a number of ways. These classifications are called Query Reformulation Types, or QRTs. Amongst many other, Boldi et al. (2009) used cognitive aspects of the searchers issuing the query to propose a taxonomy of QRTs, while C. Liu et al. (2010) proposed a similar taxonomy focusing more on the linguistic properties of the two successive queries. These are compared and contrasted in Table \\[tab_res_Q\\_QRT_txnmy\\]. Investigating user-interactions with queries: (a) Visualizing the distribution of retrieved search results prior to running a query, for helping searchers understand their queries’ effectiveness (Qvarfordt et al., 2013). The visualization is a stacked column chart with ten columns. Each column represents ten search results: first column represents results ranked 1-10, second column represents results 11-20, etc. Individual columns have three divisions, indicating the counts of results that: are already seen by the searcher (dark blue, top), will be re-retrieved, but have not been seen by the searcher (medium blue, middle), and will be newly-retrieved (bright teal blue, bottom). The system evaluates the searcher’s query continuously as it is being typed, and updates the visualization in real-time. (b) Interfaces for examining interactions with query auto-completion (QAC), by (i) Smith et al. (2016), and (ii) Hofmann et al. (2014) (overlaid with heatmaps of eye fixations for all participants). This figure is best viewed in colour. Task-type, task-topic, task-goal, and domain-expertise were found to influence query reformulation patterns of searchers (Eickhoff et al., 2015; Jiang et al., 2014; Mao et al., 2018). At first glance, a significant portion of the query reformulation terms (\\(\\sim86\\%\\)) seemed to be coming from the task-description itself (Jiang et al., 2014; Mao et al., 2018). This was characterized by significantly more fixations on the task-description, rather than other SERP elements. Jiang et al. (2014) and Mao et al. (2018) investigated this phenomenon further. Jiang et al. (2014) controlled for the task-type and task-goal, using the faceted-framework by Li &amp; Belkin (2008). Mao et al. (2018) controlled for the task-topic and the domain-expertise of the searchers. If search tasks had factual goals, searchers relied heavily on the task-description for reformulating their queries (Jiang et al., 2014). For interpretive tasks (intellectual tasks with specific goals), users spent more time reading search result surrogates, before reformulating their queries. This was observed by increased eye-fixations (indicative of visual attention) and dwell time on search result snippets (surrogates). For exploratory tasks, searchers fixated the longest on query-autocompletion (QAC) suggestions, indicating that they were possibly looking for help and suggestion based on their specific query, as the search-task had non-specific (amorphous) goals. Searchers also relied on the task-description for reformulating queries, when the search-task was outside their domain of expertise (Mao et al., 2018). For in-domain tasks, they used query terms from their own knowledge, that were not fixated on in visited SERPs and content pages. Eickhoff et al. (2015) reported that a significant share of new query terms came from visited SERPs and content pages, and query reformulation (specialization) often did not literally re-use previously encountered terms, but highly related ones8 instead. These observations can possibly be explained by Mao et al. (2018)’s findings: when exploring a new domain, the searcher may accumulate vocabulary and learn how to query during the search; when performing in-domain search-tasks, the searcher may have enough prior knowledge to come up with effective query terms. It was also seen that searchers from medicine domain used more unread query terms for their in-domain search-tasks, compared to politics and environment domains (Mao et al., 2018). This suggested that domain knowledge and expertise is more important for formulating good search queries in highly technical disciplines (e.g., medicine), compared to less technical domains (e.g., politics). Query Auto Completion (QAC) is a technological feature that suggests possible queries to web search users from the moment they start typing a query. It is nearly ubiquitous in modern search systems, and is thought to reduce physical and cognitive effort when formulating a query. QAC suggestions are usually displayed as a list (Figure 1.3(b) and (c)), and users interact in a variety of ways with the list. Hofmann et al. (2014) observed a strong position bias among searchers who examined the QAC list: the top suggestions received the highest visual attention, even when the ordering of the suggestions were randomized. Average fixation time decreased consistently on suggested items from top to bottom. Even when the ranking of suggestions were randomized, time taken to formulate queries did not significantly differ. Search topics were found to have a large effect on QAC usage (Jiang et al., 2014; Smith et al., 2016). Search was easiest for the topics with the highest QAC usage. Total eye-gaze duration was longest when visual attention was shared between the QAC suggestions and the actual search query input box. Some additional time was probably due to decision making on whether to use a QAC suggestion. Typing was faster when a QAC was not used. However, the IR system’s retrieval performance (measured using NDCG@3), was greater when QAC was used. So Smith et al. (2016) speculated that the value of using QAC suggestions was realized later in the search session by users, when they saw a reduction in the number of additional queries needed, or an increase in the value of the information found. Several user behavioural profiles were identified by exploring associations between visual attention from eye-tracking, search interactions from mouse and keyboard activity, and the use of QAC suggestions (Hofmann et al., 2014; Smith et al., 2016). These profiles are described in Table \\[tab_res_Q\\_QAC_profiles\\]. An interesting, yet common-sense observation was that participants’ touch-typing ability greatly influenced their interactions with QAC suggestions. The native language of searchers was found to influence their overall querying and searching behaviour. Ling et al. (2018) explored this space using four variations of a multi-lingual search interface. They observed that participants strongly preferred to issue queries in their first or native language. A second or non-native language was the next preferred choice. Mixing of first and second-languages occurred very rarely. In 80% of the total 300 tasks (25 users \\(\\times\\) 4 interfaces \\(\\times\\) 3 task-types), participants used a single language for querying. In the rest 20% of the tasks, participants switched languages for querying, with a transition from first language to second language being the most common. 3.2.2 Stage 2: Search Results Evaluation How do users behave when examining a list of information-objects (returned by an IR system)? After a user submits a query to an IR system, the next action they generally perform is examining and evaluating the list of search results returned by the IR system. In this section, we discuss empirical studies which investigated information-searching behaviour around a list of information-objects, or a representation of information-objects (also called surrogates). We identified some common themes in the research questions investigated. The discussion below is grouped along these themes, as relationships between search behaviour and: (i) ranking of search results; (ii) information shown in search results; (iii) individual user characteristics; and (iv) relevance judgement and feedback. Example interfaces for studying user-interactions with a search-engine results page (SERP): (a) a simplified SERP without query input facility, to judge relevance of search results (on a 4-level scale) for pre-determined search queries (in this case ‘why do airplanes have differently shaped wings?’), from Scharinger et al. (2016); (b) eye-tracking heatmap on an organic SERP from Buscher et al. (2010; Dumais et al., 2010), showing the F-shaped pattern of visual attention; (c) a multilingual SERP from Ling et al. (2018). This figure is best viewed in colour.   Relationships between ranking of search results, and search behaviour: Most search engines display results in a rank ordered list, with the highest algorithmically relevant results placed at the top, and others results ordered below. Granka et al. (2004; Lorigo et al., 2008) studied eye-movement behaviour of searchers examining SERPs, and reported observations from three user studies. They saw that in 96% of the queries, participants looked at only the first result page, containing the top 10 results. No participant looked beyond the third result page for a given query. Participants looked primarily at the first few results, with nearly equal attention (dwell time) given to the first and the second results. However, despite equal attention, the first result was clicked 42% of the time, while the second was clicked only 8% of the time. If none of the top three results appeared to be relevant, then users chose not to explore further results, but issued a reformulated query instead. When the ranking of the search results were reversed (i.e. placing less relevant results in the higher ranked positions), participants spent considerably more time scrutinizing and comparing results (more fixations and regressions) before making a decision to click or reformulate. Some effects of gender were found to influence SERP examination (Lorigo et al., 2008). Females clicked on the second result twice as often, and made more regressions or repeat viewings of already visited abstracts, compared to males. Males were more likely to click on lower ranked results, from entries 7 through 10, and also look beyond the first 10 results significantly more often than women. Males were also more linear in their scanning patterns, with less regressions. Pupil dilation did not differ significantly between gender groups. Effects of task-type and task-goals also influenced SERP examination behaviour. Guan &amp; Cutrell (2007) used Broder (2002)’s taxonomy of navigational vs. informational searches. The authors reported that when users could not find the target results for navigational searches, they either selected the first result, or switched to a new query. However, for informational searches, users rarely issued a new query and were more likely to try out the top-ranked results, even when those results had lower relevance to the task. This illustrated possible strong confidence of searchers in the search engine’s relevance ranking, even though searchers clearly saw target results at lower positions. Thus, people were more likely to deprecate their own sense of objective relevance and obeyed the ranking determined by the search engine. Jiang et al. (2014) used Li &amp; Belkin (2008)’s framework of search-tasks, and saw that in tasks having specific goals, searchers fixated more on lower ranked results after some time. On the other hand, for tasks having amorphous goals, there was a wider breadth in viewing the SERP, and less effort spent in viewing the content pages. Fixations tended to decrease as search session progressed, indicating decreased interest and increasing mental effort, which could demonstrate satisficing behaviour (Simon, 1956). A comprehensive overview of various behavioural traits associated with task-types and task-goals can be found in (Jiang et al., 2014 Table 8).   Relationships between information shown in search results, and search behaviour: The amount and quality of different kinds of information shown on SERPs also affected user’s information searching behaviour. Cutrell &amp; Guan (2007) saw that as the length of the surrogate information (result snippets) was increased, user’s search performance improved for informational tasks, but degraded for navigational tasks (Broder, 2002). Analyzing eye-tracking data, they posited that the difference in performance was due to users paying more attention to the snippet, and less attention to the URL located at the bottom of the search result. This led to performance deterioration in navigational searches. Buscher et al. (2010) studied the effects of the quality of advertisements placed in the SERPs (Figure 1.4(b)). Similar to findings discussed above, a strong position bias of visual attention was found towards the top few organic result entries — the well known F-shaped pattern of visual attention — which was stronger for informational than for navigational tasks. However, a strong bias against sponsored links was observed in general. Even for informational tasks, where participants generally had a harder time finding a solution, the ads did not receive any additional attention from the participants. Lorigo et al. (2008) compared the visual attention patterns of searchers using two different search engines: Google, and Yahoo!. Behavioural trends followed similar patterns for both search engines, even though Google was rated as the primary search engine of all but one of the participants. They found slight variations in some eye-tracking measures (reading time of surrogates, time to click results, and query reformulation time), and some self-reported measures (perceived ease of use, perceived satisfaction, and success rate). However, none of these differences were statistically significant. The novel query-preview interface by Qvarfordt et al. (2013) was discussed in Section 1.2.1 and in Figure 1.3(a). The authors also reported several observations about user behaviour on SERPs. They saw that the presence of the preview visualization enabled participants to look deeper into the results lists. Participants tried to use the preview as a navigation tool, although it was not designed as such. The tool increased the rates at which participants examined documents at middle ranks in query results, and thus helped discover more useful documents in those middle ranks than without the preview widget. The preview tool also helped to increase the diversity of documents found in a search session, which could in turn lead to better performance in terms of recall and precision. Thus, the tool helped searchers overcome the strong position bias towards top-ranked results, as observed by other studies discussed previously. Effects of differences in user characteristics on interactions with SERPs: (a) exhaustive or depth-first user (User 1), vs. economic or breadth-first user (User 2), examining mostly irrelevant results in Task A, and mostly relevant results in Task B (both users followed the second link in Task B); vertical axis denotes vertical location on SERP, and horizontal axis denotes temporal ordering of result examination; from Aula et al. (2005); (similar patterns were identified by Bilal &amp; Gwizdka (2016), in the SERP examination behaviour of children) (b) children vs. adults examining SERPs from a German search engine for children (left), and Google (right); differently from adults, children exhaustively explored all search results, paid more attention to thumbnails and embedded media, and read less text-only snippets; from Gossen et al. (2014). Similar observations as with children were reported for searchers with dyslexia (Palani et al., 2020). This figure is best viewed in colour.   Relationships between individual user characteristics, and search behaviour: Individual traits of searchers also influence their pattern of interactions with a SERP, and these patterns can be revealed by analyzing eye-tracking data. For instance, searchers have been classified as economic vs. exhaustive, based on their style of evaluating SERPs (Aula et al., 2005). Economic searchers were found to scan less than half (three) of the displayed results above the fold, before making their first action (query re-formulation, or following a link). Exhaustive searchers evaluated more than half of the visible results above the fold, or even scrolled the results page to view all of the results, before performing the first action. Thus, economic searchers demonstrated depth-first search strategy, while exhaustive users favoured the breadth-first approach (Figure 1.5(a)). Dumais et al. (2010) demonstrated the use of unsupervised clustering to re-identify the economic-exhaustive user groups, based on differences in total fixation impact9, scanpaths, task outcomes, and questionnaire data. The economic cluster was further broken down by users who looked primarily at results (economic-results cluster), and users who viewed both results and ads (economic-ads cluster). All three groups spent the highest amount of time on the first three results, with the exhaustive group being substantially slower than the other two groups. The exhaustive and economic-results groups spent the second-highest amount of time on results four through six, while the economic-eds group spent this time on the main advertisements. This group spent more than twice as much time on the main ads as the economic-results group, and even more time on main ads than the exhaustive group. This observation is incongruent to Buscher et al. (2010)‘s findings, as they observed a generally strong bias against viewing sponsored links. Abualsaud &amp; Smucker (2019) conducted further analysis using these user types, and, in general, reconfirmed the previous findings. They found that the results above the fold, especially, the first three search results are special, more so for economic users. On submitting a ’weak’ query, if economic users did not find a correct result within the first three results, they abandoned examination, and reformulated their query. Age of searchers also influence SERP evaluation behaviour. Gossen et al. (2014) demonstrated differences in SERP evaluation for children and adults (Figure 1.5(b)). When answers were not found within the top search results, the adults reformulated the query starting a new search, while young users exhaustively explored all the ten results, and used the navigation buttons between results pages to continue further examination. Children also paid more attention to thumbnails and embedded media, and focused less on textual snippets. Children saw the query suggestions at the bottom of the Google SERP (because they navigated to the bottom), while the adults did not. Bilal &amp; Gwizdka (2016; Gwizdka &amp; Bilal, 2017) investigated this phenomenon further, and observed that even within children, age plays a role in SERP evaluation behaviour. Younger children (grade six, age 11) clicked more often on results in lower-ranked positions than older children (grade eight, age 13). Older children’s clicking behaviour was based more often on reading result snippets, and not just on the ranked position of a result in a SERP. Whereas, younger children made less deliberate choices in choosing which result to click, and were more exhaustive in the exploration of results. Thus, using Aula et al. (2005)’s classification and Dumais et al. (2010)’s observations, it can be posited that (younger) children start out as exhaustive searchers. With increase in age and maturity, older children and adults evolve into economic searchers. Interestingly, very similar behaviour patterns as with children (scrolling further down on SERPs, exhaustive exploration, etc.) were also observed recently for searchers with dyslexia (Palani et al., 2020). Searcher’s native language also influenced SERP interaction behaviour (Ling et al., 2018) (Figure 1.4(c)). We discussed in Section 1.2.1 that users strongly preferred issuing queries in a single language, especially their native language. However, while examining SERPs, they marked search results in both their first language and second language to be relevant, to an equal degree. This confirms the usefulness of search result pages that integrate results from multiple languages. However, a clear separation in the language of the search results was strongly preferred, and an ‘interleaved’ presentation (e.g. odd numbered results in one language and even numbered results in another language) was least preferred. Google search engine result page (SERP) for the queries: (a) “coronavirus” (b) “toyota” (c) “evaporation”, and (d) “life of pie”. All screenshots are from ‘above-the-fold’, viewed on a \\(2560 \\times 1440\\) monitor. These examples highlight that modern SERPs have come a long way from a list of “ten blue links”. SERPs are becoming consumable information-objects in their own right, and thus require different kinds of cognitive processing and interactions, than from the early days of the internet. Inspired and adapted from (Wang et al., 2018). Accessed on May 5, 2020. This figure is best viewed in colour.   Relationships between relevance judgement, and search behaviour: (Balatsoukas &amp; Ruthven, 2010, 2012; Balatsoukas &amp; Ruthven) proposed a list of ‘relevance criteria’ for understanding how searchers evaluate search results, or perform relevance judgement. These criteria were developed based on literature reviews and their empirical findings from eye-tracking studies. The final list contains 15 relevance criteria (e.g., topicality, quality, recency, scope, availability, etc.) and can be found in (Balatsoukas &amp; Ruthven, 2012 Appendix B). Search engines are increasingly adding different modalities of information on the SERP, besides the “ten blue links”. These include images, videos, encyclopaedic information, and maps (Figure 1.6). Z. Liu et al. (2015) studied the influence of these different forms of SERP information – called ‘verticals’ – on searcher’s relevance judgements. A general observation was that if verticals were present in a SERP, they created strong attraction biases. The attraction effect was influenced by the type of verticals, while the vertical quality (relevant or not) did not have a major impact. For instance, ‘images’ and ‘software download’ verticals had higher visual attention, while news verticals had equal attention as the “ten blue links” search results. 3.2.3 Stage 3: Content Page Evaluation How do users behave when examining a single information-object (e.g., a a non-search-engine webpage, aka content page) obtained from an IR system? In online information searching, searchers repeatedly interact with individual webpages, a.k.a. ‘content pages’ in IR terminology. These webpages can be visited by following links from a search engine, following links between different webpages, or directly typing the URL in the browser. The first group of papers we discuss investigated users’ visual attention and reading behaviour on webpages. Pan et al. (2004) studied whether eye-tracking scanpaths on webpages varied based on task-type, webpage type (business, news, search, or shopping), viewing order of webpages, and gender of users. The found significant differences for all factors, except for task-type, which seemed to have no effect on scanpaths. They used weak task-types: remembering what was on a webpage vs. no specific task. In a later work on using informational vs. navigational search-tasks, they again saw limited effect of task-type on visual attention (Lorigo et al., 2006). Findings from Josephson &amp; Holmes (2002)’s study suggested that users possibly follow habitually preferred scanpaths on a webpage, which can be influenced by factors like webpage characteristics and memory. However, they used only three webpages, making the findings difficult to generalize. Goldberg et al. (2002) studied eye movements on Web portals during search-tasks, and saw that header bars were typically not viewed before focusing the main part of the page. So they suggested placing navigation bars on the left side of a page. Beymer et al. (2007) focused on a very specific feature on webpages: images that are placed next to text content and how they influence eye movements during a reading task. They found significant influence on fixation location and duration. Those influences were dependent on how the image contents related to the text contents (i.e., whether they showed ads or text-related images). Buscher et al. (2009) presented findings from a large scale study where users performed information-foraging and page-recognition tasks. They observed that in the first few moments, users quickly scanned the top left of the page, presumably looking for clues about the content, provenance, type of information, etc. for that page. The elements that were normally displayed in the upper left third of webpages (e.g., logos, headlines, titles or perhaps an important picture related to the content) seemed to be important for recognizing and categorizing a page. After these initial moments, influence of task-type set in. For page-recognition tasks, the attention remained in the top-left corner of the webpage. However, for information-foraging tasks, fixations moved to the center-left region of the webpage, where the user was possibly trying to find task-specific information. The right third of webpages attracted almost no visual attention during the first one-second of each page view. Afterwards as well, most users seemed to entirely ignore this region, or only occasionally look at it. This suggested that users had low expectations of information-content or general relevance on the right side of most webpages. As many webpages display advertisements on the right side, this was a plausible observation, and are in line with the observed “F-shaped-patterns” 10 on webpages. Buscher et al. (2009) also proposed an eye-tracking measure called fixation impact. This measure first appends a circular Gaussian distribution around each fixation on a webpage element, to create a fuzzy area of interest. This is called the distance impact value. If a webpage element completely covers the fixation circle (Gaussian distribution), it gets a distance impact value of 1. If the element partially covers the fixation circle, its distance impact value is smaller. Multiplying the distance impact value with the fixation duration gives the fixation impact for the given webpage element. Thus, an element that completely covers the fixation circle gets the full fixation duration as fixation impact value. Elements which are partially inside the circle get a value proportional to the Gaussian distribution. The authors posited that the rationale behind creating the fixation impact measure was motivated by observations from human vision research, which indicates that fixation duration correlates with the amount of visual information processed; the longer a fixation, the more information is processed around the fixation centre. Using the fixation impact measure, Buscher et al. (2009) proposed a model for predicting the amount visual attention that individual webpage elements may receive (i.e. visual salience). Another group of studies investigated how users judged relevance of webpages w.r.t. an assigned search-task or information need. (Gwizdka, 2018; Gwizdka &amp; Zhang, 2015a, 2015b) observed that when relevant pages were revisited, the webpages were read more carefully. Pupil dilations were significantly larger on visits and revisits to relevant pages, and just before relevance judgements were made. Certain conditions of visits and revisits also showed significant differences in EEG alpha frequency band power, and EEG-derived attention levels. Relevance of individual webpage elements were also assessed as click-intention: whether users would click on an element they were looking at. Slanzi et al. (2017) used pupillometry and EEG signals to predict whether a mouse click was present for each eye fixation. EEG features included simple statistical features of signals (mean, SD, power, etc.), as well as sophisticated mathematical features (Hjorth features, Fractal Dimensions, Entropy, etc.). A battery of classifier models were tested. However, the results were not promising. Logistic Regression had the highest accuracy (71%), but very low F1 score (0.33), while neural network based classifiers the had highest F1 score (0.4). The authors suspected that the low sampling rate of their instruments (30 Hz eye-tracker and 128 Hz 14-channel EEG) impacted their classifier performances. González-Ibáñez et al. (2019) compared relevance prediction performances in the presence and absence of eye-tracking data, and argued that when eye-tracking data collection is not feasible, mouse left-clicks can be used a good alternative indicator of relevance. The ‘Competition for Attention’ theory states that items in our visual field compete for our attention (Desimone &amp; Duncan, 1995). Djamasbi et al. (2013) studied web search and browsing from the perspective of this theory. Theoretical models suggest that in goal-directed searches, information-salience and/or information-relevance drives search behaviour (i.e. competition for attention does not hold true), whereas exploratory search behaviour is influenced by competition among stimuli that attracts a user’s attention (i.e. competition for attention holds true). However, in practice, information search behaviour often becomes a combination of both types of visual search activities (Groner et al., 1984). Djamasbi et al. (2013) found that, despite the goal directed nature of their search-task (finding the best snack place in Boston to take their friends) competition for attention had some effect at the content page level. Some of the users’ attention was diverted to non-focal areas on content pages. However, there was little effect of competition for attention on how the results were viewed on SERPs. Users exhibited the familiar top-to-bottom pattern of viewing (Section 1.2.2), paying the most attention to the top two entries. 3.3 Effects of Expertise and Working Memory on Search Behaviour image Our focus of discussion in this proposal is information searching and learning. As we saw in Chapter \\[ch_bg_learn\\], learning and expertise are closely connected: expertise is an evolving characteristic of users that reflects learning over time, rather than being a static property (Rieh et al., 2016; Sawyer, 2005). (White, 2016a, Chapter 7) considers three types of expertise, that are relevant in information seeking settings: (i) domain or subject-matter expertise; (ii) search expertise; and (iii) task expertise. Domain or subject-matter expertise describes people’s knowledge in a specialised subject area such as a domain of interest. Search expertise refers to people’s skill level at performing information-seeking activities, both in a Web search setting and in other settings such as specialised domains. Task expertise describes people’s expertise in performing particular search tasks, potentially independent of domain. Although considered distinctly, the boundaries between these expertise types are quite blurred, and therefore difficult to estimate at the time of search, and model it in a way that can be consumed by search systems. Previous work on domain knowledge and expertise have linked11 domain expertise and search behaviour in terms of metrics, behavioural patterns, and criteria (M. J. Cole et al., 2013; Mao et al., 2018; O’Brien et al., 2020; White et al., 2009). A representative summary is presented in Table \\[tab_search_behaviours\\], and is adapted from literature reviews by (Rieh et al., 2016) and (Vakkari, 2016). Briefly, (Wildemuth, 2004) showed that novices converge toward the same search patterns as experts, as they are exposed to a topic and learn more about it. (X. Zhang et al., 2011) found that features such as document retention, query length, and the average rank of results selected could be predictive of domain expertise. (M. J. Cole et al., 2013) showed that eye-gaze patterns could be used to predict an individual’s level of domain expertise using estimates of cognitive effort associated with reading. (White et al., 2009) showed that measures such as diverse website visitation, more narrow topical focus, less diversity (or entropy), more ‘branchiness’ of search sessions, less dwell time, and higher query and session complexity are indicative of expert knoweldge and/or search behaviour. As a stark contrast, (Zlatkin-Troitschanskaia et al., 2021) reviewed literature on higher education students’ information search behaviour. Students can be considered as novices in all three respects: domain/subject-matter, search skills, and task. The authors report that across literature, higher education students’ information search behaviour tends to follow some general general patterns: (i) foraging: no explicit (task-specific) research plan and little understanding of the differences (pros/cons) between various IR systems; (ii) Google dependence: no intention to use any search tool other than Google, causing students to struggle to understand library information structures and engage with scholarly literature effectively; (iii) rudimentary search heuristic: reliance on one and the same simple search strategy, regardless of search context; (iv) habitual topic changing: students change the search topic after rather superficial skimming, and before evaluating all search results; and (v) overuse of natural language: students type questions into the search box that are phrased as if posing them to a person. Highly ranked online sources accessed via a well-known search engine were perceived as trustworthy. Effects of memory span and working memory capacity have also been found to influence search effort and search behaviour (Arguello &amp; Choi, 2019; Bhattacharya &amp; Gwizdka, 2019a; L. Cole et al., 2020; Gwizdka, 2013, 2017). Working memory (WM) is considered a core executive function is defined as someone’s ability to hold information in short-term memory when it is no longer perceptually present (Diamond, 2013; G. A. Miller, 1956). (Bailey &amp; Kelly, 2011) showed that the amount of effort was a good indicator of user success on search tasks. (Smith &amp; Kantor, 2008) studied searcher adaptation to poorly performing systems and found that searchers changed their search behaviors between difficult and easy topics in a way that could indicate that users are satisficing. Differences in search effort between different types systems (higher effort invested in searching library database vs. web) were found by (Rieh et al., 2012). A couple of studies showed that mental effort involved in judging document relevance is lower for irrelevant and higher for relevant documents (Gwizdka, 2014; Villa &amp; Halvey, 2013). (Gwizdka, 2017) found that that higher WM searchers perform more actions and that most significant differences are in time spent on reading results pages. Behaviour of high and low WM searchers were also found to change differently in the course of a search task performance. 3.4 Assessing Learning during Search In order for IR systems to foster user-learning at scale, while respecting individual differences of searchers, there is a need for measures to represent, assess, and evaluate the learning process, possibly in an automated fashion. Consequently, a variety of assessment tools have been used in prior studies. These include self reports, close ended factual questions (multiple choice), open ended questions (short answers, summaries, essays, free recall, sentence generation), and visual mapping techniques using concept maps or mind maps. Each approach has its own associated advantages and limitations. Self-report asks searchers to rate their self-perceived pre-search and post-search knowledge levels (Ghosh et al., 2018; O’Brien et al., 2020). This approach is the easiest to construct, and can be generalised over any search topic. However, self-perceptions may not objectively represent true learning. Closed ended questions test searchers’ knowledge using factual multiple choice questions (MCQs). The answer options can be a mixture of fact-based responses (TRUE, FALSE, or I DON’T KNOW), (Gadiraju et al., 2018; Xu et al., 2020; Yu et al., 2018) or recall-based responses (I remember / don’t remember seeing this information) (Kruikemeier et al., 2018; Roy et al., 2020). Constructing topic-dependant MCQs may take time and effort, since they are topic dependant. Recent work on automatic question generation may be leveraged to overcome this limitation (Syed et al., 2020). Evaluating close ended questions is the easiest, and generally automated in various online learning platforms. Multiple choice questions, however, suffer from a limitation: they allow respondents to answer correctly by guesswork. Open ended questions assess learning by letting searchers write natural language summaries or short answers (Bhattacharya &amp; Gwizdka, 2018; O’Brien et al., 2020; Roy et al., 2021). Depending on experimental design, prompts for writing such responses can be generic (least effort) (Bhattacharya &amp; Gwizdka, 2018, 2019b), or topic-specific (some effort) (Syed et al., 2020). While this approach can provide the richest information about the searcher’s knowledge state, evaluating such responses is the most challenging, and requires extensive human intervention (Kanniainen et al., 2021; Leu et al., 2015; M. J. Wilson &amp; Wilson, 2013) (as discussed in Section \\[sec_bg_learn_artefact\\]). Visual mapping techniques such as mind maps and concept maps have also been used to assess learning during search (Egusa et al., 2010, 2014a, 2014b, 2017; Halttunen &amp; Jarvelin, 2005). Concept maps have been discussed at length in Section \\[sec_bg_concept_maps\\]. Learning has also been measured in other ways, such as user’s familiarity with concepts and relationships between concepts (Pirolli et al., 1996), gains in user’s understanding of the topic structure, e.g., via conceptual changes described in pre-defined taxonomies (P. Zhang &amp; Soergel, 2016), and user’s ability to formulate more effective queries (Chen et al., 2020; Pirolli et al., 1996). 3.5 Limitations of Current Search Systems in Fostering Learning 3.5.1 Longitudinal studies Learning is a longitudinal process, occurring gradually over time (Sections \\[sec_bg_learn_sensemaking\\] and \\[sec_bg_learn_principles\\]). Therefore, information researchers have studied participant’s search behaviour in prior, albeit few, longitudinal studies. Examples include studies by (Kelly, 2006a, 2006b; Kuhlthau, 2004; Vakkari, 2001b; White et al., 2009; Wildemuth, 2004). (Wildemuth, 2004) examined the search behaviour of medical students in microbiology. In this experiment, students were observed at three points of time (at the beginning of the course, at the end of the course, and six months after the course), under the assumption that domain expertise changes during a semester. Some search strategies, most notably the gradual narrowing of the results through iterative query modification, were the same throughout the observation period. Other strategies varied over time as individuals gained domains knowledge. Novices were less efficient in selecting concepts to include in search and less accurate in their tactics for modifying searches. (Pennanen &amp; Vakkari, 2003; Vakkari, 2001a, 2000, 2001b) also examined students at multiple points in time, as they were developing their thesis proposal. One important change in behaviour was the use of a more varied and more specific vocabulary as students learned more about their research topic. (Weber et al., 2019) examined a large sample of German students from all academic fields in a two wave study and found that the more advanced they are in their studies, the more students show a more advanced search behaviour (e.g., using more English queries and accessing academic databases more frequently). Advanced search behaviour predicted better university grades. (Weber et al., 2018) also provide mixed evidence on the potential long-term effects of such interventions, as some of their participants reverted to their previous habits two weeks after the study and therefore exhibited only short-term changes in their information-seeking behaviour. Overall, results regarding the promotion of user’ search and evaluation skills are encouraging. But there is a clear need for more longitudinal studies. The general body of search-as-learning literature examines the learner in the short-term, typically over the course of a single lab session (Kelly et al., 2009; Zlatkin-Troitschanskaia et al., 2021). The trend is similar in other Human-Computer Interaction (HCI) research venues. A meta-analysis of 1014 user studies reported in the ACM CHI 2020 conference revealed that more than 85% of the studies observed participants for a day or less. To this day, “longitudinal studies are the exception rather than the norm” (Koeman, 2020). “An over-reliance on short studies risks inaccurate findings, potentially resulting in prematurely embracing or disregarding new concepts” (Koeman, 2020). 3.5.2 Supporting sensemaking and reflection As we saw in Section \\[sec_bg_learn_sensemaking\\], learning is sensemaking. Yet, modern search systems are still quite far from supporting sensemaking and learning, and rather, at best are good locators of information. (Rieh et al., 2016) says that modern search systems should support sensemaking by offering more interactive functions, such as tagging for annotation, or tracking individuals’ search history, so that a learner could return to a particular learning point. In addition, a system could provide new features that allow learners to reflect upon their own learning process and search outcomes, thus facilitating the development of critical thinking skills. It’s easy to be impressed by the scientific and engineering feats that have produced web search engines. They are, unquestionably, one of the most impactful and disruptive information technologies of our time. However, it’s critical to remember their many limitations: they do not help us know what we want to know; they do not help us choose the right words to find it; they do not help us know if what we’ve found is relevant or true; and they do not help us make sense of it. All they do is quickly retrieve what other people on the internet have shared. While this is a great feat, all of the content on the internet is far from everything we know, and quite often a poor substitute for expertise. – (Ko, 2021) (emphasis our own) 3.6 Summary In this second chapter of the background literature review, we discussed (i) how searchers interact with three stages / interfaces of modern information retrieval system: query formulation, search results evaluation, and content page evaluation; (ii) how expertise and working memory influence overall search behaviour; (iii) how learning or knowledge gain during search has been assessed in recent search as learning literature; and (iv) what are the limitations of current search systems to foster learning, including gaps in literature about long term search behaviour and learning outcomes, as well as lack of support for sensemaking. We saw that while we have a plethora of studies investigating search behaviour searchers in the short term, we have merely a handful of studies observing the same participant for more than a day. To the best of the author’s knowledge, most of these studies were conducted over a decade ago. Thus, while we have excellent knowledge of short term nature of influence of searching on learning, we do not know what are the longer term effects. Furthermore, we we have gaps in our knowledge of (i) how practices like articulation and externalization, and user attributes like metacognition, motivation, and self regulation moderate the searching as learning process; (ii) how these moderator variables change over time; and (iii) what these phenomena collectively entail for the design of future learning-centric IR systems. In the next chapter, we take these gaps in knowledge and use them to inform our research questions and hypotheses. References Abualsaud, M., &amp; Smucker, M. D. (2019). Patterns of search result examination: Query to first action. Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 1833–1842. https://doi.org/10.1145/3357384.3358041 Arguello, J., &amp; Choi, B. (2019). The effects of working memory, perceptual speed, and inhibition in aggregated search. ACM Transactions on Information Systems, 37(3). https://doi.org/10.1145/3322128 Aula, A., Majaranta, P., &amp; Räihä, K.-J. (2005). Eye-tracking reveals the personal styles for search result evaluation. In M. F. Costabile &amp; F. Paternò (Eds.), Human-computer interaction - INTERACT 2005 (pp. 1058–1061). Springer Berlin Heidelberg. Bailey, E., &amp; Kelly, D. (2011). Is amount of effort a better predictor of search success than use of specific search tactics? Proceedings of the American Society for Information Science and Technology, 48(1), 1–10. Balatsoukas, P., &amp; Ruthven, I. (2010). The use of relevance criteria during predictive judgment: An eye tracking approach. Proceedings of the American Society for Information Science and Technology, 47(1), 1–10. https://doi.org/10.1002/meet.14504701145 Balatsoukas, P., &amp; Ruthven, I. (2012). An eye-tracking approach to the analysis of relevance judgments on the Web: The case of Google search engine. Journal of the American Society for Information Science and Technology, 63(9), 1728–1746. https://doi.org/10.1002/asi.22707 Belkin, N. J., Oddy, R. N., &amp; Brooks, H. M. (1982). ASK for information retrieval: Part i. Background and theory. Journal of Documentation. Beymer, D., Orton, P. Z., &amp; Russell, D. M. (2007). An eye tracking study of how pictures influence online reading. IFIP Conference on Human-Computer Interaction, 456–460. Bhattacharya, N., &amp; Gwizdka, J. (2018). Relating eye-tracking measures with changes in knowledge on search tasks. Symposium on Eye Tracking Research &amp; Applications (ETRA). Bhattacharya, N., &amp; Gwizdka, J. (2019b). Measuring learning during search: Differences in interactions, eye-gaze, and semantic similarity to expert knowledge. Proceedings of the 2019 Conference on Human Information Interaction and Retrieval, 63–71. Bhattacharya, N., &amp; Gwizdka, J. (2019a). Measuring learning during search: Differences in interactions, eye-gaze, and semantic similarity to expert knowledge. CHIIR’19. Bilal, D., &amp; Gwizdka, J. (2016). Children’s Eye-fixations on Google Search Results. Proceedings of the 79th ASIS&amp;T Annual Meeting, 79, 89:1–89:6. https://doi.org/10.1002/pra2.2016.14505301089 Boldi, P., Bonchi, F., Castillo, C., &amp; Vigna, S. (2009). From\" dango\" to\" japanese cakes\": Query reformulation models and patterns. 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, 1, 183–190. Borlund, P. (2013). Interactive Information Retrieval: An Introduction. Journal of Information Science Theory and Practice, 1(3), 12–32. https://doi.org/10.1633/JISTAP.2013.1.3.2 Broder, A. (2002). A taxonomy of web search. SIGIR Forum, 36(2), 3–10. https://doi.org/10.1145/792550.792552 Brookes, B. C. (1980). The foundations of information science. Part i. Philosophical aspects. Journal of Information Science, 2(3-4), 125–133. Buscher, G., Cutrell, E., &amp; Morris, M. R. (2009). What Do You See When You’re Surfing? Using Eye Tracking to Predict Salient Regions of Web Pages. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 10. Buscher, G., Dumais, S. T., &amp; Cutrell, E. (2010). The good, the bad, and the random: An eye-tracking study of ad quality in web search. Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 42–49. https://doi.org/10.1145/1835449.1835459 Chen, Y., Zhao, Y., &amp; Wang, Z. (2020). Understanding online health information consumers’ search as a learning process. Library Hi Tech. Cole, L., MacFarlane, A., &amp; Makri, S. (2020). More than words: The impact of memory on how undergraduates with dyslexia interact with information. Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, 353–357. https://doi.org/10.1145/3343413.3378005 Cole, M. J., Gwizdka, J., Liu, C., Belkin, N. J., &amp; Zhang, X. (2013). Inferring user knowledge level from eye movement patterns. Information Processing &amp; Management, 49(5), 1075–1091. Cutrell, E., &amp; Guan, Z. (2007). What are you looking for? An eye-tracking study of information usage in web search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 407–416. https://doi.org/10.1145/1240624.1240690 Desimone, R., &amp; Duncan, J. (1995). Neural mechanisms of selective visual attention. Annual Review of Neuroscience, 18(1), 193–222. Diamond, A. (2013). Executive functions. Annual Review of Psychology, 64, 135–168. Djamasbi, S., Hall-Phillips, A., &amp; Yang, R. (Rachel). (2013). Search Results Pages and Competition for Attention Theory: An Exploratory Eye-Tracking Study. In S. Yamamoto (Ed.), Human Interface and the Management of Information. Information and Interaction Design (pp. 576–583). Springer Berlin Heidelberg. http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-39209-2_64 Dumais, S. T., Buscher, G., &amp; Cutrell, E. (2010). Individual differences in gaze patterns for web search. Proceedings of the Third Symposium on Information Interaction in Context, 185–194. https://doi.org/10.1145/1840784.1840812 Egusa, Y., Saito, H., Takaku, M., Terai, H., Miwa, M., &amp; Kando, N. (2010). Using a Concept Map to Evaluate Exploratory Search. Proceedings of the Third Symposium on Information Interaction in Context, 175–184. https://doi.org/10.1145/1840784.1840810 Egusa, Y., Takaku, M., &amp; Saito, H. (2014a). How Concept Maps Change if a User Does Search or Not? Proceedings of the 5th Information Interaction in Context Symposium, 68–75. https://doi.org/10.1145/2637002.2637012 Egusa, Y., Takaku, M., &amp; Saito, H. (2014b). How to evaluate searching as learning. Searching as Learning Workshop (IIiX 2014 Workshop). http://www.diigubc.ca/IIIXSAL/program.html Egusa, Y., Takaku, M., &amp; Saito, H. (2017). Evaluating Complex Interactive Searches Using Concept Maps. SCST@ CHIIR, 15–17. Eickhoff, C., Dungs, S., &amp; Tran, V. (2015). An eye-tracking study of query reformulation. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 13–22. https://doi.org/10.1145/2766462.2767703 Gadiraju, U., Yu, R., Dietze, S., &amp; Holtz, P. (2018). Analyzing knowledge gain of users in informational search sessions on the web. Conference on Human Information Interaction &amp; Retrieval (CHIIR). Ghosh, S., Rath, M., &amp; Shah, C. (2018). Searching as learning: Exploring search behavior and learning outcomes in learning-related tasks. Conference on Human Information Interaction &amp; Retrieval (CHIIR). Goldberg, J. H., Stimson, M. J., Lewenstein, M., Scott, N., &amp; Wichansky, A. M. (2002). Eye tracking in web search tasks: Design implications. Proceedings of the 2002 Symposium on Eye Tracking Research &amp; Applications, 51–58. González-Ibáñez, R., Esparza-Villamán, A., Vargas-Godoy, J. C., &amp; Shah, C. (2019). A comparison of unimodal and multimodal models for implicit detection of relevance in interactive IR. Journal of the Association for Information Science and Technology, 0(0). https://doi.org/10.1002/asi.24202 Gossen, T., Höbel, J., &amp; Nürnberger, A. (2014). A comparative study about children’s and adults’ perception of targeted web search engines. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1821–1824. https://doi.org/10.1145/2556288.2557031 Granka, L. A., Joachims, T., &amp; Gay, G. (2004). Eye-tracking analysis of user behavior in WWW search. Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 478–479. https://doi.org/10.1145/1008992.1009079 Groner, R., Walder, F., &amp; Groner, M. (1984). Looking at faces: Local and global aspects of scanpaths. In Advances in psychology (Vol. 22, pp. 523–533). Elsevier. Guan, Z., &amp; Cutrell, E. (2007). An eye tracking study of the effect of target rank on web search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 417–420. https://doi.org/10.1145/1240624.1240691 Gwizdka, J. (2013). Effects of working memory capacity on users’ search effort. Proceedings of the International Conference on Multimedia, Interaction, Design and Innovation, 11:1–11:8. https://doi.org/10.1145/2500342.2500358 Gwizdka, J. (2014). Characterizing Relevance with Eye-tracking Measures. Proceedings of the 5th Information Interaction in Context Symposium, 58–67. https://doi.org/10.1145/2637002.2637011 Gwizdka, J. (2017). I Can and So I Search More: Effects Of Memory Span On Search Behavior. Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, 341–344. https://doi.org/10.1145/3020165.3022148 Gwizdka, J. (2018). Inferring Web Page Relevance Using Pupillometry and Single Channel EEG. In F. D. Davis, R. Riedl, J. vom Brocke, P.-M. Léger, &amp; A. B. Randolph (Eds.), Information Systems and Neuroscience (pp. 175–183). Springer International Publishing. https://doi.org/10.1007/978-3-319-67431-5_20 Gwizdka, J., &amp; Bilal, D. (2017). Analysis of Children’s Queries and Click Behavior on Ranked Results and Their Thought Processes in Google Search. Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, 377–380. https://doi.org/10.1145/3020165.3022157 Gwizdka, J., &amp; Zhang, Y. (2015a). Towards Inferring Web Page Relevance An Eye-Tracking Study. Proceedings of iConference’2015, 5. https://www.ideals.illinois.edu/handle/2142/73709 Gwizdka, J., &amp; Zhang, Y. (2015b). Differences in Eye-Tracking Measures Between Visits and Revisits to Relevant and Irrelevant Web Pages. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 811–814. https://doi.org/10.1145/2766462.2767795 Halttunen, K., &amp; Jarvelin, K. (2005). Assessing learning outcomes in two information retrieval learning environments. Information Processing &amp; Management, 41(4), 949–972. https://doi.org/10.1016/j.ipm.2004.02.004 Hofmann, K., Mitra, B., Radlinski, F., &amp; Shokouhi, M. (2014). An eye-tracking study of user interactions with query auto completion. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, 549–558. https://doi.org/10.1145/2661829.2661922 Huang, X., &amp; Soergel, D. (2013). Relevance: An improved framework for explicating the notion. Journal of the American Society for Information Science and Technology, 64(1), 18–35. https://doi.org/10.1002/asi.22811 Jiang, J., He, D., &amp; Allan, J. (2014). Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 607–616. https://doi.org/10.1145/2600428.2609633 Josephson, S., &amp; Holmes, M. E. (2002). Visual attention to repeated internet images: Testing the scanpath theory on the world wide web. Proceedings of the 2002 Symposium on Eye Tracking Research &amp; Applications, 43–49. Kanniainen, L., Kiili, C., Tolvanen, A., Aro, M., Anmarkrud, Ø., &amp; Leppänen, P. H. T. (2021). Assessing reading and online research comprehension: Do difficulties in attention and executive function matter? Learning and Individual Differences, 87, 101985. https://doi.org/10.1016/j.lindif.2021.101985 Kelly, D. (2006a). Measuring online information seeking context, Part 1: Background and method. Journal of the American Society for Information Science and Technology, 57(13), 1729–1739. https://doi.org/10.1002/asi.20483 Kelly, D. (2006b). Measuring online information seeking context, Part 2: Findings and discussion. Journal of the American Society for Information Science and Technology, 57(14), 1862–1874. https://doi.org/10.1002/asi.20484 Kelly, D. (2009). Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval, 3(1—2), 1–224. Kelly, D., Dumais, S., &amp; Pedersen, J. O. (2009). Evaluation challenges and directions for information-seeking support systems. IEEE Computer, 42(3). Ko, A. J. (2021). Seeking information. In Foundations of Information. https://faculty.washington.edu/ajko/books/foundations-of-information/#/seeking Koeman, L. (2020). HCI/UX research: What methods do we use? – lisa koeman – blog. https://lisakoeman.nl/blog/hci-ux-research-what-methods-do-we-use/. Kruikemeier, S., Lecheler, S., &amp; Boyer, M. M. (2018). Learning from news on different media platforms: An eye-tracking experiment. Political Communication, 35(1), 75–96. Kuhlthau, C. C. (2004). Seeking meaning: A process approach to library and information services (Vol. 2). Libraries Unlimited Westport, CT. Leu, D. J., Forzani, E., Rhoads, C., Maykel, C., Kennedy, C., &amp; Timbrell, N. (2015). The New Literacies of Online Research and Comprehension: Rethinking the Reading Achievement Gap. Reading Research Quarterly, 50(1), 37–59. https://doi.org/10.1002/rrq.85 Li, Y., &amp; Belkin, N. J. (2008). A faceted approach to conceptualizing tasks in information seeking. Information Processing &amp; Management, 44(6), 1822–1837. Ling, C., Steichen, B., &amp; Choulos, A. G. (2018). A comparative user study of interactive multilingual search interfaces. Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval, 211–220. https://doi.org/10.1145/3176349.3176383 Liu, C., Gwizdka, J., Liu, J., Xu, T., &amp; Belkin, N. J. (2010). Analysis and evaluation of query reformulations in different task types. Proceedings of the American Society for Information Science and Technology, 47(1), 1–9. Liu, Z., Liu, Y., Zhou, K., Zhang, M., &amp; Ma, S. (2015). Influence of vertical result in web search examination. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 193–202. https://doi.org/10.1145/2766462.2767714 Lorigo, L., Haridasan, M., Brynjarsdóttir, H., Xia, L., Joachims, T., Gay, G., Granka, L., Pellacini, F., &amp; Pan, B. (2008). Eye tracking and online search: Lessons learned and challenges ahead. Journal of the American Society for Information Science and Technology, 59(7), 1041–1052. https://doi.org/10.1002/asi.20794 Lorigo, L., Pan, B., Hembrooke, H., Joachims, T., Granka, L., &amp; Gay, G. (2006). The influence of task and gender on search and evaluation behavior using google. Information Processing &amp; Management, 42(4), 1123–1131. Mao, J., Liu, Y., Kando, N., Zhang, M., &amp; Ma, S. (2018). How does domain expertise affect users’ search interaction and outcome in exploratory search? ACM Transactions on Information Systems, 36. Marchionini, G. (1995). Information Seeking in Electronic Environments. Cambridge University Press. Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63(2), 81. O’Brien, H. L., Kampen, A., Cole, A. W., &amp; Brennan, K. (2020). The role of domain knowledge in search as learning. Conference on Human Information Interaction and Retrieval (CHIIR). Palani, S., Fourney, A., Williams, S., Larson, K., Spiridonova, I., &amp; Morris, M. R. (2020). An eye tracking study of web search by people with and without dyslexia. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 729–738. https://doi.org/10.1145/3397271.3401103 Pan, B., Hembrooke, H. A., Gay, G. K., Granka, L. A., Feusner, M. K., &amp; Newman, J. K. (2004). The determinants of web page viewing behavior: An eye-tracking study. Proceedings of the 2004 Symposium on Eye Tracking Research &amp; Applications, 147–154. Pennanen, M., &amp; Vakkari, P. (2003). Students’ conceptual structure, search process, and outcome while preparing a research proposal: A longitudinal case study. Journal of the American Society for Information Science and Technology, 54(8), 759–770. Pirolli, P., Schank, P., Hearst, M., &amp; Diehl, C. (1996). Scatter/gather browsing communicates the topic structure of a very large text collection. Conference on Human Factors in Computing Systems (CHI’96). Qvarfordt, P., Golovchinsky, G., Dunnigan, T., &amp; Agapie, E. (2013). Looking ahead: Query preview in exploratory search. Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 243–252. https://doi.org/10.1145/2484028.2484084 Rieh, S. Y., Collins-Thompson, K., Hansen, P., &amp; Lee, H.-J. (2016). Towards searching as a learning process: A review of current perspectives and future directions. Journal of Information Science, 42(1), 19–34. https://doi.org/10.1177/0165551515615841 Rieh, S. Y., Kim, Y.-M., &amp; Markey, K. (2012). Amount of invested mental effort (AIME) in online searching. Information Processing &amp; Management, 48(6), 1136–1150. Roy, N., Moraes, F., &amp; Hauff, C. (2020). Exploring users’ learning gains within search sessions. Conference on Human Information Interaction and Retrieval (CHIIR). Roy, N., Torre, M. V., Gadiraju, U., Maxwell, D., &amp; Hauff, C. (2021). Note the highlight: Incorporating active reading tools in a search as learning environment. Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, 229–238. Saracevic, T. (1975). Relevance: A review of and a framework for the thinking on the notion in information science. Journal of the American Society for Information Science, 26(6), 321–343. Saracevic, T. (2007a). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: Nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(13), 1915–1933. https://doi.org/10.1002/asi.20682 Saracevic, T. (2007b). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126–2144. Saracevic, T. (2016). The Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Synthesis Lectures on Information Concepts, Retrieval, and Services. Sawyer, R. K. (2005). The Cambridge handbook of the learning sciences. Cambridge University Press. Scharinger, C., Kammerer, Y., &amp; Gerjets, P. (2016). Fixation-Related EEG Frequency Band Power Analysis: A Promising Neuro-Cognitive Methodology to Evaluate the Matching-Quality of Web Search Results? HCI International 2016 Posters’ Extended Abstracts, 245–250. https://doi.org/10.1007/978-3-319-40548-3_41 Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological Review, 63(2), 129. Slanzi, G., Balazs, J. A., &amp; Velásquez, J. D. (2017). Combining eye tracking, pupil dilation and EEG analysis for predicting web users click intention. Information Fusion, 35, 51–57. https://doi.org/10.1016/j.inffus.2016.09.003 Smith, C. L., Gwizdka, J., &amp; Feild, H. (2016). Exploring the use of query auto completion: Search behavior and query entry profiles. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 101–110. https://doi.org/10.1145/2854946.2854975 Smith, C. L., &amp; Kantor, P. B. (2008). User adaptation: Good results from poor systems. Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 147–154. Spink, A. (1997). Study of interactive feedback during mediated information retrieval. Journal of the American Society for Information Science. Syed, R., Collins-Thompson, K., Bennett, P. N., Teng, M., Williams, S., Tay, D. W. W., &amp; Iqbal, S. (2020). Improving learning outcomes with gaze tracking and automatic question generation. The Web Conference (WWW). Vakkari, P. (2001a). A theory of the task-based information retrieval process: A summary and generalisation of a longitudinal study. Journal of Documentation, 57(1), 44–60. https://doi.org/10.1108/EUM0000000007075 Vakkari, P. (2016). Searching as learning: A systematization based on literature. Journal of Information Science, 42(1), 7–18. https://doi.org/10.1177/0165551515615833 Vakkari, P. (2000). Cognition and changes of search terms and tactics during task performance: A longitudinal case study. In Content-based multimedia information access-volume 1 (pp. 894–907). Vakkari, P. (2001b). Changes in search tactics and relevance judgements when preparing a research proposal a summary of the findings of a longitudinal study. Information Retrieval, 4(3), 295–310. Villa, R., &amp; Halvey, M. (2013). Is relevance hard work? Evaluating the effort of making relevant assessments. Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 765–768. Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., &amp; Mei, Q. (2018). Optimizing whole-page presentation for web search. ACM Trans. Web, 12(3). https://doi.org/10.1145/3204461 Weber, H., Becker, D., &amp; Hillmert, S. (2019). Information-seeking behaviour and academic success in higher education: Which search strategies matter for grade differences among university students and how does this relevance differ by field of study? Higher Education, 77(4), 657–678. https://doi.org/10.1007/s10734-018-0296-4 Weber, H., Hillmert, S., &amp; Rott, K. J. (2018). Can digital information literacy among undergraduates be improved? Evidence from an experimental study. Teaching in Higher Education, 23(8), 909–926. https://doi.org/10.1080/13562517.2018.1449740 White, R. (2016a). Interactions with search systems. Cambridge University Press. White, R., Dumais, S., &amp; Teevan, J. (2009). Characterizing the influence of domain expertise on web search behavior. Proceedings of the Second ACM International Conference on Web Search and Data Mining - WSDM ’09, 132. https://doi.org/10.1145/1498759.1498819 Wildemuth, B. M. (2004). The effects of domain knowledge on search tactic formulation. Journal of the American Society for Information Science and Technology, 55(3), 246–258. https://doi.org/10.1002/asi.10367 Wilson, M. J., &amp; Wilson, M. L. (2013). A comparison of techniques for measuring sensemaking and learning within participant-generated summaries. Journal of the American Society for Information Science and Technology, 64(2), 291–306. Wilson, T. D. (1999). Models in information behaviour research. Journal of Documentation, 55(3), 249–270. Xu, L., Zhou, X., &amp; Gadiraju, U. (2020). How does team composition affect knowledge gain of users in collaborative web search? Conference on Hypertext and Social Media (HT). Yu, R., Gadiraju, U., Holtz, P., Rokicki, M., Kemkes, P., &amp; Dietze, S. (2018). Predicting User Knowledge Gain in Informational Search Sessions. The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 75–84. https://doi.org/10.1145/3209978.3210064 Zhang, P., &amp; Soergel, D. (2016). Process patterns and conceptual changes in knowledge representations during information seeking and sensemaking: A qualitative user study. Journal of Information Science, 42(1), 59–78. Zhang, X., Cole, M., &amp; Belkin, N. (2011). Predicting Users’ Domain Knowledge from Search Behaviors. Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1225–1226. https://doi.org/10.1145/2009916.2010131 Zlatkin-Troitschanskaia, O., Hartig, J., Goldhammer, F., &amp; Krstev, J. (2021). Students’ online information use and learning progress in higher education A critical literature review. Studies in Higher Education, 1–26. https://doi.org/10.1080/03075079.2021.1953336 https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ https://youtu.be/RkBUZ4At8Qg↩︎ https://www.nngroup.com/articles/f-shaped-pattern-reading-web-content↩︎ and continue to link↩︎ "],["research-questions-and-hypotheses.html", "4 Research Questions and Hypotheses 4.1 Research Questions 4.2 Overarching Hypotheses 4.3 Anticipated Contributions", " 4 Research Questions and Hypotheses 4.1 Research Questions Combining empirical findings and gaps in the literature from the disciplines of Education (Chapter \\[ch_bg_learn\\]) and Information (Chapter \\[ch_bg_search\\]), we saw that: searching for information online is an integral part of new learning (Section \\[sec_bg_learn_info_eval\\]) learning happens when students connect new pieces of information to their existing knowledge structures via assimilation, restructuring, or tuning (Section \\[sec_bg_learn_sensemaking\\]), and this process is influenced by the learner’s individual traits (Section \\[sec_bg_learn_promoting_learning\\]) modern knowledge-work requires less of long term memory, and more of creation of knowledge-artefacts, which should be treated as better assessors and outcomes of learning (Section \\[sec_bg_learn_artefact\\]) domain expertise and search behaviour are strongly linked (Section \\[sec_bg_search_expertise\\]) learning is a process that takes place longitudinally over time (Sections \\[sec_bg_learn_sensemaking\\] and \\[sec_bg_learn_principles\\]), yet only a handful of studies (mostly over a decade ago) have investigated the intertwined process of searchers’ learning and their information searching behaviour over time (Section \\[sec_bg_search_longitudinal_studies\\]) this creates acute gaps in our knowledge about long term information searching and learning behaviour, which is crucial for building learning-centric search systems of the future, which can support sensemaking and knowledge-gain Guided by the above insights, we ask the following research questions in this dissertation proposal, and aim to answer them via a longitudinal study of students’ information search behaviour and learning outcomes over the course of a university semester (Section \\[sec_method_exp_design\\]). For the purposes of this dissertation, we consider learning as change in a student’s knowledge about certain topics over the duration of a university semester. The research questions are first stated in this section, to put them all together in one place for easy reference. Then the overarching hypotheses are discussed in Section 1.2. RQ1: What kind of longitudinal information search behaviours are correlated to the degree of change in students’ knowledge levels and learning outcomes? RQ2: What are the similarities and differences in information search behaviours for tasks where the learning goals are new (new search tasks), versus those where the learning goals are repeated (repeated search tasks)? RQ3: How does externalisation and articulation affect students’ learning outcomes and experiences during search? RQ4: How do (changing) individual differences of students moderate their information search behaviours and learning outcomes? 4.2 Overarching Hypotheses In this Section, we discuss the research framework and hypotheses behind the research questions. The study is primarily planned to be exploratory, therefore the hypotheses are exploratory in nature as well. 4.2.1 Learning as Students’ Transition from Novice to Expert (RQ1, RQ2) Learning and expertise are closely connected: expertise is an evolving characteristic of learners that reflects learning over time, rather than being a static property (Rieh et al., 2016). Domain expertise and search behaviour has been studied, albeit mostly during single lab sessions, and sometimes longitudinally (Section \\[sec_bg_search_expertise\\]). There is a clear gap in understanding how higher education students search for information in the long term, how their information use behaviour develop over time, and how it affects their learning (Zlatkin-Troitschanskaia et al., 2021). RQ1 and RQ2 aims to address some of these gaps. Hypothesis for RQ1: Search behaviours described in Table \\[tab_search_behaviours\\] will occur both within individual search sessions, and across progressive search sessions recorded over a semester, as domain expertise of students increases (Eickhoff et al., 2014). Hypotheses for RQ2: This research question stems from the idea of lifelong or continuous learning: how do search behaviours evolve over time when gaining knowledge about perpetual life skills (e.g., financial literacy). We hypothesize that relevance judgement of previously viewed information on this topic will change over time, as searcher gains more knowledge and expertise the decision or choice to put effort into searching again, or satisfice with previously found information, will have links to motivation and self-regulation 4.2.2 Promoting Better Learning (RQ3, RQ4) Better learning takes place when students articulate and their unformed and still developing understanding, and continue to articulate it throughout the learning process (Section \\[sec_bg_learn_articulation\\]). Also, students’ motivation, self-regulation and metacognition capabilities determine, direct, and sustain the approaches they take to learn (Section \\[sec_bg_learn_promoting_learning\\]). Effective searching for learning is affected by students’ search tactics and information evaluation capabilities (Section \\[sec_bg_learn_info_eval\\]) as well as cognitive capabilities such as memory span (Section \\[sec_bg_search_expertise\\]). Hypothesis for RQ3: articulation during the search as learning process (via concurrent think aloud) will lead to better learning (and possibly better searching) outcomes, than working silently. Hypotheses for RQ4: with respect to the individual differences and contexts in which students search to learn, we speculate the following hypotheses: students showing sustained or increasing metacognition, self-regulation, and motivation over the duration of the semester will put more “effort” into their searches, and demonstrate better learning and search outcomes students with higher memory span will demonstrate more ‘branchiness’ and parallel browsing in their search behaviour students with better information evaluation capabilities will demonstrate better learning and search outcomes 4.3 Anticipated Contributions We anticipate by answering the proposed research hypotheses and question, the results can greatly contribute to the existing knowledge of Interactive Information Retrieval and Educational Sciences in general, and Search as Learning in particular. Referring back to some of the research agenda advocated by the multiple workshops and journal special issues on Search as Learning (Section \\[sec_intro_outline\\]), our research questions aim to investigate (i) the contexts in which students search to learn; (ii) the factors that influence their learning outcomes; and (iii) whether students are more critical consumers of information. Many researchers have expressed their concern with the lack of longitudinal studies in IIR and related domains (Kelly et al., 2009; Koeman, 2020; Zlatkin-Troitschanskaia et al., 2021). If significant relationships were to be found between students’ information search behaviours and learning outcomes, the results of this dissertation can provide great insights and contributions towards (i) understanding how search behaviours can predict learning outcomes; (ii) creating reliable measures, methods, and instruments for capturing changes in people’s knowledge level, learning experiences, and learning outcomes (Rieh, 2020); and (iii) developing search systems that better support learning and sensemaking. References Eickhoff, C., Teevan, J., White, R., &amp; Dumais, S. (2014). Lessons from the journey: A query log analysis of within-session learning. Proceedings of the 7th ACM International Conference on Web Search and Data Mining, 223–232. Kelly, D., Dumais, S., &amp; Pedersen, J. O. (2009). Evaluation challenges and directions for information-seeking support systems. IEEE Computer, 42(3). Koeman, L. (2020). HCI/UX research: What methods do we use? – lisa koeman – blog. https://lisakoeman.nl/blog/hci-ux-research-what-methods-do-we-use/. Rieh, S. Y. (2020). Research area 1: Searching as learning. https://rieh.ischool.utexas.edu/research. Rieh, S. Y., Collins-Thompson, K., Hansen, P., &amp; Lee, H.-J. (2016). Towards searching as a learning process: A review of current perspectives and future directions. Journal of Information Science, 42(1), 19–34. https://doi.org/10.1177/0165551515615841 Zlatkin-Troitschanskaia, O., Hartig, J., Goldhammer, F., &amp; Krstev, J. (2021). Students’ online information use and learning progress in higher education A critical literature review. Studies in Higher Education, 1–26. https://doi.org/10.1080/03075079.2021.1953336 "],["method-proposed-longitudinal-study.html", "5 Method: Proposed Longitudinal Study 5.1 Study Design 5.2 Apparatus 5.3 Search Task Template 5.4 Procedure 5.5 Measurement and Variables 5.6 Data Analysis Plans 5.7 Anticipated Limitations 5.8 Proposed Dissertation Timeline", " 5 Method: Proposed Longitudinal Study We propose to conduct a remote longitudinal study to investigate the research questions and hypotheses discussed in Section \\[sec_rq\\]. The study will primarily be exploratory in nature. The following sections discuss the apparatus, procedure, measurement of variables, plans for data analysis, anticipated limitations, and the expected timeline for the data collection and dissertation defence. 5.1 Study Design Final project description, setting up the search tasks throughout the semester. The remote longitudinal study is planned to span over the course of a typical university semester (approximately 16 weeks). Participants will be recruited from a course (study site) at the University of Texas at Austin (UT Austin). The ideal study site will be a final-project based course, where the final-project report (artefact, Section \\[sec_bg_learn_artefact\\]) will be built over time throughout the semester, with periodic check-ins. Completing the final project will require searching and navigating information online, finding relevant sources for a set of assigned or self-chosen topics, and weaving a narration around the information found in the selected sources. The study design was informed by running a pilot study during Summer 2021 semester, in partnership with two courses at UT Austin School of Information: Information in Cyberspace, and Academic Success in the Digital University. More details of the pilot study are presented in Appendix \\[ch_pilot_study\\]. The final project overview from the Information in Cyberspace course is presented in Figure 1.1. The words or phrases enclosed in \\[\\[square brackets\\]] will be appropriately modified when the study site is finalised. We aim to recruit upwards of 30 participants. Since the study is exploratory in nature, and is intended to inform future confirmatory studies, we are unable to conduct a power analysis. 5.2 Apparatus 5.2.1 YASBIL Browsing Logger The YASBIL browsing logger (Bhattacharya &amp; Gwizdka, 2021) will be utilised for this study. YASBIL (Yet Another Search Behaviour and Interaction Logger) is a two-component logging solution for ethically recording a user’s browsing activity for Interactive IR user studies. It was developed by the author in early Spring 2021, and was employed in the pilot study for data collection and testing. YASBIL comprises a Firefox browser extension and a WordPress plugin. The browser extension logs the browsing activity in the participants’ machines. The WordPress plugin collects the logged data into the researcher’s data server. YASBIL captures participant’s behavioural data, such as webpage visits, time spent on pages, identification of popular search engines and their SERPs, identifying rank of result clicked on SERPs, tracking mouse clicks and scrolls, and the order and sequences of this events. The logging works on any webpage, without the need to own or have knowledge about the HTML structure of the webpage. To protect the privacy of participants, the logger software can be switched on or off, and participants will be instructed (and encouraged) to switch on the logger only when they are performing search activities related to the experiment. YASBIL also offers ethical data transparency and security towards participants, by enabling them to view and obtain copies of the logged data, as well as securely upload the data to the researcher’s server over an HTTPS connection. Although developed using the cross-browser WebExtension API (Mozilla Developer Network, 2021), YASBIL currently works in the Firefox Web Browser. So participants will be instructed to install Firefox and YASBIL on their machines if they choose to participate in the study. 5.2.2 Online Concept Mapping Software As discussed in Section \\[sec_bg_concept_maps\\], concept maps are an appropriate way to understand and assess sense-making and change in knowledge structures. In this study, participants will produce concept maps whenever they are working on an information search task. An appropriate browser based concept-mapping tool will be used for this purpose. The “Sero!” platform12 is a promising choice for this purpose. Sero! “uses concept mapping for knowledge and learning assessment” and “facilitates big-picture comprehension and gives educators deeper insight into students’ thinking”. 5.3 Search Task Template Template for each search task, adapted from the Information in Cyberspace course at the UT Austin School of Information. Phrases enclosed in \\[\\[square brackets\\]] will be modified for individual tasks. Participants of the longitudinal study will be performing several search tasks throughout the duration of the semester (Section 1.4). The search tasks mentioned will generally be on the topics of the material taught in the course. The generic template for each search task is presented in Figure 1.2, and is adapted from the Information in Cyberspace course. The words or phrases enclosed in \\[\\[square brackets\\]] will be appropriately modified when the study site is finalized. 5.4 Procedure Overview of the study procedure. The longitudinal study will consist of six data collection stages, as outlined in Figure 1.3. Each of these stages are described below. 5.4.1 SUR1: Entry Survey Participants will be recruited for the study via the entry survey \\[SUR1\\]. The description of the study and the link to the survey will be posted in the Learning Management System used for the course (Canvas). The entry survey will serve dual purpose: recruit participants, as well as capture their individual-differences, or moderating variables. This is done so as to avoid having to capture all of this information in the initial session (SES1), which would have made the SES1 session overly long. Details of the data captured in SUR1 are described below, with references to sections in the Appendix, where the full-text of the questionnaire can be found. Consent Form: The first page of the entry survey will be the online consent form for participating in the study. Participants will be able to proceed once they provide consent. Demographics: (Appendix \\[app_demographics\\]) demographic information Search and IT proficiency: (Appendix \\[app_search_it_proficiency\\]) Captures previous search experience, and proficiency in navigating the web. Some items are adapted from the Digital Health Literacy Instrument (DHLI) by (Van Der Vaart &amp; Drossaert, 2017), and the Search Self-Efficacy scale (SSE) by (Brennan et al., 2016). Course Load and other engagements: (Appendix \\[app_course_load\\]) To determine how busy the participant will be in the semester, and how much time they plan to allocate for the course with which the study is integrated. This will help to establish the learner’s context. Note-taking Strategies: (Appendix \\[app_note_taking_strategies\\]) Captures styles and strategies used by participants to take notes. Adapted from Listening and Note Taking Survey by (Penn State Learning, 2021), and Note Taking Strategies Inventory by (UMass Amherst Student Success, 2021). Motivation: (Appendix \\[app_imi\\]) Adapted from the Intrinsic Motivation Inventory (IMI) by (Ryan, 1982), which is a multidimensional measurement device intended to assess participants’ subjective experience related to a target activity (the assignments for the course they are taking). The instrument assesses participants’ interest/enjoyment, perceived competence, effort/importance, pressure/tension, perceived choice, and value/usefulness, while performing a given activity, thus yielding six subscale scores. Three items in the value/usefulness subscale will be completed with contextual information when the study site is finalized. The pressure/tension and the perceived choice components will not be included in the entry survey, and will be present in the mid-term \\[SUR2\\] and exit \\[SUS3\\] surveys. Self-regulation: (Appendix \\[app_srq\\]) Adapted from the Self-Regulation Questionnaire (SRQ) by (J. M. Brown et al., 1999), which assess seven self-regulatory processes through self-report: receiving relevant information, evaluating the information and comparing it to norms, triggering change, searching for options, formulating a plan, implementing the plan, and assessing the plan’s effectiveness (Section \\[sec_bg_learn_self_regulation\\]). Metacognition: (Appendix \\[app_mai\\]) Adapted from the Metacognivite Awareness Inventory (MAI), originally proposed by (Schraw &amp; Dennison, 1994) as a 52-item true / false questionnaire, and later revised by (Terlecki &amp; McMahon, 2018) to use five-point Likert scales. The instrument measures two components of cognition through self-report: knowledge about cognition, and regulation of cognition (Section \\[sec_bg_learn_metacognition\\]). After completing the entry survey, participants will be asked to prepare for the initial synchronous session, \\[SES1\\], by (i) installing Firefox web browser and the YASBIL extension on their machines, (ii) get a quick introduction to concept maps (by watching a short video), and (iii) familiarising themselves with the Sero! learning platform (for creating and assessing concept maps). This is a one-time step. If a participant cannot find the time for this step, they will be informed that an extra 5-10 minutes will be taken in the beginning of SES1 to complete this step. The entry survey and the software installation is expected to take about 10-15 minutes to complete. Participants will be compensated with USD 5 for their time for completing this step. The survey will be floated in the first week of the semester, and will be closed after sufficient a number participants have been recruited. 5.4.2 SES1: Initial Session Prompts for the search task repeated in Initial Session \\[SES1\\] and Final Session \\[SES3\\]. For the Initial Session, (aka SES1), participants will be invited to a remote study session over a video-conferencing platform (Zoom). The purpose of the the pre-test will be to establish baseline search behaviour and domain knowledge of the participants. The study will consist of a training task, two actual search tasks, one website reliability assessment, and a memory span test. Each of these components are described below. Training Task: Participants will perform a training task to familiarize themselves with how to operate the YASBIL browser extension to log their browsing activity, and how to create concept maps while they are searching. The training task is expected to take around 5-10 minutes. Two Search Tasks: Participants will perform two search tasks, of which one will be repeated in the final session \\[SES3\\] at the end of the semester, and the other will not be repeated. This will help to answer research question RQ2 (Section \\[sec_rq\\]). The order of the two search tasks will be randomized. The generic format the search tasks are described in Figure 1.2. The repeated search task will be on the topic of financial literacy, a topic that may be considered to be universally important to college students, and part of lifelong learning. The SES1 and SES3 versions of the repeated task are presented in Figure 1.4. The non-repeated search task will be on topics that will be taught in the course serving as the site for the study. Examples of search tasks used in the Pilot Study are in Appendix \\[ch_pilot_study\\]. Each search task is expected to take around 20 minutes. To answer RQ3 (effect of externalisation and articulation in learning), each participant will perform one of the search tasks while thinking aloud (concurrent think-aloud, or CTA condition), and perform the other one in silence (silent condition). The choice of the task for each of the conditions will be randomized and balanced. Each search task will begin with a pre-task questionnaire (Appendix \\[app_pre_task_qsn\\]), which asks participants to self-rate their pre-search topic knowledge and interest. Then participants will turn on the YASBIL browsing logger and start searching. The deliverable for each search task will be a written summary (artefact), and a concept map. After participants are satisfied with the quality of the deliverable, they will turn off YASBIL browsing logger, and proceed to the post-task questionnaire. The post-task questionnaire (Appendix \\[app_post_task_qsn\\]) asks participants to self-rate their post-search topic knowledge, search experience, interest and motivation, and overall perceptions. The pre-task and post-task questionnaires are adapted from (Collins-Thompson et al., 2016; Crescenzi, 2020). After the two search tasks are completed, participants will answer questions about whether they preferred the think-aloud condition or the silent condition, and why (Appendix \\[app_cta_v\\_silent\\]). Webpage Comparison Assessment: To assess participants’ (mis)information evaluation capabilities, they will perform a Website Comparison assessment created by the (Stanford History Education Group, 2021). This task asks students to compare two websites and select the one that they would use to begin research on a topic. One of the pages is a Wikipedia article. The other has “.edu” in its URL, but the page reveals that the content is a student-written blog post created as part of a university course. Many students have been taught that Wikipedia pages are completely unreliable and should be avoided. Many have also been taught that sites with a .edu domain are trustworthy. This assessment gauges their ability to think in more nuanced ways about these kinds of sites. The website reliability assessment is expected to take 10 minutes. Memory Span Test: The session will conclude with the assessment of the participant’s working memory capacity (WMC) using a memory span task (Francis et al., 2004). Memory span assessment is kept in the synchronous session because it is a timed task, and needs to conducted in a controlled (experimenter observed) condition. Participants will be asked to share their screen for the whole duration of the session. Their screens and audio will be recorded for the entire duration. They may choose to turn off their video. The total time for SES1 is expected to not exceed 1.5 hours (90 minutes). Participants will be compensated with USD 25 for this session. 5.4.3 SES2: Longitudinal Tracking The longitudinal tracking \\[SES2\\] will be conducted asynchronously over the duration of the semester, to understand the change (or lack thereof) of participants’ search behaviour and knowledge gain over time. Whenever participants will work on different parts of their final project research paper (which will be termed as SES2a, SES2b, …etc.), as described in Figure  1.1, they will use Firefox web browser, and will log their browsing activity using the YASBIL browsing logger. To protect their privacy, participants will be instructed to turn YASBIL on only when they are searching for information related to the course. In addition to the submitted working-draft of their research paper, participants will submit a cumulative concept map with each document submission. The cumulative concept map will help to track participants’ evolution of knowledge about the final project topic(s) over the course of the semester. Participants will receive reminder emails before the deadline of each assignment, to remind them to use Firefox, turn YASBIL on, and incrementally update the concept map. Participants will receive USD 5 per each assignment for which they log data, up to a maximum of USD 20 for four assignments. 5.4.4 SUR2: Mid-Term Survey The mid-term survey \\[SUR2\\] will take place around the mid point of the semester (Week 8-9). The purpose is to track whether any of the participants’ individual difference measures (e.g. motivation, metacognition, course load etc.) changed during the first half of the semester. This survey will essentially be a replica of the Entry Survey \\[SUR1\\] (Section 1.4.1), with two modifications. First, the consent form and the demographics sections will be absent. Second, Intrinsic Motivation Inventory (IMI) will include the ‘pressure/tension’ and the ‘perceived choice’ subscales, as these scales are more meaningful after an activity has taken place (Ryan, 1982). The IMI will also be reworded to reflect the mid-point of the semester (Appendix \\[app_midterm_survey\\]). Participants will be compensated with USD 5 for their time for completing this step. 5.4.5 SES3: Final Session The Final Session \\[SES3\\] will be similar in structure to the Initial Session (SES1), and will take place at the end of the semester, after all the course related tasks are completed by the student. The purpose of the session is to record the ‘evolved’ search behaviour, and final knowledge state. Participants will perform two search tasks, one website reliability assessment task, and take the memory span test once again. In the end, there will be a short semi-structured interview. Of the two search tasks, the topic of one will be repeated from SES1 (financial literacy, Figure 1.4), while the topic of the other will come from the course material. In both search tasks, participants will be given the option of not searching if they feel confident enough to answer the search task questions from their prior knowledge. The deliverables for each search-task will be a written summary (artefact) and a concept map (Figure 1.2). Following the two search task, participants will perform another website comparison/reliability task created by the (Stanford History Education Group, 2021), which will assess their evolved information evaluation skills. Then they will retake the memory span test (Francis et al., 2004). A semi-structured interview will be conducted in the end, where participants will reflect on their overall searching and learning experience. Certain ‘interesting’ handpicked sessions from their submitted logs may be identified and questions about them can also be asked to participants. A list of the interview questions asked in the Pilot Study (Appendix \\[ch_pilot_study\\]) are presented in Appendix \\[app_post_task_interview\\], which can be reused. Similar to SES1, participants will be asked to share their screen for the whole duration of the session, except for the interview, whence they can stop screen-sharing. Their screens and audio will be recorded for the same. They may choose to turn off their video. The total time for SES3 is expected to not exceed 1.5 hours (90 minutes). Participants will be compensated with USD 25 for this session, and will be asked to complete the Exit Survey \\[SUR3\\] as soon as convenient. 5.4.6 SUR3: Exit Survey The exit survey \\[SUR3\\] will take place after the Final Session \\[SES3\\]. The purpose is to record the final state of the participants’ individual difference measures (e.g. motivation, metacognition, course load etc.), and whether they changed during the second half of the semester. This survey will essentially be a replica of the mid-term survey \\[SUR2\\] (Section 1.4.4), with the Intrinsic Motivation Inventory (IMI) reworded to reflect the end-point of the semester (Appendix \\[app_final_survey\\]). Participants will be compensated with USD 5 for their time for completing this step. If participants do not take the survey within three days (say) after appearing for SES3, they will be sent reminder emails. Participants will be compensated with a bonus payment of USD 15, if they complete all the parts of the study without missing any component. 5.5 Measurement and Variables 5.5.1 Independent / Explanatory: Search Interaction and Process Measures The independent variables will be the search process behavioural measures. Information searching behaviour will be operationalized using a battery of search process measures, based on the three-stages of user interaction discussed in Section \\[sec_bg_search_3\\_stage\\]. These measures include query reformulation types and measures (Table \\[tab_res_Q\\_QRT_txnmy\\]), SERP examination measures, content page examination measures, and overall search session measures (Table \\[tab_search_behaviours\\]). A non-exhaustive list of such search process measures which have been used in prior literature is presented in Appendices \\[sec_app_vars_qry\\] through \\[sec_app_vars_overall_search\\]. 5.5.2 Dependant / Outcome: Learning Measures Learning (knowledge gain) will constitute the dependant variables, or factor variables, when dichotomized via median split. Learning outcomes are planned to be assessed by: (i) analysis of concept maps (Halttunen &amp; Jarvelin, 2005) (Appendix \\[sec_app_vars_concept_maps\\]); (ii) analysis of written summaries / knowledge artefacts (M. J. Wilson &amp; Wilson, 2013); and (iii) instructor awarded scores and grades received by students in the course, which will be obtained via FERPA release. Time, resources and feasibility permitting, other possible ways of assessing learning can be by using (iv) Online Research and Comprehension Assessment (ORCA) (Leu et al., 2015 Table 3), (Kanniainen et al., 2021 Appendix A); and (v) information-use from websites in written artefacts (Vakkari, 2020; Vakkari et al., 2019). 5.5.3 Moderator: Individual Differences The variables of individual differences that are hypothesized to moderate learning are (i) motivation, scored using Intrinsic Motivation Inventory (IMI) (Ryan, 1982); (ii) self-regulation, scored using Self-Regulation Questionnaire (SRQ) (J. M. Brown et al., 1999); (iii) metacognition, scored using revised Metacognivite Awareness Inventory (MAI) (Schraw &amp; Dennison, 1994; Terlecki &amp; McMahon, 2018) (iv) memory span, scored using memory span test (Francis et al., 2004); (v) search proficiency, scored using Digital Health Literacy Instrument (DHLI) (Van Der Vaart &amp; Drossaert, 2017) and Search Self-Efficacy Scale (SSE) (Brennan et al., 2016); (vi) information evaluation capabilities (mastery / emerging / beginner), scored according to rubrics provided by (Stanford History Education Group, 2021) assessments (an example grading rubric is present in Appendix \\[sec_app_pilot_ses3\\] Task 3). 5.6 Data Analysis Plans Exploratory data analysis (such as time series plotting) and descriptive statistics will be used to identify if changes in search process / interaction measures can be visually observed over the course of time. Inferential statistics (difference between groups) will be employed to test if there are significant differences in the learning measures between student groups who learn more versus learn less. Pattern Mining and clustering approaches may also be used to identify clusters or patterns in the search process (time-series) data, and see if these clusters correlate with high and low learning. An example of measuring such changes in variables can be found in (Mao et al., 2018, sec. 3.2). Advanced search interactions such as parallel browsing behaviour (multi-tabbed and multi-windowed browsing) may also be analysed (J. Huang &amp; White, 2010; Labaj &amp; Bielikova, 2012). 5.7 Anticipated Limitations There are foreseeable limitations to this proposed longitudinal study. First, there may not be enough participants who sign up for the study. The remedy for this is choosing a course with a large number of students, and using appealing messaging in the recruitment material (e.g., an attractive video message was used to recruit participants in the Pilot Study13). Second, participants may drop off due to various reasons during the study. This can be tackled by regularly communicating with the participants, keeping them engaged with affectionate, caring and encouraging messaging, and letting them know that their participation is valued highly by the researchers. Third, participants may not show any changes in their search behaviours, of the changes may be random. As one anonymous reviewer put it “the smart kids will show up smart and with good search skills and they will leave smart with good search skills and they will not change their inherent behavior over the time. Similarly, the dumb kids will show up dumb, do whatever keeps them dumb and end up dumb.” Alternatively, there may be are not changes over time, but rather strategies that are consistently followed by students who will learn more, and students who will learn less. There is no easy fix to this, and if this happens, it will be treated as a finding from the study. A possible rescue from this situation would then be to use the (semi) qualitative data – the semi structured interview at the end, the concurrent think alouds, and others – to derive interesting findings. 5.8 Proposed Dissertation Timeline image November 2021: prepare and submit IRB proposal December 2021: defend proposal, and if changes to the study protocol are suggested by the committee, submit them as an IRB amendment January - May 2022: conduct longitudinal study; a week-by-week schedule is in Table \\[tab_timeline_data_collection\\] Summer and Fall 2022: two backup semesters for data collection, if something goes wrong in Spring 2022, especially due to COVID-19 pandemic situations. August 2022 - February 2023: analyze data and write dissertation April 2023: Dissertation defense May 2023: revise and complete dissertation References Bhattacharya, N., &amp; Gwizdka, J. (2021). YASBIL: Yet another search behaviour (and) interaction logger. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2585–2589. Brennan, K., Kelly, D., &amp; Zhang, Y. (2016). Factor analysis of a search self-efficacy scale. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 241–244. Brown, J. M., Miller, W. R., &amp; Lawendowski, L. A. (1999). The self-regulation questionnaire. In V. L. &amp; J. T. L. (Eds.), Innovations in clinical practice: A sourcebook (Vol. 17, pp. 281–292). Professional Resource Press/Professional Resource Exchange. Collins-Thompson, K., Rieh, S. Y., Haynes, C. C., &amp; Syed, R. (2016). Assessing learning outcomes in web search: A comparison of tasks and query strategies. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 163–172. Crescenzi, A. M. C. (2020). Adaptation in Information Search and Decision-Making under Time Pressure [PhD thesis, The University of North Carolina at Chapel Hill University Libraries]. https://doi.org/10.17615/YT6K-AC37 Francis, G., MacKewn, A., &amp; Goldthwaite, D. (2004). CogLab on a CD. Wadsworth Publishing Company. Halttunen, K., &amp; Jarvelin, K. (2005). Assessing learning outcomes in two information retrieval learning environments. Information Processing &amp; Management, 41(4), 949–972. https://doi.org/10.1016/j.ipm.2004.02.004 Huang, J., &amp; White, R. (2010). Parallel browsing behavior on the web. Proceedings of the 21st ACM Conference on Hypertext and Hypermedia - HT ’10, 13. https://doi.org/10.1145/1810617.1810622 Kanniainen, L., Kiili, C., Tolvanen, A., Aro, M., Anmarkrud, Ø., &amp; Leppänen, P. H. T. (2021). Assessing reading and online research comprehension: Do difficulties in attention and executive function matter? Learning and Individual Differences, 87, 101985. https://doi.org/10.1016/j.lindif.2021.101985 Labaj, M., &amp; Bielikova, M. (2012). Modeling parallel web browsing behavior for web-based educational systems. 2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA), 229–234. https://doi.org/10.1109/ICETA.2012.6418330 Leu, D. J., Forzani, E., Rhoads, C., Maykel, C., Kennedy, C., &amp; Timbrell, N. (2015). The New Literacies of Online Research and Comprehension: Rethinking the Reading Achievement Gap. Reading Research Quarterly, 50(1), 37–59. https://doi.org/10.1002/rrq.85 Mao, J., Liu, Y., Kando, N., Zhang, M., &amp; Ma, S. (2018). How does domain expertise affect users’ search interaction and outcome in exploratory search? ACM Transactions on Information Systems, 36. Mozilla Developer Network. (2021). Building a cross-browser extension - mozilla | MDN. https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Build_a_cross_browser_extension. Penn State Learning. (2021). Listening and note taking survey | penn state learning. https://pennstatelearning.psu.edu/listening-and-note-taking-survey; Pennsylvania State University. Ryan, R. M. (1982). Control and information in the intrapersonal sphere: An extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43(3), 450. Schraw, G., &amp; Dennison, R. S. (1994). Assessing Metacognitive Awareness. Contemporary Educational Psychology, 19(4), 460–475. https://doi.org/10.1006/ceps.1994.1033 Stanford History Education Group. (2021). Webpage comparison | Civic Online Reasoning. https://cor.stanford.edu/curriculum/assessments/webpage-comparison. Terlecki, M., &amp; McMahon, A. (2018). A Call for Metacognitive Intervention: Improvements Due to Curricular Programming in Leadership. Journal of Leadership Education, 17(4), 130–145. https://doi.org/10.12806/V17/I4/R8 UMass Amherst Student Success. (2021). Note taking strategies inventory | success@UMass. https://www.umass.edu/studentsuccess/sites/default/files/inline-files/note-taking-strategies_0.pdf. Vakkari, P. (2020). The Usefulness of Search Results: A Systematization of Types and Predictors. Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, 243–252. https://doi.org/10.1145/3343413.3377955 Vakkari, P., Völske, M., Potthast, M., Hagen, M., &amp; Stein, B. (2019). Modeling the usefulness of search results as measured by information use. Information Processing &amp; Management, 56(3), 879–894. https://doi.org/10.1016/j.ipm.2019.02.001 Van Der Vaart, R., &amp; Drossaert, C. (2017). Development of the digital health literacy instrument: Measuring a broad spectrum of health 1.0 and health 2.0 skills. Journal of Medical Internet Research, 19(1), e27. Wilson, M. J., &amp; Wilson, M. L. (2013). A comparison of techniques for measuring sensemaking and learning within participant-generated summaries. Journal of the American Society for Information Science and Technology, 64(2), 291–306. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ https://youtu.be/RkBUZ4At8Qg↩︎ "],["ch_pilot_study.html", "6 Prior Work: Pilot Study 6.1 SES1: Initial Session 6.2 SES2: Longitudinal Tracking 6.3 SES3: Final Session", " 6 Prior Work: Pilot Study A pilot study was conducted in the Summer of 2021 at the School of Information, University of Texas at Austin (Texas iSchool). This was mainly a feasibility study to determine the technical logistics and participant retention rates. It included SES1, SES2, and SES3 from the proposed study procedure (Figure \\[fig_study_procedure\\]). There was no recording of individual differences via recruitment, mid-term, and final surveys. It also did not include submission of concept maps. Eight students from two courses at the Texas iSchool – Academic Success in the Digital University (ACS), and Information in Cyberspace (CYB) – participated in the pilot study. The study ran from start of June 2021 to mid-August 2021. There was no participant drop-off. Synchronous sessions (SES1 and SES3) were conducted over the Zoom video conferencing platform. Log data was captured using the YASBIL browser extension (Bhattacharya &amp; Gwizdka, 2021). All setup and technical logistics worked out properly, without any major technological issues. Details of the search task descriptions are presented below. Participants were compensated with USD 15 for SES1, 15 points of extra course credit for longitudinal tracking in SES2, and USD 15 for SES3. 6.1 SES1: Initial Session Participants performed a training task to familiarise themselves with the YASBIL browser extension. The they performed two search tasks as described below. Each search task was followed by measurement of mental workload using NASA-TLX.   Task 1: Financial Literacy (Repeated in SES3) Money management and financial literacy are essential life skills, and what better time to learn about them than in college? Write a note to your future self, about essential money-related advice and skills that college students should know and practice. What to do: Find at least 3 unique, good quality online resources that are relevant to this topic Look for resources that help establish connections and develop a narrative What to deliver: Write a summary of the lessons, advice, and/or tips you found across the different resources. This is a note to your future self, so the narrative can be in a format that is most useful and interesting to YOU Paste the links of ALL the resources that you finally selected to develop your narrative, in the second text box, one link per line   Task 2: Social Media during COVID-19 (Topic was part of course content in ACS and CYB) “What was the role of Social Media during the COVID-19 pandemic? How did it affect people’s lives during quarantine and social distancing?” Suppose a family member (say your aunt) or a friend asked you these question over a phone call, and you want to talk to them on this topic for a couple of minutes. What to do: Find at least 3 unique, good quality online resources that are relevant to this topic Look for resources that help establish connections and develop a narrative What to deliver: Write a short summary of the content that you found across the different resources. The length and writing style can be such that you can read it out to your family member/friend over a phone call, without them losing interest. In the summary, briefly mention your thoughts about each resource - do you agree or disagree with the content in the resource? Anything else? Paste the links of ALL the resources that you finally selected to develop your narrative, in the second text box, one link per line 6.2 SES2: Longitudinal Tracking The longitudinal tracking session SES2 involved student participants submitting log data for two final-project assignments for the ACS course, and four final project assignments for the CYB course. Participants received reminder emails to log and sync their data a few days before each assignment was due. Seven (out of 8) participants logged their data and synced it with our data server in a timely fashion, without major technical issues. One participant CYB course forgot to log their data for the first two assignments, despite the email reminder. However, upon following up with them, they remembered to log their data for the third and fourth sessions. 6.3 SES3: Final Session All eight participants from SES1 completed SES3 (no drop off). Participants performed two search tasks, and one website reliability evaluation task as described below. All three tasks were followed by measurement of mental workload using NASA-TLX. At the end of the tasks, they underwent a short semi-structured interview to reflect on their overall study experience. The interview questions can be found in Appendix \\[app_post_task_interview\\].   Task 1: Financial Literacy (Repeated from SES1) At the start of the semester, you wrote a note to your future self, about essential money-related advice and skills that college students should know and practice. Here is what you wrote: [dynamic content showing participants’ previous responses] Here are the resources you took help from: [dynamic content showing participants’ previous responses] Now you have a chance to update or revise the note with more information. You can either choose to write afresh, or copy-paste the note from above into the first textbox below and add to it /edit it. Feel free to search the web if you need to, after turning YASBIL on. You can choose NOT to search, as well. If you do choose to search, please paste the links of ALL the resources that you finally selected for updating your note, one link per line, in the second textbox. The links can be the same ones you visited earlier, or different. Did you need to search the web for updating the note? Why?   Task 2: HTML CSS (Topic was part of course content in ACS and CYB) In your course, you studied about websites, HTML, and CSS. Therefore, for answering the questions below, you may choose NOT to search the web, if you feel you can answer the questions reasonably well. If you do need to search the web, feel free to do so, after turning on YASBIL. As you understand these concepts, please explain (with examples if necessary) what is the purpose of HTML? what is the purpose of CSS? how do HTML and CSS come together when someone visits a website? List as many HTML tags as you can, one per line, in the following format: [HTML tag] - [few words explaining the function of the tag] List as many CSS properties as you can, one per line, in the following format: [CSS property] - [few words explaining the function of the property] Did you need to search the web for this task? Why?   Task 3: Website Reliability Evaluation (From Stanford History Education Group14) You are researching children’s health and come across this website: https://acpeds.org. Please decide if this website is a trustworthy source of information on children’s health. You may use any information on this website, or you can open a new tab and do an Internet search if you want. Take about 5 minutes to complete this task. Turn YASBIL on before proceeding. Is this website a trustworthy source to learn about children’s health? Explain your answer, citing evidence from the webpages you used. Be sure to provide the URLs to the webpages you cite in the next textbox. Please paste the URLs to the webpages you used to explain your answer above, one per line. Grading Rubric for Task 3 (as provided by SHEG): This task presents students with the website of the American College of Pediatricians (ACPeds.org) and asks them whether it is a trustworthy source to learn about children’s health. Despite the site’s professional title and appearance, the American College of Pediatricians is not the nation’s major professional organization of pediatricians. That designation belongs to the similarly named American Academy of Pediatrics. The American College of Pediatricians is a conservative advocacy organization established in 2002 in response to the American Academy of Pediatrics’ support of adoption by same-gender couples. The American College of Pediatricians website features a mission statement that reads, in part, “We recognize the basic father-mother family unit, within the context of marriage, to be the optimal setting for childhood development.” News releases on the site include headlines like, “Same-Sex Marriage – Detrimental to Children” and “Know Your ABCs: The Abortion Breast Cancer Link.”   This exercise is an open web search in which students are free to stay on the American College of Pediatricians site or leave it to search for information about the group. Successful students will look beyond the surface features of the site and detect its agenda from its new releases or other focus issues. A faster route, however, is to leave the site almost immediately to search for reliable information about the true agenda of this organization.   Mastery: Student rejects the website as a trustworthy source and provides a clear rationale. Student provides reliable supporting evidence and cites the source of information. Emerging: Student rejects the website as a trustworthy source and provides supporting evidence. However, the response falls short of Mastery because: (i) Student provides relevant evidence and says where the evidence is from, but the explanation is incomplete; (ii) Student provides a complete explanation that is supported by relevant evidence but does not say where the evidence is from. Beginning: Student rejects the source but provides an incoherent, irrelevant, or unreasonable explanation; or the student simply accepts the source as trustworthy. After Task 3, participants participated in a semi-structured interview to reflect on their overall searching and learning experience, as well as participating in the study. The questions asked in the interview are presented in Appendix \\[app_post_task_interview\\]. References Bhattacharya, N., &amp; Gwizdka, J. (2021). YASBIL: Yet another search behaviour (and) interaction logger. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2585–2589. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ "],["appendix:signup_survey.html", "7 SUR1: Entry Survey 7.1 Demographics 7.2 Search and IT Proficiency 7.3 Course Load and Other Engagements 7.4 Note-taking Strategies 7.5 Motivation 7.6 Self-regulation 7.7 Metacognition", " 7 SUR1: Entry Survey 7.1 Demographics Please select the degree level/name of the program you are in. Please state which year of the program you are in. Please state your major(s) Do you have native-level familiarity with English language? Yes / No / Other: Please state your age (in years) Please state your gender With which ethnicities do you identify? Please check all that apply: African African American / Black Asian - East Asian - South East Asian - South Asian - Middle East Caucasian / White Hispanic / Latinx Native American Pacific Islander Mixed Other: Are you an international student? Yes / No; If Yes, where are you originally from? Please enter an email address that you check regularly. We will send communications and compensation information to this email address. Your name as you would like us to address you. 7.2 Search and IT Proficiency Which device(s) and browser(s) do you normally use to surf the internet? Chrome Safari Firefox Edge Opera Other None Desktop Laptop Tablet Smartphone How comfortable are you with using Mozilla Firefox to search information on the internet? I do not know how to use Mozilla Firefox. I have never used Mozilla Firefox. I feel very uncomfortable to use Mozilla Firefox. I feel uncomfortable to use Mozilla Firefox. I feel neither comfortable nor uncomfortable to use Mozilla Firefox. I feel comfortable to use Mozilla Firefox. I feel very comfortable to use Mozilla Firefox. Other: Which search engines do you normally use? Google Bing Baidu Yahoo! Yandex DuckDuckGo Other:   The following items are adapted from the Digital Health Literacy Instrument (DHLI) by (Van Der Vaart &amp; Drossaert, 2017). On a scale of 1 to 5 … (1) Very difficult/Very seldom | Difficult/Seldom | Neutral | Easy/Often | Very easy/Very often (5) How easy or difficult is it for you to… Use the keyboard of a computer (e.g., to type words)? Use the mouse (e.g., to put the cursor in the right field or to click)? Use the buttons or links and hyperlinks on websites? When you search the Internet for information, how easy or difficult is it for you to … Make a choice from all the information you find? Use the proper words or search query to find the information you are looking for Find the exact information you are looking for? Decide whether the information is reliable or not? Decide whether the information is written with commercial interests (e.g., by people trying to sell a product)? Check different websites to see whether they provide the same information? Decide if the information you found is applicable to your situation? Apply the information you found in your daily life? Use the information you found to make decisions about your life When you search the Internet for information, how often does it happen that… You lose track of where you are on a website or the Internet? You do not know how to return to a previous page? You click on something and get to see something different than you expected?   The following items are adapted from the Search Self-Efficacy Scale (SSE) by (Brennan et al., 2016). On a scale of 1 to 5, how confident are you that you can … (1) Not at all confident | Neither confident nor unconfident | Totally confident (5) Identify the major requirements of the search from the initial statement of the topic. Correctly develop search queries to reflect my requirements. Use special syntax in advanced searching (e.g., AND, OR, NOT). Evaluate the resulting list to monitor the success of my approach. Develop a search query which will retrieve a large number of appropriate articles. Find an adequate number of articles. Find articles similar in quality to those obtained by a professional searcher. Devise a query which will result in a very small percentage of irrelevant items on my list. Efficiently structure my time to complete the task. Develop a focused search query that will retrieve a small number of appropriate articles. Distinguish between relevant and irrelevant articles. Complete the search competently and effectively. Complete the individual steps of the search with little difficulty. Structure my time effectively so that I will finish the search in the allocated time. 7.3 Course Load and Other Engagements How many total weekly hours of coursework are you registered for this semester? How many weekly hours do you anticipate putting in for studying this course? What are your other time commitments, as hours per week? (enter 0 if not applicable) jobs extra-curriculars other Do you hold a position of responsibility (officer / committee member) in any (student) organisation? Yes / No 7.4 Note-taking Strategies Adapted from Listening and Note Taking Survey by (Penn State Learning, 2021), and Note Taking Strategies Inventory by (UMass Amherst Student Success, 2021). For each question, choose the response that best describes your actions (not the one that describes what you think you should be doing). There are no right or wrong answers. In general (not specifically for this course) I take notes using (check all that apply) Paper and Pen / Pencil Laptop / Desktop Tablet with Keyboard Tablet with Stylus / Digital Pen When taking notes on the laptop, I minimize distractions by: Are you familiar with “concept maps”? Never heard of it Heard of it, never used it Used it a few times in the past Used it quite often in the past; now do not use it Use it regularly On a scale of 1 to 5 … (1) Never | Rarely | Sometimes | Often | Always (5) I read my assignments before I go to lecture. I find lectures interesting and/or challenging. My lecture notes are well organized. I recognize main ideas in lectures. I recognize supporting details of main ideas. I recognize patterns in lectures, e.g., cause-effect, concept-example. My lecture notes are complete. I recognize relationships between lecture and readings. I integrate my lecture notes with my reading notes. I summarize my notes, both lecture and reading, in my own words. I review my notes immediately after class. I conduct weekly reviews of my notes. I edit my notes within 24 hours after class. I take notes I put dates on my notes I makes notes in the margins of the text when I read (on paper / digital medium, e.g. iPad and Apple Pencil) I pause periodically while reviewing notes to summarize or paraphrase the information. I use diagrams in my notes I use different colours when writing notes I create outlines, concept maps or organizational charts of how ideas fit together. I write down questions I want to ask the instructor I reorganize and fill in notes I took in class I put things in my own words I rewrite my notes I use abbreviations in my notes I write out my own descriptions of the main concepts I keep track of things I do not understand and note when they finally become clear and what made that happen I understand my notes I refer back to my notes How do you organise your notes? How often do you refer back to your notes? Have you ever wished that you had written better notes? Yes / No. Why? How long do you store your notes for? Till the end of the semester End of academic year End of college Lifelong How do you search for a bit of information in your notes? 7.5 Motivation Adapted from Intrinsic Motivation Inventory (IMI) (Ryan, 1982). Items will be randomly ordered. For each of the following statements, please indicate how true it is for you, using the following scale: (1) not at all true | somewhat true | very true (5) Interest/Enjoyment I will enjoy taking this course very much. This course will be fun to do. I think this will be a boring course. (R) This course will not hold my attention at all. (R) I would describe this course as very interesting. I think this course will be quite enjoyable.   Perceived Competence I think I will be pretty good at this course. I think I will be doing pretty well at this course, compared to other students. After working at this course for awhile, I will feel pretty competent. I think I will be satisfied with my performance in this course. I think I am pretty skilled at this course. This is a course that I think would not be able to do very well. (R)   Effort/Importance I plan to put a lot of effort into this course. I don’t think I will try very hard to do well at this course. (R) I will try very hard on this course. It is important to me to do well in this course. I do not plan to put much energy into this course. (R)   Value/Usefulness 15 I believe this course could be of some value to me. I think that doing this course is useful for learning about I think this course is important to take because it can I think taking this course could help me to I believe taking this course could be beneficial to me. I think this is an important course.   Scoring directions: Score each response from 1 (not at all true) to 5 (very true). Then reverse score the items marked with (R). To do that, subtract the item response from 6, and use the resulting number as the item score. Then, calculate subscale scores by averaging across all of the items on that subscale. The subscale scores are then used in the analyses of relevant research questions. 7.6 Self-regulation Self-Regulation Questionnaire (SRQ) by (J. M. Brown et al., 1999). Please answer the following questions by selecting the option that best describes how you are. There are no right or wrong answers. Work quickly and don’t think too long about your answers. (1) Strongly Disagree | Disagree | Neutral | Agree | Strongly Agree (5) 2 I usually keep track of my progress toward my goals. My behavior is not that different from other people’s. (R) Others tell me that I keep on with things too long. (R) I doubt I could change even if I wanted to. (R) I have trouble making up my mind about things. (R) I get easily distracted from my plans. (R) I reward myself for progress toward my goals. I don’t notice the effects of my actions until it’s too late. (R) My behavior is similar to that of my friends. Evaluating It’s hard for me to see anything helpful about changing my ways. Triggering R I am able to accomplish goals I set for myself. Searching I put off making decisions. Planning R I have so many plans that it’s hard for me to focus on any one of them. (R) I change the way I do things when I see a problem with how things are going. It’s hard for me to notice when I’ve “had enough” (alcohol, food, sweets, internet, social media) (R) I think a lot about what other people think of me. I am willing to consider other ways of doing things. If I wanted to change, I am confident that I could do it. When it comes to deciding about a change, I feel overwhelmed by the choices. (R) I have trouble following through with things once I’ve made up my mind to do something. (R) I don’t seem to learn from my mistakes. (R) I’m usually careful not to overdo it when working, eating, drinking, or being on social media. I tend to compare myself with other people. I enjoy a routine, and like things to stay the same. (R) I have sought out advice or information about changing. Searching I can come up with lots of ways to change, but it’s hard for me to decide which one to use. (R) I can stick to a plan that’s working well. I usually only have to make a mistake one time in order to learn from it. I don’t learn well from punishment. (R) I have personal standards, and try to live up to them. I am set in my ways. (R) As soon as I see a problem or challenge, I start looking for possible solutions. I have a hard time setting goals for myself. (R) I have a lot of willpower. When I’m trying to change something, I pay a lot of attention to how I’m doing. I usually judge what I’m doing by the consequences of my actions. I don’t care if I’m different from most people. (R) As soon as I see things aren’t going right I want to do something about it. There is usually more than one way to accomplish something. I have trouble making plans to help me reach my goals. (R) I am able to resist temptation. I set goals for myself and keep track of my progress. Most of the time I don’t pay attention to what I’m doing. (R) I try to be like people around me. I tend to keep doing the same thing, even when it doesn’t work. (R) I can usually find several different possibilities when I want to change something. Once I have a goal, I can usually plan how to reach it. I have rules that I stick by no matter what. If I make a resolution to change something, I pay a lot of attention to how I’m doing. Often I don’t notice what I’m doing until someone calls it to my attention. (R) I think a lot about how I’m doing. Usually I see the need to change before others do. I’m good at finding different ways to get what I want. I usually think before I act. Little problems or distractions throw me off course. (R) I feel bad when I don’t meet my goals. I learn from my mistakes. I know how I want to be. It bothers me when things aren’t the way I want them. I call in others for help when I need it. Before making a decision, I consider what is likely to happen if I do one thing or another. I give up quickly. (R) I usually decide to change and hope for the best. (R) Scoring Directions: Score each response from 1 (strongly disagree) to 5 (strongly agree), and calculate the following subscale scores by summing the items on that subscale. Items marked (R) are reverse-coded (i.e. 1 = strongly agree and 5 = strongly disagree). Receiving relevant information: 1, 8, 15, 22, 29, 36, 43, 50, 57 Evaluating the information and comparing it to norms: 2, 9, 16, 23, 30, 37, 44, 51, 58 Triggering change: 3, 10, 17, 24, 31, 38, 45, 52, 59 Searching for options: 4, 11, 18, 25, 32, 39, 46, 53, 60 Formulating a plan: 5, 12, 19, 26, 33, 40, 47, 54, 61 Implementing the plan: 6, 13, 20, 27, 34, 41, 48, 55, 62 Assessing the plan’s effectiveness: 7, 14, 21, 28, 35, 42, 49, 56, 63 7.7 Metacognition Metacognivite Awareness Inventory (MAI) proposed by (Schraw &amp; Dennison, 1994) and revised by (Terlecki &amp; McMahon, 2018). Think of yourself as a learner. Read each statement carefully, and rate it as it generally applies to you when you are in the role of a learner (student, attending classes, university etc.) Please indicate how true each reason is for you using the following scale: | x0.18 | x0.18 | x0.18 | x0.18 | x0.18 | I NEVER do this &amp; I do this infrequently &amp; I do this inconsistently &amp; I do this frequently &amp; I ALWAYS do this 2 I ask myself periodically if I am meeting my goals. I consider several alternatives to a problem before I answer. I try to use strategies that have worked in the past. I pace myself while learning in order to have enough time. I understand my intellectual strengths and weaknesses. I think about what I really need to learn before I begin a task. I know how well I did once I finish a test. I set specific goals before I begin a task. I slow down when I encounter important information. I know what kind of information is most important to learn. I ask myself if I have considered all options when solving a problem. I am good at organizing information. I consciously focus my attention on important information. I have a specific purpose for each strategy I use. I learn best when I know something about the topic. I know what the teacher expects me to learn. I am good at remembering information. I use different learning strategies depending on the situation. I ask myself if there was an easier way to do things after I finish a task. I have control over how well I learn. I periodically review to help me understand important relationships. I ask myself questions about the material before I begin. I think of several ways to solve a problem and choose the best one. I summarize what I’ve learned after I finish. I ask others for help when I don’t understand something. I can motivate myself to learn when I need to. I am aware of what strategies I use when I study. I find myself analyzing the usefulness of strategies while I study. I use my intellectual strengths to compensate for my weaknesses. I focus on the meaning and significance of new information. I create my own examples to make information more meaningful. I am a good judge of how well I understand something. I find myself using helpful learning strategies automatically. I find myself pausing regularly to check my comprehension. I know when each strategy I use will be most effective. I ask myself how well I accomplish my goals once I’m finished. I draw pictures or diagrams to help me understand while learning. I ask myself if I have considered all options after I solve a problem. I try to translate new information into my own words. I change strategies when I fail to understand. I use the organizational structure of the text to help me learn. I read instructions carefully before I begin a task. I ask myself if what I’m reading is related to what I already know. I reevaluate my assumptions when I get confused. I organize my time to best accomplish my goals. I learn more when I am interested in the topic. I try to break studying down into smaller steps. I focus on overall meaning rather than specifics. I ask myself questions about how well I am doing while I am learning something new. I ask myself if I learned as much as I could have once I finish a task. I stop and go back over new information that is not clear. I stop and reread when I get confused. Scoring Directions: Score each response from 1 (never) to 5 (always), and calculate the following subscale scores by summing the items on that subscale. Knowledge about Cognition: Declarative Knowledge: 5, 10, 12, 16, 17, 20, 32, 46 (score out of \\(8\\times5 = 40\\)) Procedural Knowledge: 3, 14, 27, 33 (score out of \\(4\\times5 = 20\\)) Conditional Knowledge: 15, 18, 26, 29, 35 (score out of \\(5\\times5 = 25\\)) Regulation of Cognition: Planning: 4, 6, 8, 22, 23, 42, 45 (score out of \\(7\\times5 = 35\\)) Information Management Strategies: 9, 13, 30, 31, 37, 39, 41, 43, 47, 48 (score out of \\(10\\times5 = 50\\)) Comprehension Monitoring: 1, 2, 11, 21, 28, 34, 49 (score out of \\(7\\times5 = 35\\)) Debugging Strategies: 25, 40, 44, 51, 52 (score out of \\(5\\times5 = 25\\)) Evaluation: 7, 19, 24, 36, 38, 50 (score out of \\(6\\times5 = 30\\)) References Brennan, K., Kelly, D., &amp; Zhang, Y. (2016). Factor analysis of a search self-efficacy scale. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 241–244. Brown, J. M., Miller, W. R., &amp; Lawendowski, L. A. (1999). The self-regulation questionnaire. In V. L. &amp; J. T. L. (Eds.), Innovations in clinical practice: A sourcebook (Vol. 17, pp. 281–292). Professional Resource Press/Professional Resource Exchange. Penn State Learning. (2021). Listening and note taking survey | penn state learning. https://pennstatelearning.psu.edu/listening-and-note-taking-survey; Pennsylvania State University. Ryan, R. M. (1982). Control and information in the intrapersonal sphere: An extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43(3), 450. Schraw, G., &amp; Dennison, R. S. (1994). Assessing Metacognitive Awareness. Contemporary Educational Psychology, 19(4), 460–475. https://doi.org/10.1006/ceps.1994.1033 Terlecki, M., &amp; McMahon, A. (2018). A Call for Metacognitive Intervention: Improvements Due to Curricular Programming in Leadership. Journal of Leadership Education, 17(4), 130–145. https://doi.org/10.12806/V17/I4/R8 UMass Amherst Student Success. (2021). Note taking strategies inventory | success@UMass. https://www.umass.edu/studentsuccess/sites/default/files/inline-files/note-taking-strategies_0.pdf. Van Der Vaart, R., &amp; Drossaert, C. (2017). Development of the digital health literacy instrument: Measuring a broad spectrum of health 1.0 and health 2.0 skills. Journal of Medical Internet Research, 19(1), e27. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ "],["appendix:pre_post_tasks.html", "8 Questionnaires for Initial (SES1) and Final (SES3) Sessions 8.1 Pre-Task Questionnaire (for SES1 and SES3) 8.2 Post-Task Questionnaire (for SES1 and SES3) 8.3 Preference for CTA vs Silent Condition 8.4 Semi-structured Interview Questions (at the end of SES3)", " 8 Questionnaires for Initial (SES1) and Final (SES3) Sessions Pre-Test session (SES1) is conducted at the beginning of the semester, and the Post-Test session (SES3) is conducted at the end of the semester. 8.1 Pre-Task Questionnaire (for SES1 and SES3) The following items are adapted from (Collins-Thompson et al., 2016). How much do you know about this topic? (1) nothing | I know a lot (5) How interested are you to learn more about this topic? (1) not at all | very much (5) How difficult do you think it will be to search for information about this topic? (1) very easy | very difficult (5) The following items are adapted from (Crescenzi, 2020). Indicate your agreement with the following statements. (1) Strongly Disagree | Neutral | Strongly Agree (5) I am interested to learn more about the topic of this task. I know a lot about this topic. I can write a good summary now without needing to look for information. It will be difficult to determine when I have enough information to write my summary. I think this will be a difficult task. I am confident I know (or can find) adequate information to write a good summary. 8.2 Post-Task Questionnaire (for SES1 and SES3) The following items are adapted from (Collins-Thompson et al., 2016). Indicate your agreement with the following statements. (1) Not at all | Unlikely | Somewhat | Likely | Very Likely (5) Search for information exploration: I was cognitively engaged in search task. I made an effort at performing the search task. The time for search was spent productively on meaningful tasks. I was able to explore relationships among multiple concepts. I was able to expand the scope of my knowledge about the topic. I feel that I was able to put together pieces of information into one big concept. Learner interest and motivation: I feel that I have full understanding of the topic of this task I became more interested in this topic. I would like to find more information about this topic I would like to share what I learned with my people I know. I feel that I learned useful information as a result of this search. I was able to develop new ideas or perspectives. Perceived learning and search success: On a scale of 0 - 100 How would you grade your learning outcome? How would you grade your search outcome?   The following items are adapted from (Crescenzi, 2020). Indicate your agreement with the following statements. (1) Strongly Disagree | Neutral | Strongly Agree (5) Overall, it was difficult to search for information to make the summary. It was difficult to determine search terms to use to find relevant information. It was difficult to decide whether to continue inspecting the search results or to search again. It was difficult to choose which search results to view. It was difficult to determine when to stop looking for information. I would have preferred to think longer about my summary. If I had more time, I would have considered more information. I felt anxious while completing this task. I did not have enough time. It was difficult to decide which sources to select. I felt hurried or rushed during this task. I had adequate information to make a good summary. I felt I had enough information. My understanding of the topic was no longer changing. I collected enough information to make a summary. I was no longer learning about the topic. I felt I had adequate information to make a summary. I was focused on getting information about one thing. I felt continuing the search was a waste of time, as the same information was showing up. I had a list of certain things I was interested in. I stopped searching because I was not finding new information. I stopped searching when I had an option that satisfied the things that were important to me. I only considered looking for the piece of information most important to me. I kept finding the same information in every search. My view of the topic was no longer changing. I was most concerned about finding information on one specific aspect. 8.3 Preference for CTA vs Silent Condition You were asked to talk-aloud for one task, and work in silence for the other. Which one was better? Talk aloud much was a lot better Talk aloud was slightly better I did not feel any difference Working in silence was slightly better Working in silence was a lot better Why? 8.4 Semi-structured Interview Questions (at the end of SES3) What role did searching the web for information play in the course you just completed? How about other courses? How do you take notes, and manage your notes? PROBE: Do you have some strategies to organise information? Over the course of the semester, do you think your information search behaviour and strategies changed? Or remained the same? I understand that Firefox may not be your usual browser. How was your experience of using Firefox and YASBIL to complete the assignments? Did you feel restricted while using Firefox? Did you ever need to switch back to your old browser while completing the assignments? How did you feel about using YASBIL to record your browsing activity? Did you feel ever “under surveillance” while using YASBIL? If you were asked to redesign the way students interact with Google or any other Search Engine, what would you do? (PROBE: think about the tasks you had to do for your course, or in your daily life, or perhaps this study; redesign to better support student education) Imagine a scenario where search engines can measure how much information students already know about a certain topic. Do you think it will be useful or desirable? (if yes to previous question): What are some ways by which a search engine could possibly assess a student’s existing knowledge? References Collins-Thompson, K., Rieh, S. Y., Haynes, C. C., &amp; Syed, R. (2016). Assessing learning outcomes in web search: A comparison of tasks and query strategies. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 163–172. Crescenzi, A. M. C. (2020). Adaptation in Information Search and Decision-Making under Time Pressure [PhD thesis, The University of North Carolina at Chapel Hill University Libraries]. https://doi.org/10.17615/YT6K-AC37 "],["app_midterm_survey.html", "9 SUR2: Midterm Survey", " 9 SUR2: Midterm Survey Overall Instructions: You may have seen these questions before. We ask you to carefully consider the questions again, reflect back on your activities and experiences since the beginning of the semester, and answer the questions accordingly. We all grow and evolve with time. These questions will help us to understand how you have evolved over the past few weeks.   Reworded Version of Intrinsic Motivation Inventory: (slightly different from SUR1 to indicate midpoint of the semester) For each of the following statements, please indicate how true it is for you, using the following scale: (1) not at all true | somewhat true | very true (5) Interest/Enjoyment I am enjoying doing the final project activities very much The final project activities are fun to do. I think this is a boring course. (R) The course and the final project activities are not holding my attention at all. (R) I would describe the final project activities as very interesting. I think the final project activities were quite enjoyable. While I was doing the final project activities, I was thinking about how much I was enjoying them.   Perceived Competence I think I am pretty good at the final project activities. I think I am doing pretty well in the final project activities, compared to other students. After working at the final project activities for awhile, I am feeling pretty competent. I am satisfied with my performance in this course. I am pretty skilled at the final project activities. This is a course that I am not able to do very well. (R)   Effort/Importance I am putting a lot of effort into this course. I am not trying very hard to do well at the final project activities. (R) I am trying very hard on the final project activities. It is important to me to do well in this course. I am not putting much energy into the final project activities. (R)   Pressure/Tension I was not feeling nervous at all while doing the final project activities. (R) I was feeling very tensed while doing the final project activities. I was very relaxed while doing the final project activities. (R) I was anxious while working on the final project parts. I felt pressured while doing the final project activities.   Perceived Choice I believe I have some choice about doing the final project activities. I feel like it is not my own choice to do the final project parts. (R) I didn’t really have a choice about doing the final project tasks. (R) I feel like I have to do the final project tasks. (R) I did the final project activities because I had no choice. (R) I did the final project activities because I wanted to. I did the final project activities because I had to. (R)   Value/Usefulness 16 I believe the course and the final project activities could be of some value to me. I think that doing the final project activities is useful for I think this is important to do because it can I would be willing to do research on the final project topic again because it has some value to me. I think doing the final project activities will help me to I believe doing the final project activities will be beneficial to me. I think this is an important course. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ "],["app_final_survey.html", "10 SUR3: Exit Survey", " 10 SUR3: Exit Survey Overall Instructions: You may have seen these questions before. We ask you to carefully consider the questions again, reflect back on your activities and experiences since the beginning of the semester, and answer the questions accordingly. We all grow and evolve with time. These questions will help us to understand how you have evolved over the past few months.   Reworded Version of Intrinsic Motivation Inventory: (Slightly different from SUR2 to indicate end of the semester) For each of the following statements, please indicate how true it is for you, using the following scale: (1) not at all true | somewhat true | very true (5) Interest/Enjoyment I enjoyed taking the course and doing the final project activities very much. The course and final project activities were fun to do. I think this was a boring course. (R) The course and the final project activities were not holding my attention at all. (R) I would describe the final project activities as very interesting. I think the final project activities were quite enjoyable. While I was doing the final project activities, I was thinking about how much I enjoyed them.   Perceived Competence I think I am pretty good at this course and the final project activities. I think I did pretty well in the course, and in the final project activities, compared to other students. After working at the final project activities for awhile, I felt pretty competent. I am satisfied with my performance in this course. I am pretty skilled at the final project activities. This is a course that I was not able to do very well. (R)   Effort/Importance I put a lot of effort into this course. I did not try very hard to do well in this course, and in the final project activities. (R) I tried very hard in the course, and in the final project activities. It was important to me to do well in this course. I did not put much energy into the course and the final project activities. (R)   Pressure/Tension I did not feel nervous at all while doing the final project activities. (R) I felt very tensed while doing the final project activities. I felt very relaxed while doing the final project activities. (R) I was anxious while working on the final project parts. I felt pressured while doing the final project activities.   Perceived Choice I believe I have some choice about doing the final project. I felt like it is not my own choice to do the final project parts. (R) I didn’t really have a choice about doing the final project tasks. (R) I felt like I had to do the final project tasks. (R) I did the final project activities because I had no choice. (R) I did the final project activities because I wanted to. I did the final project activities because I had to. (R)   Value/Usefulness 17 I believe the course and the final project activities could be of some value to me. I think that doing the final project activities is useful for I think this is important to do because it can I would be willing to do research on the final project topics/subtopics again because it has some value to me. I think doing the final project activities will help me to I believe doing the final project activities will be beneficial to me. I think this was an important course and final project for me. https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ "],["app_variables.html", "11 Variables and Measures 11.1 Query Reformulation Variables 11.2 SERP / Content Page Examination variables 11.3 SERP-only variables 11.4 Content page-only variables 11.5 Overall Search Behaviour 11.6 Webpage Transition Types 11.7 Concept Map Analysis Measures", " 11 Variables and Measures This Section contains a non-exhaustive list of possible variables and operationalizations of information search behaviour collated from recent literature. 11.1 Query Reformulation Variables #Queries: the number of issued queries in a session (Mao et al., 2018; Vakkari, 2016) Query Reformulation Type (QRT), and corresponding counts: automated: generalization, specialization, word substitution, repeat, new (C. Liu et al., 2010) partially manual: generalization, specialization, parallel move, mission change, error correction (Boldi et al., 2009) #Terms per q.: the number of terms in a query (Mao et al., 2018) #Unique terms: the number of unique terms in a session (Mao et al., 2018) #Unique terms per q.: (#Unique terms / #queries) , the number of unique terms per query (query vocabulary richness, QVR) (Mao et al., 2018) #Synonyms: number of synonyms in terms (Vakkari, 2016) %Terms from desc.: the ratio of terms from the task description / assignment description (Mao et al., 2018) %Terms from SERP: the proportion of novel query terms found in the text of SERPs (Mao et al., 2018) %Terms from content page: the proportion of novel query terms found in the text of content pages (Mao et al., 2018) %Others: the proportion of novel query terms from other sources (i.e., not webpage text) (Mao et al., 2018) Query type time: time taken to issue the query (may not be a signal; will be dependent on typing speed) (Downey et al., 2007 Table 3) has more linguistic features of query terms: web frequency, geo mean, max bigram, etc. of query terms 11.2 SERP / Content Page Examination variables Transition Type: how the browser navigated to a particular URL on a particular visit (Appendix 1.6) Dwell time: time spent on the page (seconds) Avg. page display time: average length of time for which a webpage is viewed during a session (White et al., 2009) #Clicks: number of clicks on page Scroll depth: how far down the page was scrolled (pixels, viewport proportion, % of total webpage length) Click depth: how far down in the page was a click made Scrolled Distance: total distance scrolled %Scroll Time: % of time spent in scrolling Scroll speed: indicative of reading vs scanning Scroll pattern: entropy / scroll chaosness #Scroll direction change: the number of times scrolling direction was changed — indicative of hesitancy Non-scroll time: % of time NOT spent in scrolling, which is indicative of reading #Visit-in-session: whether first visit or revisit, in this session #Visit-overall: whether first visit or revisit, in the whole data Number of unclicked hovers: Median number of times for which the query was issued and the URL is hovered on but not clicked, per the earlier definition. “We selected the number of unclicked hovers as a feature because we found that it was correlated with clickthrough in our previous analysis.” (J. Huang et al., 2011) Cursor trail length: Total distance (in pixels) travelled by the cursor on the SERP. (J. Huang et al., 2011) Movement time: Total time (in seconds) for which the cursor was being moved on the SERP. (J. Huang et al., 2011) Cursor speed: The average cursor speed (in pixels per second) as a function of trail length and movement time (J. Huang et al., 2011) All distances / depths can be measured using raw pixel values, viewport proportions and % of total webpage lengths 11.3 SERP-only variables Rank: rank of result clicked Avg. SERP dwell time per query: Total SERP Dwell Time / #queries entered (Collins-Thompson et al., 2016) Focus: “degree to which a SERP is covered by a single topic. Topics may be derived based on existing class hierarchies such as the Open Directory Project; Experts tend to explore more narrow topical spaces than non-experts” (Eickhoff et al., 2014; Rieh et al., 2016) Entropy: “captures diversity across multiple topics on search results pages. Experts typically have higher focus, less diversity, and thus, lower entropy across topics” (Rieh et al., 2016) 11.4 Content page-only variables Avg. Content dwell time per query: Total content page dwell time / #queries (Collins-Thompson et al., 2016) 11.5 Overall Search Behaviour Session length (time): total time spent in the session, from logging software turn on to turn-off (or last page-visit, if turn-off time is null) Session length (pages): no. of pages visited in session, including search engine home pages and result pages (White et al., 2009) Session length (queries): no. of queries issued in session (White et al., 2009) #Search engines: number of unique search engines used (will probably be one?) #Tabs: number of tabs opened during the session #Windows: number of windows opened during the session Branchiness: no. of revisits to previous pages in the session that were then followed by a forward motion to a previously unvisited page in the session(White et al., 2009) #Unqiue domains: number of unique websites visited during the session; diversity of websites / breadth of coverage(White et al., 2009) Ratio of querying to browsing: proportion of the session that is devoted to querying versus browsing pages retrieved by the search engine or linked to from search results. A high number (much greater than one) means that the session was query-intensive. In contrast, a low number (much less than one) means that the session was browse-intensive(White et al., 2009) #Bookmarks-add: no. of bookmarks added #Bookmarks-delete: no. of bookmarks deleted Search success: “If the final event in a search session was a URL click, we scored the session as a success, and if the final action was a query, we score the session as a failure.” (White et al., 2009) Last Page Visited: Where was the last click in a session? (Eickhoff et al., 2014) treated sessions ending at ehow.com as procedural search sessions (users visiting tutorial articles), and sessions ending at Wikipedia as declarative search sessions. Search visualization: as described by (Bateman et al., 2012) 11.6 Webpage Transition Types In browser history API parlance, a transition type is used to describe how the browser navigated to a particular URL on a particular visit. E.g., if a user visits a page by clicking a link on another page, the transition type is “link”. The following definitions are taken from Google Chrome Developer Website18. link: The user got to this page by clicking a link on another page. typed: The user got this page by typing the URL in the address bar. Also used for other explicit navigation actions. See also generated, which is used for cases where the user selected a choice that didn’t look at all like a URL. auto_bookmark: The user got to this page through a suggestion in the UI—for example, through a menu item. auto_subframe: Subframe navigation. This is any content that is automatically loaded in a non-top-level frame. For example, if a page consists of several frames containing ads, those ad URLs have this transition type. The user may not even realize the content in these pages is a separate frame, and so may not care about the URL (see also manual_subframe). manual_subframe: For subframe navigations that are explicitly requested by the user and generate new navigation entries in the back/forward list. An explicitly requested frame is probably more important than an automatically loaded frame because the user probably cares about the fact that the requested frame was loaded. generated: The user got to this page by typing in the address bar and selecting an entry that did not look like a URL. For example, a match might have the URL of a Google search result page, but it might appear to the user as \"Search Google for ...\". These are not quite the same as typed navigations because the user didn’t type or see the destination URL. See also keyword. auto_toplevel: The page was specified in the command line or is the start page. form_submit: The user filled out values in a form and submitted it. Note that in some situations—such as when a form uses script to submit contents—submitting a form does not result in this transition type. reload: The user reloaded the page, either by clicking the reload button or by pressing Enter in the address bar. Session restore and Reopen closed tab use this transition type, too. keyword: The URL was generated from a replaceable keyword other than the default search provider. See also keyword_generated. keyword_generated: Corresponds to a visit generated for a keyword. See also keyword. 11.7 Concept Map Analysis Measures The following concept map analysis measures are adapted from (Halttunen &amp; Jarvelin, 2005). The number of concepts in the beginning of instruction (beginning), The number of new concepts presented in second essay (new), The number of concepts in the end of instruction (end), The difference between the number of concepts in the beginning and end of instruction (difference), The number of concepts which remained same along the time (stable), The number of concepts that were ignored or changed fundamentally along the instruction (changed/ignored), The number of top-level concepts in the end (top level), The number of new top-level concepts presented in the second essay (new top level), The number of links between concept hierarchies in the end (links), Maximum depth of hierarchy levels in the end, The number of concepts per different levels of hierarchy in the end, The number of concepts per top-level concepts i.e. hierarchies in the end, and The level where new concepts were introduced. References Bateman, S., Teevan, J., &amp; White, R. (2012). The search dashboard: How reflection and comparison impact search behavior. Proceedings of the 2012 ACM Annual Conference on Human Factors in Computing Systems - CHI ’12, 1785. https://doi.org/10.1145/2207676.2208311 Boldi, P., Bonchi, F., Castillo, C., &amp; Vigna, S. (2009). From\" dango\" to\" japanese cakes\": Query reformulation models and patterns. 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, 1, 183–190. Collins-Thompson, K., Rieh, S. Y., Haynes, C. C., &amp; Syed, R. (2016). Assessing learning outcomes in web search: A comparison of tasks and query strategies. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 163–172. Downey, D., Dumais, S. T., &amp; Horvitz, E. (2007). Models of searching and browsing: Languages, studies, and application. IJCAI, 7, 2740–2747. Eickhoff, C., Teevan, J., White, R., &amp; Dumais, S. (2014). Lessons from the journey: A query log analysis of within-session learning. Proceedings of the 7th ACM International Conference on Web Search and Data Mining, 223–232. Halttunen, K., &amp; Jarvelin, K. (2005). Assessing learning outcomes in two information retrieval learning environments. Information Processing &amp; Management, 41(4), 949–972. https://doi.org/10.1016/j.ipm.2004.02.004 Huang, J., White, R., &amp; Dumais, S. (2011). No clicks, no problem: Using cursor movements to understand and improve search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1225–1234. Liu, C., Gwizdka, J., Liu, J., Xu, T., &amp; Belkin, N. J. (2010). Analysis and evaluation of query reformulations in different task types. Proceedings of the American Society for Information Science and Technology, 47(1), 1–9. Mao, J., Liu, Y., Kando, N., Zhang, M., &amp; Ma, S. (2018). How does domain expertise affect users’ search interaction and outcome in exploratory search? ACM Transactions on Information Systems, 36. Rieh, S. Y., Collins-Thompson, K., Hansen, P., &amp; Lee, H.-J. (2016). Towards searching as a learning process: A review of current perspectives and future directions. Journal of Information Science, 42(1), 19–34. https://doi.org/10.1177/0165551515615841 Vakkari, P. (2016). Searching as learning: A systematization based on literature. Journal of Information Science, 42(1), 7–18. https://doi.org/10.1177/0165551515615833 White, R., Dumais, S., &amp; Teevan, J. (2009). Characterizing the influence of domain expertise on web search behavior. Proceedings of the Second ACM International Conference on Web Search and Data Mining - WSDM ’09, 132. https://doi.org/10.1145/1498759.1498819 https://developer.chrome.com/docs/extensions/reference/history/#transition-types↩︎ "],["the-first-appendix.html", "A The First Appendix", " A The First Appendix This first appendix includes an R chunk that was hidden in the document (using echo = FALSE) to help with readibility: In 02-rmd-basics-code.Rmd And here’s another one from the same chapter, i.e. Chapter ??: "],["the-second-appendix-for-fun.html", "B The Second Appendix, for Fun", " B The Second Appendix, for Fun "],["references.html", "References", " References Abualsaud, M., &amp; Smucker, M. D. (2019). Patterns of search result examination: Query to first action. Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 1833–1842. https://doi.org/10.1145/3357384.3358041 Agosti, M., Fuhr, N., Toms, E., &amp; Vakkari, P. (2014). Evaluation methodologies in information retrieval dagstuhl seminar 13441. ACM SIGIR Forum, 48, 36–41. Allan, J., Croft, B., Moffat, A., &amp; Sanderson, M. (2012). Frontiers, challenges, and opportunities for information retrieval: Report from SWIRL 2012 the second strategic workshop on information retrieval in lorne. ACM SIGIR Forum, 46, 2–32. Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C., &amp; Norman, M. K. (2010). How Learning Works: Seven Research-Based Principles for Smart Teaching. John Wiley &amp; Sons. Amina, T. (2017). Active knowledge making: Epistemic dimensions of e-learning. In E-learning ecologies (pp. 65–87). Routledge. Arguello, J., &amp; Choi, B. (2019). The effects of working memory, perceptual speed, and inhibition in aggregated search. ACM Transactions on Information Systems, 37(3). https://doi.org/10.1145/3322128 Aula, A., Majaranta, P., &amp; Räihä, K.-J. (2005). Eye-tracking reveals the personal styles for search result evaluation. In M. F. Costabile &amp; F. Paternò (Eds.), Human-computer interaction - INTERACT 2005 (pp. 1058–1061). Springer Berlin Heidelberg. Ausubel, D. P. (2012). The acquisition and retention of knowledge: A cognitive view. Springer Science &amp; Business Media. Ausubel, D. P., Novak, J. D., Hanesian, H., et al. (1968). Educational psychology: A cognitive view (Vol. 6). Holt, Rinehart; Winston New York. Bailey, E., &amp; Kelly, D. (2011). Is amount of effort a better predictor of search success than use of specific search tactics? Proceedings of the American Society for Information Science and Technology, 48(1), 1–10. Balatsoukas, P., &amp; Ruthven, I. (2010). The use of relevance criteria during predictive judgment: An eye tracking approach. Proceedings of the American Society for Information Science and Technology, 47(1), 1–10. https://doi.org/10.1002/meet.14504701145 Balatsoukas, P., &amp; Ruthven, I. (2012). An eye-tracking approach to the analysis of relevance judgments on the Web: The case of Google search engine. Journal of the American Society for Information Science and Technology, 63(9), 1728–1746. https://doi.org/10.1002/asi.22707 Bateman, S., Teevan, J., &amp; White, R. (2012). The search dashboard: How reflection and comparison impact search behavior. Proceedings of the 2012 ACM Annual Conference on Human Factors in Computing Systems - CHI ’12, 1785. https://doi.org/10.1145/2207676.2208311 Belkin, N. J., Oddy, R. N., &amp; Brooks, H. M. (1982). ASK for information retrieval: Part i. Background and theory. Journal of Documentation. Beymer, D., Orton, P. Z., &amp; Russell, D. M. (2007). An eye tracking study of how pictures influence online reading. IFIP Conference on Human-Computer Interaction, 456–460. Bhattacharya, N. (2021). A longitudinal study to understand learning during search. Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, 363–366. Bhattacharya, N., &amp; Gwizdka, J. (2018). Relating eye-tracking measures with changes in knowledge on search tasks. Symposium on Eye Tracking Research &amp; Applications (ETRA). Bhattacharya, N., &amp; Gwizdka, J. (2019b). Measuring learning during search: Differences in interactions, eye-gaze, and semantic similarity to expert knowledge. Proceedings of the 2019 Conference on Human Information Interaction and Retrieval, 63–71. Bhattacharya, N., &amp; Gwizdka, J. (2019a). Measuring learning during search: Differences in interactions, eye-gaze, and semantic similarity to expert knowledge. CHIIR’19. Bhattacharya, N., &amp; Gwizdka, J. (2021). YASBIL: Yet another search behaviour (and) interaction logger. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2585–2589. Bilal, D., &amp; Gwizdka, J. (2016). Children’s Eye-fixations on Google Search Results. Proceedings of the 79th ASIS&amp;T Annual Meeting, 79, 89:1–89:6. https://doi.org/10.1002/pra2.2016.14505301089 Blanken-Webb, J. (2017). Metacognition: Cognitive dimensions of e-learning. In E-learning ecologies (pp. 163–182). Routledge. Boldi, P., Bonchi, F., Castillo, C., &amp; Vigna, S. (2009). From\" dango\" to\" japanese cakes\": Query reformulation models and patterns. 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, 1, 183–190. Borlund, P. (2013). Interactive Information Retrieval: An Introduction. Journal of Information Science Theory and Practice, 1(3), 12–32. https://doi.org/10.1633/JISTAP.2013.1.3.2 Breakstone, J., McGrew, S., Smith, M., Ortega, T., &amp; Wineburg, S. (2018). Why we need a new approach to teaching digital literacy. Phi Delta Kappan, 99(6), 27–32. Breakstone, J., Smith, M., Wineburg, S., Rapaport, A., Carle, J., Garland, M., &amp; Saavedra, A. (2021). Students’ Civic Online Reasoning: A National Portrait. Educational Researcher. https://doi.org/10.3102/0013189X211017495 Brennan, K., Kelly, D., &amp; Zhang, Y. (2016). Factor analysis of a search self-efficacy scale. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 241–244. Broder, A. (2002). A taxonomy of web search. SIGIR Forum, 36(2), 3–10. https://doi.org/10.1145/792550.792552 Brookes, B. C. (1980). The foundations of information science. Part i. Philosophical aspects. Journal of Information Science, 2(3-4), 125–133. Brown, J. (1998). Self-regulation and the addictive behaviours. New York: Plenum Press. Brown, J. M., Miller, W. R., &amp; Lawendowski, L. A. (1999). The self-regulation questionnaire. In V. L. &amp; J. T. L. (Eds.), Innovations in clinical practice: A sourcebook (Vol. 17, pp. 281–292). Professional Resource Press/Professional Resource Exchange. Buscher, G., Cutrell, E., &amp; Morris, M. R. (2009). What Do You See When You’re Surfing? Using Eye Tracking to Predict Salient Regions of Web Pages. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 10. Buscher, G., Dumais, S. T., &amp; Cutrell, E. (2010). The good, the bad, and the random: An eye-tracking study of ad quality in web search. Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 42–49. https://doi.org/10.1145/1835449.1835459 Chen, Y., Zhao, Y., &amp; Wang, Z. (2020). Understanding online health information consumers’ search as a learning process. Library Hi Tech. Cherry, K. (2020). What Is Motivation? In Verywell Mind. https://www.verywellmind.com/what-is-motivation-2795378 Cole, L., MacFarlane, A., &amp; Makri, S. (2020). More than words: The impact of memory on how undergraduates with dyslexia interact with information. Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, 353–357. https://doi.org/10.1145/3343413.3378005 Cole, M. J., Gwizdka, J., Liu, C., Belkin, N. J., &amp; Zhang, X. (2013). Inferring user knowledge level from eye movement patterns. Information Processing &amp; Management, 49(5), 1075–1091. Collins, C. (2021). Reimagining Digital Literacy Education to Save Ourselves. Learning for Justice, Fall 2021. https://www.learningforjustice.org/magazine/fall-2021/reimagining-digital-literacy-education-to-save-ourselves Collins-Thompson, K., Hansen, P., &amp; Hauff, C. (2017). Search as learning (dagstuhl seminar 17092). Dagstuhl Reports, 7. Collins-Thompson, K., Rieh, S. Y., Haynes, C. C., &amp; Syed, R. (2016). Assessing learning outcomes in web search: A comparison of tasks and query strategies. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 163–172. Cope, B., &amp; Kalantzis, M. (2017). E-Learning Ecologies: Principles for New Learning and Assessment. Taylor &amp; Francis. Cope, B., &amp; Kalantzis, M. (2013). Towards a New Learning: The Scholar Social Knowledge Workspace, in Theory and Practice. E-Learning and Digital Media, 10(4), 332–356. https://doi.org/10.2304/elea.2013.10.4.332 Crescenzi, A. M. C. (2020). Adaptation in Information Search and Decision-Making under Time Pressure [PhD thesis, The University of North Carolina at Chapel Hill University Libraries]. https://doi.org/10.17615/YT6K-AC37 Cutrell, E., &amp; Guan, Z. (2007). What are you looking for? An eye-tracking study of information usage in web search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 407–416. https://doi.org/10.1145/1240624.1240690 Deci, E. L., &amp; Ryan, R. M. (2013). Intrinsic motivation and self-determination in human behavior. Springer Science &amp; Business Media. Dervin, B., &amp; Naumer, C. M. (2010). Sense-making. In M. J. Bates &amp; M. M. N. (Eds.), Encyclopedia of library and information sciences (3rd ed.) (pp. 4696--4707). Taylor; Francis. Desimone, R., &amp; Duncan, J. (1995). Neural mechanisms of selective visual attention. Annual Review of Neuroscience, 18(1), 193–222. Diamond, A. (2013). Executive functions. Annual Review of Psychology, 64, 135–168. DiCerbo, K. E., &amp; Behrens, J. T. (2014). Impacts of the digital ocean on education. London: Pearson, 1. Djamasbi, S., Hall-Phillips, A., &amp; Yang, R. (Rachel). (2013). Search Results Pages and Competition for Attention Theory: An Exploratory Eye-Tracking Study. In S. Yamamoto (Ed.), Human Interface and the Management of Information. Information and Interaction Design (pp. 576–583). Springer Berlin Heidelberg. http://link.springer.com.ezproxy.lib.utexas.edu/chapter/10.1007/978-3-642-39209-2_64 Downey, D., Dumais, S. T., &amp; Horvitz, E. (2007). Models of searching and browsing: Languages, studies, and application. IJCAI, 7, 2740–2747. Dumais, S. T., Buscher, G., &amp; Cutrell, E. (2010). Individual differences in gaze patterns for web search. Proceedings of the Third Symposium on Information Interaction in Context, 185–194. https://doi.org/10.1145/1840784.1840812 Egusa, Y., Saito, H., Takaku, M., Terai, H., Miwa, M., &amp; Kando, N. (2010). Using a Concept Map to Evaluate Exploratory Search. Proceedings of the Third Symposium on Information Interaction in Context, 175–184. https://doi.org/10.1145/1840784.1840810 Egusa, Y., Takaku, M., &amp; Saito, H. (2014a). How Concept Maps Change if a User Does Search or Not? Proceedings of the 5th Information Interaction in Context Symposium, 68–75. https://doi.org/10.1145/2637002.2637012 Egusa, Y., Takaku, M., &amp; Saito, H. (2014b). How to evaluate searching as learning. Searching as Learning Workshop (IIiX 2014 Workshop). http://www.diigubc.ca/IIIXSAL/program.html Egusa, Y., Takaku, M., &amp; Saito, H. (2017). Evaluating Complex Interactive Searches Using Concept Maps. SCST@ CHIIR, 15–17. Eickhoff, C., Dungs, S., &amp; Tran, V. (2015). An eye-tracking study of query reformulation. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 13–22. https://doi.org/10.1145/2766462.2767703 Eickhoff, C., Gwizdka, J., Hauff, C., &amp; He, J. (2017). Introduction to the special issue on search as learning. Information Retrieval Journal, 20(5), 399–402. Eickhoff, C., Teevan, J., White, R., &amp; Dumais, S. (2014). Lessons from the journey: A query log analysis of within-session learning. Proceedings of the 7th ACM International Conference on Web Search and Data Mining, 223–232. Francis, G., MacKewn, A., &amp; Goldthwaite, D. (2004). CogLab on a CD. Wadsworth Publishing Company. Freund, L., Gwizdka, J., Hansen, P., Kando, N., &amp; Rieh, S. Y. (2013). From searching to learning. Evaluation Methodologies in Information Retrieval. Dagstuhl Reports, 13441, 102–105. Freund, L., He, J., Gwizdka, J., Kando, N., Hansen, P., &amp; Rieh, S. Y. (2014). Searching as learning (SAL) workshop 2014. Proceedings of the 5th Information Interaction in Context Symposium, 7–7. Gadiraju, U., Yu, R., Dietze, S., &amp; Holtz, P. (2018). Analyzing knowledge gain of users in informational search sessions on the web. Conference on Human Information Interaction &amp; Retrieval (CHIIR). Ghosh, S., Rath, M., &amp; Shah, C. (2018). Searching as learning: Exploring search behavior and learning outcomes in learning-related tasks. Conference on Human Information Interaction &amp; Retrieval (CHIIR). Goldberg, J. H., Stimson, M. J., Lewenstein, M., Scott, N., &amp; Wichansky, A. M. (2002). Eye tracking in web search tasks: Design implications. Proceedings of the 2002 Symposium on Eye Tracking Research &amp; Applications, 51–58. González-Ibáñez, R., Esparza-Villamán, A., Vargas-Godoy, J. C., &amp; Shah, C. (2019). A comparison of unimodal and multimodal models for implicit detection of relevance in interactive IR. Journal of the Association for Information Science and Technology, 0(0). https://doi.org/10.1002/asi.24202 Gossen, T., Höbel, J., &amp; Nürnberger, A. (2014). A comparative study about children’s and adults’ perception of targeted web search engines. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1821–1824. https://doi.org/10.1145/2556288.2557031 Grabowski, B. L. (1996). Generative learning: Past, present, and future. Handbook of Research for Educational Communications and Technology, 897–918. Granka, L. A., Joachims, T., &amp; Gay, G. (2004). Eye-tracking analysis of user behavior in WWW search. Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 478–479. https://doi.org/10.1145/1008992.1009079 Groner, R., Walder, F., &amp; Groner, M. (1984). Looking at faces: Local and global aspects of scanpaths. In Advances in psychology (Vol. 22, pp. 523–533). Elsevier. Guan, Z., &amp; Cutrell, E. (2007). An eye tracking study of the effect of target rank on web search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 417–420. https://doi.org/10.1145/1240624.1240691 Guyan, M. (2013). Improving Learner Motivation for eLearning. In Learning Snippets. https://learningsnippets.wordpress.com/2013/10/30/improving-learner-motivation-for-elearning/ Gwizdka, J. (2013). Effects of working memory capacity on users’ search effort. Proceedings of the International Conference on Multimedia, Interaction, Design and Innovation, 11:1–11:8. https://doi.org/10.1145/2500342.2500358 Gwizdka, J. (2014). Characterizing Relevance with Eye-tracking Measures. Proceedings of the 5th Information Interaction in Context Symposium, 58–67. https://doi.org/10.1145/2637002.2637011 Gwizdka, J. (2017). I Can and So I Search More: Effects Of Memory Span On Search Behavior. Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, 341–344. https://doi.org/10.1145/3020165.3022148 Gwizdka, J. (2018). Inferring Web Page Relevance Using Pupillometry and Single Channel EEG. In F. D. Davis, R. Riedl, J. vom Brocke, P.-M. Léger, &amp; A. B. Randolph (Eds.), Information Systems and Neuroscience (pp. 175–183). Springer International Publishing. https://doi.org/10.1007/978-3-319-67431-5_20 Gwizdka, J., &amp; Bilal, D. (2017). Analysis of Children’s Queries and Click Behavior on Ranked Results and Their Thought Processes in Google Search. Proceedings of the 2017 Conference on Conference Human Information Interaction and Retrieval, 377–380. https://doi.org/10.1145/3020165.3022157 Gwizdka, J., Hansen, P., Hauff, C., He, J., &amp; Kando, N. (2016). Search as learning (SAL) workshop 2016. Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1249–1250. Gwizdka, J., &amp; Zhang, Y. (2015a). Towards Inferring Web Page Relevance An Eye-Tracking Study. Proceedings of iConference’2015, 5. https://www.ideals.illinois.edu/handle/2142/73709 Gwizdka, J., &amp; Zhang, Y. (2015b). Differences in Eye-Tracking Measures Between Visits and Revisits to Relevant and Irrelevant Web Pages. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 811–814. https://doi.org/10.1145/2766462.2767795 Halttunen, K., &amp; Jarvelin, K. (2005). Assessing learning outcomes in two information retrieval learning environments. Information Processing &amp; Management, 41(4), 949–972. https://doi.org/10.1016/j.ipm.2004.02.004 Hansen, P., &amp; Rieh, S. Y. (2016). Editorial: Recent advances on searching as learning: An introduction to the special issue. Journal of Information Science, 42(1), 3–6. https://doi.org/10.1177/0165551515614473 Hofmann, K., Mitra, B., Radlinski, F., &amp; Shokouhi, M. (2014). An eye-tracking study of user interactions with query auto completion. Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, 549–558. https://doi.org/10.1145/2661829.2661922 Huang, J., &amp; White, R. (2010). Parallel browsing behavior on the web. Proceedings of the 21st ACM Conference on Hypertext and Hypermedia - HT ’10, 13. https://doi.org/10.1145/1810617.1810622 Huang, J., White, R., &amp; Dumais, S. (2011). No clicks, no problem: Using cursor movements to understand and improve search. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1225–1234. Huang, X., &amp; Soergel, D. (2013). Relevance: An improved framework for explicating the notion. Journal of the American Society for Information Science and Technology, 64(1), 18–35. https://doi.org/10.1002/asi.22811 Jiang, J., He, D., &amp; Allan, J. (2014). Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 607–616. https://doi.org/10.1145/2600428.2609633 Josephson, S., &amp; Holmes, M. E. (2002). Visual attention to repeated internet images: Testing the scanpath theory on the world wide web. Proceedings of the 2002 Symposium on Eye Tracking Research &amp; Applications, 43–49. Jossberger, H., Brand-Gruwel, S., Boshuizen, H., &amp; Van de Wiel, M. (2010). The challenge of self-directed and self-regulated learning in vocational education: A theoretical analysis and synthesis of requirements. Journal of Vocational Education and Training, 62(4), 415–440. Kahne, J., Lee, N.-J., &amp; Feezell, J. T. (2012). Digital media literacy education and online civic and political participation. International Journal of Communication, 6, 24. Kalantzis, M., &amp; Cope, B. (2012). New Learning: Elements of a Science of Education. Cambridge University Press. Kanfer, F. H. (1970a). Self-monitoring: Methodological limitations and clinical applications. Kanfer, F. H. (1970b). Self-regulation: Research, issues, and speculations. Behavior Modification in Clinical Psychology, 74, 178–220. Kanniainen, L., Kiili, C., Tolvanen, A., Aro, M., Anmarkrud, Ø., &amp; Leppänen, P. H. T. (2021). Assessing reading and online research comprehension: Do difficulties in attention and executive function matter? Learning and Individual Differences, 87, 101985. https://doi.org/10.1016/j.lindif.2021.101985 Karapanos, E., Gerken, J., Kjeldskov, J., &amp; Skov, M. B. (Eds.). (2021). Advances in Longitudinal HCI Research. Springer International Publishing. https://doi.org/10.1007/978-3-030-67322-2 Kelly, D. (2006a). Measuring online information seeking context, Part 1: Background and method. Journal of the American Society for Information Science and Technology, 57(13), 1729–1739. https://doi.org/10.1002/asi.20483 Kelly, D. (2006b). Measuring online information seeking context, Part 2: Findings and discussion. Journal of the American Society for Information Science and Technology, 57(14), 1862–1874. https://doi.org/10.1002/asi.20484 Kelly, D. (2009). Methods for evaluating interactive information retrieval systems with users. Foundations and Trends in Information Retrieval, 3(1—2), 1–224. Kelly, D., Dumais, S., &amp; Pedersen, J. O. (2009). Evaluation challenges and directions for information-seeking support systems. IEEE Computer, 42(3). Knowles, M. S. (1975). Self-directed learning: A guide for learners and teachers. New York: Association press. Ko, A. J. (2021). Seeking information. In Foundations of Information. https://faculty.washington.edu/ajko/books/foundations-of-information/#/seeking Koeman, L. (2020). HCI/UX research: What methods do we use? – lisa koeman – blog. https://lisakoeman.nl/blog/hci-ux-research-what-methods-do-we-use/. Kruikemeier, S., Lecheler, S., &amp; Boyer, M. M. (2018). Learning from news on different media platforms: An eye-tracking experiment. Political Communication, 35(1), 75–96. Kuhlthau, C. C. (2004). Seeking meaning: A process approach to library and information services (Vol. 2). Libraries Unlimited Westport, CT. Labaj, M., &amp; Bielikova, M. (2012). Modeling parallel web browsing behavior for web-based educational systems. 2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA), 229–234. https://doi.org/10.1109/ICETA.2012.6418330 Lei, P.-L., Sun, C.-T., Lin, S. S., &amp; Huang, T.-K. (2015). Effect of metacognitive strategies and verbal-imagery cognitive style on biology-based video search and learning performance. Computers &amp; Education, 87, 326–339. Leu, D. J., Forzani, E., Rhoads, C., Maykel, C., Kennedy, C., &amp; Timbrell, N. (2015). The New Literacies of Online Research and Comprehension: Rethinking the Reading Achievement Gap. Reading Research Quarterly, 50(1), 37–59. https://doi.org/10.1002/rrq.85 Li, Y., &amp; Belkin, N. J. (2008). A faceted approach to conceptualizing tasks in information seeking. Information Processing &amp; Management, 44(6), 1822–1837. Ling, C., Steichen, B., &amp; Choulos, A. G. (2018). A comparative user study of interactive multilingual search interfaces. Proceedings of the 2018 Conference on Human Information Interaction &amp; Retrieval, 211–220. https://doi.org/10.1145/3176349.3176383 Liu, C., Gwizdka, J., Liu, J., Xu, T., &amp; Belkin, N. J. (2010). Analysis and evaluation of query reformulations in different task types. Proceedings of the American Society for Information Science and Technology, 47(1), 1–9. Liu, Z., Liu, Y., Zhou, K., Zhang, M., &amp; Ma, S. (2015). Influence of vertical result in web search examination. Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 193–202. https://doi.org/10.1145/2766462.2767714 Lorigo, L., Haridasan, M., Brynjarsdóttir, H., Xia, L., Joachims, T., Gay, G., Granka, L., Pellacini, F., &amp; Pan, B. (2008). Eye tracking and online search: Lessons learned and challenges ahead. Journal of the American Society for Information Science and Technology, 59(7), 1041–1052. https://doi.org/10.1002/asi.20794 Lorigo, L., Pan, B., Hembrooke, H., Joachims, T., Granka, L., &amp; Gay, G. (2006). The influence of task and gender on search and evaluation behavior using google. Information Processing &amp; Management, 42(4), 1123–1131. Loyens, S. M. M., Magda, J., &amp; Rikers, R. M. J. P. (2008). Self-Directed Learning in Problem-Based Learning and its Relationships with Self-Regulated Learning. Educational Psychology Review, 20(4), 411–427. https://doi.org/10.1007/s10648-008-9082-7 Mannion, J. (2020). Metacognition, self-regulation and self-regulated learning: What’s the difference? In impact.chartered.college. https://impact.chartered.college/article/metacognition-self-regulation-regulated-learning-difference/ Mao, J., Liu, Y., Kando, N., Zhang, M., &amp; Ma, S. (2018). How does domain expertise affect users’ search interaction and outcome in exploratory search? ACM Transactions on Information Systems, 36. Marchionini, G. (1995). Information Seeking in Electronic Environments. Cambridge University Press. Marchionini, G. (2006). Toward human-computer information retrieval. Bulletin of the American Society for Information Science and Technology, 32(5), 20–22. Marton, F., &amp; Säaljö, R. (1976). On qualitative differences in learning—ii outcome as a function of the learner’s conception of the task. British Journal of Educational Psychology, 46(2), 115–127. Marton, F., &amp; Säljö, R. (1976). On qualitative differences in learning: I—outcome and process. British Journal of Educational Psychology, 46(1), 4–11. McGrew, S. (2020). Learning to evaluate: An intervention in civic online reasoning. Computers &amp; Education, 145, 103711. McGrew, S. (2021). Skipping the source and checking the contents: An in-depth look at students’ approaches to web evaluation. Computers in the Schools, 38(2), 75–97. McGrew, S., Breakstone, J., Ortega, T., Smith, M., &amp; Wineburg, S. (2018). Can students evaluate online sources? Learning from assessments of civic online reasoning. Theory &amp; Research in Social Education, 46(2), 165–193. McGrew, S., &amp; Glass, A. C. (2021). Click Restraint: Teaching Students to Analyze Search Results. Proceedings of the 14th International Conference on Computer-Supported Collaborative Learning-CSCL 2021. McGrew, S., Ortega, T., Breakstone, J., &amp; Wineburg, S. (2017). The challenge that’s bigger than fake news: Civic reasoning in a social media environment. American Educator, 41(3), 4. Mihailidis, P., &amp; Thevenin, B. (2013). Media literacy as a core competency for engaged citizenship in participatory democracy. American Behavioral Scientist, 57(11), 1611–1622. Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. Psychological Review, 63(2), 81. Miller, W. R., &amp; Brown, J. M. (1991). Self-regulation as a conceptual basis for the prevention and treatment of addictive behaviours. Self-Control and the Addictive Behaviours, 3–79. Moon, B., Hoffman, R. R., Novak, J., &amp; Canas, A. (Eds.). (2011). Applied Concept Mapping: Capturing, Analyzing, and Organizing Knowledge (Zeroth). CRC Press. https://doi.org/10.1201/b10716 Mozilla Developer Network. (2021). Building a cross-browser extension - mozilla | MDN. https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Build_a_cross_browser_extension. National Research Council. (2000). How people learn: Brain, mind, experience, and school: Expanded edition. The National Academies Press. https://doi.org/10.17226/9853 New London Group. (1996). A pedagogy of multiliteracies: Designing social futures. Harvard Educational Review, 66(1), 60–92. Next Generation Science Standards. (2021). Task annotation project in science | sense-making. https://www.nextgenscience.org/sites/default/files/TAPS%20Sense-making.pdf. Novak, J. D. (2002). Meaningful learning: The essential factor for conceptual change in limited or inappropriate propositional hierarchies leading to empowerment of learners. Science Education, 86(4), 548–571. Novak, J. D. (2010). Learning, creating, and using knowledge: Concept maps as facilitative tools in schools and corporations (2nd ed). Routledge. Novak, J. D., &amp; Gowin, D. B. (1984). Learning how to learn. Cambridge University Press. https://doi.org/10.1017/CBO9781139173469 O’Brien, H. L., Kampen, A., Cole, A. W., &amp; Brennan, K. (2020). The role of domain knowledge in search as learning. Conference on Human Information Interaction and Retrieval (CHIIR). Palani, S., Fourney, A., Williams, S., Larson, K., Spiridonova, I., &amp; Morris, M. R. (2020). An eye tracking study of web search by people with and without dyslexia. Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 729–738. https://doi.org/10.1145/3397271.3401103 Pan, B., Hembrooke, H. A., Gay, G. K., Granka, L. A., Feusner, M. K., &amp; Newman, J. K. (2004). The determinants of web page viewing behavior: An eye-tracking study. Proceedings of the 2004 Symposium on Eye Tracking Research &amp; Applications, 147–154. Pea, R., &amp; Jacks, D. (2014). The learning analytics workgroup: A report on building the field of learning analytics for personalized learning at scale. https://ed.stanford.edu/sites/default/files/law_report_complete_09-02-2014.pdf; Stanford, CA: Stanford University. Penn State Learning. (2021). Listening and note taking survey | penn state learning. https://pennstatelearning.psu.edu/listening-and-note-taking-survey; Pennsylvania State University. Pennanen, M., &amp; Vakkari, P. (2003). Students’ conceptual structure, search process, and outcome while preparing a research proposal: A longitudinal case study. Journal of the American Society for Information Science and Technology, 54(8), 759–770. Piaget, J. (1936). Origins of intelligence in children. Pirolli, P., Schank, P., Hearst, M., &amp; Diehl, C. (1996). Scatter/gather browsing communicates the topic structure of a very large text collection. Conference on Human Factors in Computing Systems (CHI’96). Qvarfordt, P., Golovchinsky, G., Dunnigan, T., &amp; Agapie, E. (2013). Looking ahead: Query preview in exploratory search. Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 243–252. https://doi.org/10.1145/2484028.2484084 Rieh, S. Y. (2020). Research area 1: Searching as learning. https://rieh.ischool.utexas.edu/research. Rieh, S. Y., Collins-Thompson, K., Hansen, P., &amp; Lee, H.-J. (2016). Towards searching as a learning process: A review of current perspectives and future directions. Journal of Information Science, 42(1), 19–34. https://doi.org/10.1177/0165551515615841 Rieh, S. Y., Kim, Y.-M., &amp; Markey, K. (2012). Amount of invested mental effort (AIME) in online searching. Information Processing &amp; Management, 48(6), 1136–1150. Roy, N., Moraes, F., &amp; Hauff, C. (2020). Exploring users’ learning gains within search sessions. Conference on Human Information Interaction and Retrieval (CHIIR). Roy, N., Torre, M. V., Gadiraju, U., Maxwell, D., &amp; Hauff, C. (2021). Note the highlight: Incorporating active reading tools in a search as learning environment. Proceedings of the 2021 Conference on Human Information Interaction and Retrieval, 229–238. Rumelhart, D. E., &amp; Norman, D. A. (1981). Accretion, tuning and restructuring: Three modes of learning. In J. W. Cotton &amp; K. R. (Eds.), Semantic factors in cognition (pp. 37–90). Rumelhart, D. E., &amp; Ortony, A. (1977). The representation of knowledge in memory. In R. C. Anderson, S. R. J., &amp; M. W. E. (Eds.), Schooling and the acquisition of knowledge (pp. 99–135). Hillsdale, NJ: Erlbaum. Ryan, R. M. (1982). Control and information in the intrapersonal sphere: An extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43(3), 450. Ryan, R. M., &amp; Deci, E. L. (2000a). Intrinsic and extrinsic motivations: Classic definitions and new directions. Contemporary Educational Psychology, 25(1), 54–67. Ryan, R. M., &amp; Deci, E. L. (2000b). Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist, 55(1), 68. Ryan, R. M., &amp; Deci, E. L. (2017). Self-determination theory: Basic psychological needs in motivation, development, and wellness. Guilford Publications. Saks, K., &amp; Leijen, Ä. (2014). Distinguishing Self-directed and Self-regulated Learning and Measuring them in the E-learning Context. Procedia - Social and Behavioral Sciences, 112, 190–198. https://doi.org/10.1016/j.sbspro.2014.01.1155 Saracevic, T. (1975). Relevance: A review of and a framework for the thinking on the notion in information science. Journal of the American Society for Information Science, 26(6), 321–343. Saracevic, T. (2007a). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: Nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(13), 1915–1933. https://doi.org/10.1002/asi.20682 Saracevic, T. (2007b). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126–2144. Saracevic, T. (2016). The Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Synthesis Lectures on Information Concepts, Retrieval, and Services. Sawyer, R. K. (2005). The Cambridge handbook of the learning sciences. Cambridge University Press. Scharinger, C., Kammerer, Y., &amp; Gerjets, P. (2016). Fixation-Related EEG Frequency Band Power Analysis: A Promising Neuro-Cognitive Methodology to Evaluate the Matching-Quality of Web Search Results? HCI International 2016 Posters’ Extended Abstracts, 245–250. https://doi.org/10.1007/978-3-319-40548-3_41 Schraw, G., &amp; Dennison, R. S. (1994). Assessing Metacognitive Awareness. Contemporary Educational Psychology, 19(4), 460–475. https://doi.org/10.1006/ceps.1994.1033 Simon, H. A. (1956). Rational choice and the structure of the environment. Psychological Review, 63(2), 129. Slanzi, G., Balazs, J. A., &amp; Velásquez, J. D. (2017). Combining eye tracking, pupil dilation and EEG analysis for predicting web users click intention. Information Fusion, 35, 51–57. https://doi.org/10.1016/j.inffus.2016.09.003 Smith, C. L., Gwizdka, J., &amp; Feild, H. (2016). Exploring the use of query auto completion: Search behavior and query entry profiles. Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, 101–110. https://doi.org/10.1145/2854946.2854975 Smith, C. L., &amp; Kantor, P. B. (2008). User adaptation: Good results from poor systems. Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 147–154. Spink, A. (1997). Study of interactive feedback during mediated information retrieval. Journal of the American Society for Information Science. Stanford History Education Group. (2021). Webpage comparison | Civic Online Reasoning. https://cor.stanford.edu/curriculum/assessments/webpage-comparison. Syed, R., Collins-Thompson, K., Bennett, P. N., Teng, M., Williams, S., Tay, D. W. W., &amp; Iqbal, S. (2020). Improving learning outcomes with gaze tracking and automatic question generation. The Web Conference (WWW). Tahamtan, I. (2019). The effect of motivation on web search behaviors of health consumers. Proceedings of the 2019 Conference on Human Information Interaction and Retrieval, 401–404. Terlecki, M. (2020). Revising the Metacognitive Awareness Inventory (MAI) to be More User-Friendly. In Improve with Metacognition. https://www.improvewithmetacognition.com/revising-the-metacognitive-awareness-inventory/ Terlecki, M., &amp; McMahon, A. (2018). A Call for Metacognitive Intervention: Improvements Due to Curricular Programming in Leadership. Journal of Leadership Education, 17(4), 130–145. https://doi.org/10.12806/V17/I4/R8 UMass Amherst Student Success. (2021). Note taking strategies inventory | success@UMass. https://www.umass.edu/studentsuccess/sites/default/files/inline-files/note-taking-strategies_0.pdf. Vakkari, P. (2001a). A theory of the task-based information retrieval process: A summary and generalisation of a longitudinal study. Journal of Documentation, 57(1), 44–60. https://doi.org/10.1108/EUM0000000007075 Vakkari, P. (2016). Searching as learning: A systematization based on literature. Journal of Information Science, 42(1), 7–18. https://doi.org/10.1177/0165551515615833 Vakkari, P. (2020). The Usefulness of Search Results: A Systematization of Types and Predictors. Proceedings of the 2020 Conference on Human Information Interaction and Retrieval, 243–252. https://doi.org/10.1145/3343413.3377955 Vakkari, P. (2000). Cognition and changes of search terms and tactics during task performance: A longitudinal case study. In Content-based multimedia information access-volume 1 (pp. 894–907). Vakkari, P. (2001b). Changes in search tactics and relevance judgements when preparing a research proposal a summary of the findings of a longitudinal study. Information Retrieval, 4(3), 295–310. Vakkari, P., Völske, M., Potthast, M., Hagen, M., &amp; Stein, B. (2019). Modeling the usefulness of search results as measured by information use. Information Processing &amp; Management, 56(3), 879–894. https://doi.org/10.1016/j.ipm.2019.02.001 Van Der Vaart, R., &amp; Drossaert, C. (2017). Development of the digital health literacy instrument: Measuring a broad spectrum of health 1.0 and health 2.0 skills. Journal of Medical Internet Research, 19(1), e27. Villa, R., &amp; Halvey, M. (2013). Is relevance hard work? Evaluating the effort of making relevant assessments. Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval, 765–768. Wang, Y., Yin, D., Jie, L., Wang, P., Yamada, M., Chang, Y., &amp; Mei, Q. (2018). Optimizing whole-page presentation for web search. ACM Trans. Web, 12(3). https://doi.org/10.1145/3204461 Weber, H., Becker, D., &amp; Hillmert, S. (2019). Information-seeking behaviour and academic success in higher education: Which search strategies matter for grade differences among university students and how does this relevance differ by field of study? Higher Education, 77(4), 657–678. https://doi.org/10.1007/s10734-018-0296-4 Weber, H., Hillmert, S., &amp; Rott, K. J. (2018). Can digital information literacy among undergraduates be improved? Evidence from an experimental study. Teaching in Higher Education, 23(8), 909–926. https://doi.org/10.1080/13562517.2018.1449740 White, R. (2016a). Interactions with search systems. Cambridge University Press. White, R. (2016b). Learning and use. In Interactions with search systems (pp. 231–248). Cambridge University Press. https://doi.org/10.1017/CBO9781139525305.010 White, R., Dumais, S., &amp; Teevan, J. (2009). Characterizing the influence of domain expertise on web search behavior. Proceedings of the Second ACM International Conference on Web Search and Data Mining - WSDM ’09, 132. https://doi.org/10.1145/1498759.1498819 Wildemuth, B. M. (2004). The effects of domain knowledge on search tactic formulation. Journal of the American Society for Information Science and Technology, 55(3), 246–258. https://doi.org/10.1002/asi.10367 Wilson, M. J., &amp; Wilson, M. L. (2013). A comparison of techniques for measuring sensemaking and learning within participant-generated summaries. Journal of the American Society for Information Science and Technology, 64(2), 291–306. Wilson, T. D. (1999). Models in information behaviour research. Journal of Documentation, 55(3), 249–270. Wineburg, S., &amp; McGrew, S. (2016). Why students can’t google their way to the truth. Education Week, 36(11), 22–28. Wineburg, S., &amp; McGrew, S. (2017). Lateral reading: Reading less and learning more when evaluating digital information. Wittrock, M. C. (1989). Generative processes of comprehension. Educational Psychologist, 24(4), 345–376. Xu, L., Zhou, X., &amp; Gadiraju, U. (2020). How does team composition affect knowledge gain of users in collaborative web search? Conference on Hypertext and Social Media (HT). Yu, R., Gadiraju, U., Holtz, P., Rokicki, M., Kemkes, P., &amp; Dietze, S. (2018). Predicting User Knowledge Gain in Informational Search Sessions. The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, 75–84. https://doi.org/10.1145/3209978.3210064 Zhang, P., &amp; Soergel, D. (2014). Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking. Journal of the Association for Information Science and Technology, 65(9), 1733–1756. https://doi.org/10.1002/asi.23125 Zhang, P., &amp; Soergel, D. (2016). Process patterns and conceptual changes in knowledge representations during information seeking and sensemaking: A qualitative user study. Journal of Information Science, 42(1), 59–78. Zhang, X., Cole, M., &amp; Belkin, N. (2011). Predicting Users’ Domain Knowledge from Search Behaviors. Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1225–1226. https://doi.org/10.1145/2009916.2010131 Zimmerman, B. J. (1989). A social cognitive view of self-regulated academic learning. Journal of Educational Psychology, 81(3), 329. Zlatkin-Troitschanskaia, O., Hartig, J., Goldhammer, F., &amp; Krstev, J. (2021). Students’ online information use and learning progress in higher education A critical literature review. Studies in Higher Education, 1–26. https://doi.org/10.1080/03075079.2021.1953336 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
