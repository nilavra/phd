# Results



As the sample sizes were often small, and / or did not match the normality assumptions of parametric tests, we have employed the non-parametric Mann-Whitney U test for all the null-hypothesis testing. This allows easy comparison between different categories of results.



Additionally, we also report Common Language Effect Size (CLES) for each statistical test result.
The common language effect size is the proportion of pairs where $x$ is higher than $y$. 
In other words, it is the probability that a score sampled at random from one distribution will be greater than a score sampled from some other distribution. 
CLES was first introduced by McGraw and Wong (1992) [TODO]. The Python statistical library employed in data analysis -- Pingouin [@vallat2018pingouin] -- uses a brute-force version of the formula given by Vargha and Delaney 2000 [TODO]:
The advantage is of this method are twofold. First, the brute-force approach pairs each observation of x to its y counterpart, and therefore does not require normally distributed data. Second, the formula takes ties into account and therefore works with ordinal data.

In the interest of better presentation and readability of this chapter, some of the ancillary plots are presented in Appendices \@ref(app-plots-query) and \@ref(app-plots-search).



## Latent Profiles (and Transitions)


TODO: describe the high and low groups appropriately.

TODO: Describe stats from `HI` and `LO` profiles, as in YSC Sec 5.1.

- how the following changed over time 
  - motivation, metacognition, self regulation
  - perceived learning, perceived search outcome



<!-- 

The rank biserial correlation [2] is the difference between the proportion of favorable evidence minus the proportion of unfavorable evidence.



Common language effect size (CLES):
the probability that a score sampled at random from one distribution will be greater than a score sampled from some other distribution.

CLES are concordant with sig_dir_mean: so use them


start
long
end
overall

total, avg, SD

$5 \pm 4$

TODO: change in mean and sig diff over time / task type

  - longitudinal phase
  - all
  - repeated task
  - non-repeated task
  - 
-->



TODO: 

- For each section below on measures
  - how the measure was operationalized 
  - what is the value / trend
  - 1-2 lines
- For Disucssion later
  - Pool together the 1-2 lines

Proposal (`PHASE2A`)

Outline (`PHASE2B`)

Rough Draft (`PHASE2C`)

Final Paper (`PHASE2D`)



## Query Formulation


### Length of Queries

Query length was operationalized as the number of terms (words separated by spaces) in the search query that participants submitted to the search engines or other information retrieval sites.
Query length can vary from a single word to several phrases or a full sentence. 
<!-- Search engines use algorithms to analyze search query length and interpret the intent of the user's search in order to provide the most relevant search results.  -->
Longer search queries may indicate a more specific or complex information need, while shorter search queries may be more general or broad in scope.
<!-- The number of terms in each query was aggregated over each participant-task pair, to obtain total, average, and standard deviation (variability) of query length. -->

For all tasks, on average, the high group issued longer queries than the low group (Figure  ResP4090733126835336320), except before the Outline (`PHASE2B`), and the Final Paper submission (`PHASE2D`).
The low group, with lower levels of motivation, metacognition, and self-regulation, were issuring queries which were often two terms (words) shorter on average, than the corresponding queries from the high group.

The total query length for the low group increased monotonically over the semester during `PHASE2` (Figure  ResP494716060171793945), with the longest queries being issued by this group before the Final Paper submission phase (`PHASE2D`).
On the other hand, queries from the high group were the longest during the Outline phase (`PHASE2B`), and then it decreased in the subsequent phases.
This indicates that the high group, with higher levels of motivation, metacognition, and self-regulation, were planning ahead in the early parts of the semester to find their references. 
On the other hand, the low group, with lower levels of those traits, demonstrated a tendency to procrastinate and delay their work until the final paper.
A quote from participant `P006_HANOI` illustrates this phenomenon:

> *"so far for this rough draft I haven't actually had to google anything since I downloaded everything into pdfs during the outline stage and haven't needed any new material so far."*
>
> `r nbQuoteAuthor("--- P006_HANOI")`
>

<!-- 
LO group had highest total query length before final paper (ResN5894660104957913932)


HI group had highest average query length during rough draft (ResP6726131652814458688)

LO group was slacking and putting it off till last minute.

 -->



  

### Count of queries per search task

Count of queries per search task refers to the number of separate queries or search attempts that a participant issued in order to complete a task. 
<!-- having the same information need. -->
<!-- single information-seeking task on a search engine or other information retrieval system. A search task is typically defined as a specific information need or question that the user is attempting to answer through a series of search queries.  -->
The count of queries per search task may vary depending on the complexity of the information need, the user's level of expertise with the search system, and other factors. 
<!-- This metric is commonly used in information retrieval research to evaluate the effectiveness of search systems and user search behavior. -->

The average number of queries issued by the high group increased from Proposal to Outline to Rough Draft, and then decreased before the Final Paper task (Figure ResP4526976288600811998).
In contrast, the low group issued higher number of queries during the early part of the semester -- Proposal and Outline -- but then they issued lower number of queries during the later parts of the semester, w.r.t. the high group. 
The difference in the total number of queries issued by the two groups approched statistical significance according to a two-tailed Mann-Whitney U test, but the effect size was very weak $(U = 14.0, n_{low}=8, n_{high}=8, p=.06, CLES=0.22)$.

<!-- were the highest before the Rough Draft (`PHASE2C`) . -->


<!-- Like avg query length, LO group issued highest number of queries when writing final paper, while HI roup did that during outline.
Even, one participant commendted "I had downloaded all the papers I needed during..." -->



### Count of reformulated query types

Query reformulation refers to the process of modifying or refining a search query in order to improve the relevance of search results and better match the user's information needs (Section \@ref(sec-bg-search-query)). 
Query reformulation typically occurs due to a searcher's improved understanding of how to better translate their information need into a search query.
Using the taxonomy proposed by @liu2010analysis (Figure \@ref(fig:res-Q-QRT-txnmy)), we classified each previous-next query pair issued by participants into one of the five query reformulation types (QRTs):
New,
Generalization,
Specialization,
Word Substitution,
and 
Repeat.

<!-- When a user submits a search query that does not return satisfactory results, they may attempt to reformulate their query by using different keywords, rephrasing their question, or adding or removing search terms. Query reformulation can be done manually by the user or automatically by the search system using techniques such as query expansion or query suggestion. The goal of query reformulation is to help users find the information they are looking for more efficiently and effectively. -->


The low group issued more number of **New** queries in total (Figure ResN3873804091287075248) and on average (Figure ResP3026382842695489517), compared to the high group. 
During the Rough Draft and Final Paper phases, the low group exhibited heightened tendency to generate New search queries in their pursuit of novel information. 
On average, participants in the low group issued four more search queries than those in the HI group in these last two phases of the research project (Figure ResP3026382842695489517). 
This suggests that the LO group engaged in more active and extensive information-seeking behaviour, potentially reflecting a greater sense of urgency and pressure to complete their assignments.
The difference in total number of New queries between the groups approached significance $(U = 50.0, n_{low}=8, n_{high}=8, p=.06, CLES=0.78)$, with a moderately strong effect size.


<!-- ResN8316003849035045794 
no. of new queries was the highest during final paper for the LO group
they were frantically searching for new information
on an average, each participant in the LO group was making 4 more search queries than those in the HI group during the final paper phase. -->

The low group demonstrated a significantly higher count of total **Query Generalizations** $(U = 44.5, n_{low}=7, n_{high}=7, p=.01, CLES=0.91)$. 
While the high group issued a total of $2.57 \pm 2.1$ query generalizations per task the low group were issuing $9.86 \pm 6.7$ generalized queries for the same, across the entire study (Figure ResN3302769723271690264).
Query generalization is an indicative of exploratory behaviour, and this increase by the low group indicates a lack of specificity in the participants' information needs, which may lead to lower precision in search results.



The high group issued an average at least one word substitution for each search task.
The trend in the count of total **Query Specializations** was similar to that of Query Generalizations (Figure ResN6450294753032251158). 
The high group demonstrated the maximum number of query specialization before the Outline phase, whereas the low group did the same before the Final Paper phase.
While the difference was not significant, the effect size was moderately high $(U = 42.5, n_{low}=8, n_{high}=7, p=.10, CLES=0.76)$.


**Word Substitutions** were performed in all the tasks by the high group, whereas the low group skipped it during Proposal, and the `PHASE3-BIAS` task (Figure ResN3108315939720148502). 


We faced a challenge in disentangling **Repeat** queries from "hub-and-spoke" behaviour, where the user goes back and forth between the SERP and a content page by using the browser's forward and back buttons ^[The SERP is the hub, which represents the initial point of inquiry, while the spokes represent the subsequent branches of exploration along different content pages. This search behaviour is often used when users have a general idea of the topic they are interested in, but need to explore different facets of the topic to narrow down their search and find relevant information.].
Each back button press on the browser (to go back to the SERP from a content page) meant a fresh HTTP GET request was sent to the search engine. 
This resulted in YASBIL logging the move as a resubmission of the query.
Keeping this in mind, we observed that the low group issued a significantly higher number of repeat queries (or hub-and-spoke moves), compared to the high group $(U = 51.5, n_{low}=7, n_{high}=8, p=.0077, CLES=0.92)$.
The highest differences occurred in the Outline phase and the Final paper phase (Figure ResP7955607431899741484), with the low group issuing double the number of repeat queries $(6.11 \pm 3.7)$ on average, w.r.t. the high group $(3.51 \pm 2.8)$ 


<!-- 

ResN3604624796642370520 generalizations
ResP5443351906342031714 specializations
ResN4290201084087196149 word substitutions
ResN9056495301645634830 repeat
Repeat must be taken with a grain of salt as repeat behaviour was diffcult to tease out from hub and spoke behaviour, if the user pressed the back button of the browser -->

### Number of clicks per query

ResP878771263343515297
HI group was more efficient. Average number of clicks per query was lowe (less than 4) as they moved forward in the semester, with highest in the Outline phase.


ResN2805268620307804410

$(U = 1069.5, n_{HI}=56, n_{LO}=30, p=.03)$


ResN5118140679486679749
SD also says something



### Entropy of Query Reformulation {#sec-res-query-H}

Inspired by previous works in analysing eye-movement sequences [@krejtz2014entropy, @krejtz2015gaze] and search tactics sequences [@he2016beyond], we employed a similar entropy analysis of query reformulation sequences issued by users.

Transition analysis and entropy helps to cover differences in disparate tasks and activities (get text from Qvardford paper)

If we consider the sequence of query reformulations issued by a participant 
(e.g., *New -> New -> Specialization -> Specialization -> Word Substitution -> Generalization*)
then this sequence can be considered as a first order Markov chain, wherein, the next step in the chain depends only on the current state.
Entropy analysis on these Markov chains quantifies how predictable the states are, and yields two categories of uncertainty measures: transition entropy, $H_t$, and stationary entropy $H_s$.

In the context of query reformulations, the maximum **transition entropy** $(s \log s)$ can be reached when there is an equal probability of switching between each of the $s = 5$ states, or QRTs (query reformulation types). 
The minimum transition entropy (0) is achieved in a fully deterministic Markov chain where all transition probabilities are either 1 or 0. 
This means that with a higher transition entropy there is more randomness in the participant's transitions between different QRTs. 
This randomness is an indication that the participants do not have a clear progression from one QRT to another. 
On the other hand, a lower transition entropy indicates that the participant's transition between QRTs are highly predictable.
**Stationary entropy** is calculated from the distribution of QRTs. 
A higher stationary entropy value indicates that the QRTs were used uniformly, while a lower stationary entropy indicates that some QRTs are preferred over others. 
Values of stationary entropy vary between 0 and $(\log s)$, where $s$ is the number of possible states (QRTs)^[The explanation is adapted from @he2016beyond].
The entropies discussed in the following sections were normalized by their theoretical maximums, for equivalent comparison across different conditions.


<!-- A higher query reformulation entropy indicates a greater degree of uncertainty or randomness in the user's query reformulation behaviour, indicating that the user may be struggling to find relevant information or is uncertain about their information needs. In contrast, a lower query reformulation entropy suggests a more systematic and directed approach to search, indicating that the user has a clearer understanding of their information needs and is using query reformulation strategies to efficiently retrieve relevant information. Query reformulation entropy is a useful metric for evaluating the effectiveness of search systems and understanding user search behaviour.

Transition entropy is a metric that measures the uncertainty or randomness in the transitions between different search states, such as between queries, documents, or web pages. It quantifies the level of unpredictability or randomness in the user's navigation behavior as they move from one state to another, and can be used to evaluate the efficiency and effectiveness of search systems and interfaces.

Stationary entropy, on the other hand, measures the uncertainty or randomness in the distribution of user behavior within a single search state. For example, it can be used to analyze the frequency and distribution of query terms used by a user during a search session, or the distribution of clicks on search results pages. Stationary entropy provides insights into the structure and characteristics of user search behavior within a given context, and can be used to optimize search algorithms and ranking models. -->



#### Transition Entropy of Query Reformulation

Transition entropy remained lower for HI group at beginning, middle and end


For repeated task (finance) HI group stopped querying from the middle of the task.
Hi group felt more certain (TODO: get quote from response?)
While LO group continued till the end to query - they were more uncertain.


TODO: add network figure?

#### Stationary Entropy of Query Qeformulation

ResN2968292861275564559 - whole
stationary entropy

Stationary entropy:
for HI goup
high in the beginning of task
then low at the end of task
their chaosness decreased as they progressed through the tasks, while for the LO group, their uncertainty increased?



ResN634943316862196398 - beginning
ResN6777243644016606483 - middle
ResN6573661794052961083 - end




<!-- ### Query Location

where did participants query?
Google, Library, etc -->










## Webpages Visits

As the final project task involved searching for 20 references, we structure the discussion around visits to "normal" webpages, and visits to "academic" webpages containing publications.

### All

HI group visited more number of pages in total and on average

Count

Dwell time


### L: Lists / Search Results

#### LWEB

Count

Dwell Time


Web Search Results

ResN7422811346952663922

#### LPUB

Count

Dwell Time

Publication / Academic search results




### I: Information Objects / Content Pages

#### IWEB - Normal web pages

Count

Dwell Time


#### IPUB - Publications

Count

Dwell Time






## Entropy of Search Tactics


Similar to entropy analysis of Query Reformulation types (Section \@ref(sec-res-query-H)), we perform entropy analysis of search tactic transitions.
This analysis is directly inspired from @he2016beyond, who in turn were inspired from [@krejtz2014entropy, @krejtz2015gaze].

In the context of search tactic transitions, a higher value of transition entropy ($H_t$) indicates more randomness and uncertainty in the participant's search behaviour (which is composed of different search tactics, and transitioning or switching between those tactics).
<!-- in transitions between different search tactics, and consequently in search behaviour. -->
A lower value of transition entropy indicates that the search behaviour (i.e. tactic switching behaviour) is highly predictable.
For stationary entropy of search tactics ($H_s$), a higher value indicates that participants utilize all the search tactics with equal probability, while a lower value suggests that certain search tactics are favoured over others.
<!-- all search tactics are used uniformly, whereas a lower value indicates that some search tactics are more preferred over others. -->
The entropies discussed in the following sections were normalized by their theoretical maximums, for equivalent comparison across different conditions.


### Transition Entropy of Search Tactics

ResN7289045485214067875 
Both groups had highest Ht during 2C (and also 3 Finance?)

ResP5638176378435107439 - beginning
ResN3435548740759333098 - middle
ResP6845508037293675246 - end

No sig diff for whole task, and at the beginning of task
Sig diff appeared at the middle and increased at the end of task (similar to as observed by Gwizdka).


### Stationary Entropy of Search Tactics

ResP8685879301269737254
LO group had hghest Hs in 2C
No sig diff in overall task

No diff at beginning.
Diff is most prominent at middle (ResN1140102300744288543)
and also somewhat at end (ResN663758335140886910)







R!esN7928923989685423424 is a VERY good result!
Model ID = 10




More plots are in Appendix \@ref(app-plots), such as Figure \@ref(fig:ResP5183239600404564866)



## Learning Outcomes

Correlation of SPL with Grades

- self-perceived learning (SPL)
- grades
- paired t-tests for SPL with obtained grade
- CONCL: obtained grades are not good indicators of learning















<!-- ================= OLD CONTENTS =============== -->


<!-- 

What about learning??
What are the measures of learning?

- Also see Yung Sheng's Dissertation
- think hard about which data component has not been touched / analysed 
-->






<!-- 

## Qualitative / Free Text Results

Note-taking strategies

- how do you organize your notes
- how long do you store your notes
- how do you search for a bit of info in the notes

Other surveys










Difference between timepoints (across all profiles?)
Difference between profiles (across all times)
Mix (?)

For everything below: change in mean and sig diff over time / task type
    - repeated task
        - A / B
    - non-repeated task
        - A / B
    - longitudinal task
        - A / B

    ✅ Number of terms per query: 
      - query_len_{sum, avg, sd}

    ✅ Number (count) of queries per search task: 
      - query_n_{sum, avg, sd}

    ✅ Number (count) of reformulated query types:
      - query_reform_gen_{sum, avg, sd}
      - query_reform_spec_{sum, avg, sd}
      - query_reform_wsub_{sum, avg, sd}
      - query_reform_rpt_{sum, avg, sd}
      - query_reform_new_{sum, avg, sd}

    ✅ Entropy of query reformulation types
      - {overall, beginning, middle, end} of tasks
      - for different task types
      - query_reform_{Hs, Ht}
      - 🧠 always use closed state space for better entropy

    📌 characterizing words in query according to difficulty
      - freq in vocabulary / specialized terms
        - whether this changes over time

    ✅ No. of clicks per query
    - query_click_n_{sum, avg, sd}
  
    📌 Abandoned queries (Percentage of queries with no clicks) ==> per task
    - query_abandoned_pct_{sum, avg, sd}
    - only 56 queries were without clicks
    
    ✅ generate report containing each variable (cols?), and each task name (rows)
    like CHI SRC paper

    📃 create graphs

    Query Location: where did participants query?
    - Google, Library, etc

    

  TODO: across time, somehow
-->

<!--



## L - source selection / Item Selection

- dwell times
- "Item" selection as in IWSS

## I - interacting with sources

- dwell times

> 
> *If the dwell time is long, i.e. ≥ 5 seconds, it is more likely that a user is reading the search results summary (ER) rather than only skimming it (EI). The 5 second threshold was determined based on reading research using eye tracking (Rayner, 1998) and the size of the summaries in Querium. A time span of less than 5 seconds is a too short period for being able to read a summary and extract information.*
>
> `r nbQuoteAuthor('--- @he2016beyond')`
>


## Overall search behaviour

- sig diffs at (beginning, middle and end of tasks) x (LPA profiles)
- 



Sequences

✅ Transition and Stationary Entropies
- Change in entropy with time (Qvardfort paper)
- Transition matrix density (ET bibile Sec 10.7.2)

- Patterns from Russell Paper
  - Short Navigation: [BREAK]* -> S -> X -> BREAK
  - Topic Exploration: S –> X –> X –> X –> X –> …
  - Methodical Results Exploration: S –> X –> S –> X –> S …
  - Query Refinement: S –> S –> S –> S …

- query without clicks
  
- [Overall / Beginning / Middle / End of task] x [LPA Profiles]
  ✅ Entropies
  - Patterns
  - Sequence Lengths ==> no. of actions performed.


✅ Number (Count) of unique URLs (webpages) visited [visit_n_{sum,avg,sd}]
- url_n
    _{lweb, lpub, lother, iweb, ipub, iother, misc, all}
    _{sum, avg, sd}
    _{whole, beg, mid, end}

⏳ Dwell Times on webpages [dwell_{category}_{sum,avg,sd}] - seconds
  - url_dur
    _{lweb, lpub, lother, iweb, ipub, iother, misc, all}
    _{sum, avg, sd}
    _{whole, beg, mid, end}

^^ Already Too much

📃 Viz: LPA Profile Mean + Transition parallel categories (like CHI paper)
- generate for each profile (60)
- extract HTML for each plot, and put all of them together in one HTML file

🚨🚨 LEARNING OUTCOME!!! 🚨🚨 
- grades

Task Time
  - task_dur_{sum, avg, sd}
  - maybe too much, because need to exclude IDLE time from task
  - or, aka, `url_dur_all_sum` is an indicator of `task_dur`?


Other Time
  - time between queries [later]



Unique domains
  - Percentage and number of unique  clicked domains



- 

📃 Viz: Distributions of all tactics - tables / bars / etc.

-->
