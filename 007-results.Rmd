# Results and Discussion


Additionally we also report Common Language Effect Size (CLES) for each statistical test result.
The common language effect size is the proportion of pairs where $x$ is higher than $y$. 
In other words, it is the probability that a score sampled at random from one distribution will be greater than a score sampled from some other distribution. 
CLES was first introduced by McGraw and Wong (1992) [TODO]. The Python statistical library employed in data analysis -- Pingouin [@vallat2018pingouin] -- uses a brute-force version of the formula given by Vargha and Delaney 2000 [TODO]:
The advantage is of this method are twofold. First, the brute-force approach pairs each observation of x to its y counterpart, and therefore does not require normally distributed data. Second, the formula takes ties into account and therefore works with ordinal data.



## Latent Profiles


TODO: describe the high and low groups appropriately.

TODO: Describe stats from `HI` and `LO` profiles, as in YSC Sec 5.1.

<!-- 

The rank biserial correlation [2] is the difference between the proportion of favorable evidence minus the proportion of unfavorable evidence.



Common language effect size (CLES):
the probability that a score sampled at random from one distribution will be greater than a score sampled from some other distribution.

CLES are concordant with sig_dir_mean: so use them


start
long
end
overall

total, avg, SD

$5 \pm 4$

TODO: change in mean and sig diff over time / task type

  - longitudinal phase
  - all
  - repeated task
  - non-repeated task
  - 
-->



TODO: 

- For each section below on measures
  - how the measure was operationalized 
  - what is the value / trend
  - 1-2 lines
- For Disucssion later
  - Pool together the 1-2 lines

Proposal (`PHASE2A`)

Outline (`PHASE2B`)

Rough Draft (`PHASE2C`)

Final Paper (`PHASE2D`)



## Query Formulation


### Length of Queries

Query length was operationalized as the number of terms (words separated by spaces) in the search query that participants submitted to the search engines or other information retrieval sites.
Query length can vary from a single word to several phrases or a full sentence. 
<!-- Search engines use algorithms to analyze search query length and interpret the intent of the user's search in order to provide the most relevant search results.  -->
Longer search queries may indicate a more specific or complex information need, while shorter search queries may be more general or broad in scope.
<!-- The number of terms in each query was aggregated over each participant-task pair, to obtain total, average, and standard deviation (variability) of query length. -->

For all tasks, on average, the high group issued longer queries than the low group (Figure  ResP4090733126835336320), except before the Outline (`PHASE2B`), and the Final Paper submission (`PHASE2D`).
The low group, with lower levels of motivation, metacognition, and self-regulation, were issuring queries which were often two terms (words) shorter on average, than the corresponding queries from the high group.

The total query length for the low group increased monotonically over the semester during `PHASE2` (Figure  ResP494716060171793945), with the longest queries being issued by this group before the Final Paper submission phase (`PHASE2D`).
On the other hand, queries from the high group were the longest during the Outline phase (`PHASE2B`), and then it decreased in the subsequent phases.
This indicates that the high group, with higher levels of motivation, metacognition, and self-regulation, were planning ahead in the early parts of the semester to find their references. 
On the other hand, the low group, with lower levels of those traits, demonstrated a tendency to procrastinate and delay their work until the final paper.
A quote from participant `P006_HANOI` illustrates this phenomenon:

> *"so far for this rough draft I haven't actually had to google anything since I downloaded everything into pdfs during the outline stage and haven't needed any new material so far."*
>
> `r nbQuoteAuthor("--- P006_HANOI")`
>

<!-- 
LO group had highest total query length before final paper (ResN5894660104957913932)


HI group had highest average query length during rough draft (ResP6726131652814458688)

LO group was slacking and putting it off till last minute.

 -->



  

### Count of queries per search task

Count of queries per search task refers to the number of separate queries or search attempts that a participant issued in order to complete a task. 
<!-- having the same information need. -->
<!-- single information-seeking task on a search engine or other information retrieval system. A search task is typically defined as a specific information need or question that the user is attempting to answer through a series of search queries.  -->
The count of queries per search task may vary depending on the complexity of the information need, the user's level of expertise with the search system, and other factors. 
<!-- This metric is commonly used in information retrieval research to evaluate the effectiveness of search systems and user search behavior. -->

The average number of queries issued by the high group were the highest before the Outline (`PHASE2B`).
ResP1663923804528545778

Like avg query length, LO group issued highest number of queries when writing final paper, while HI roup did that during outline.
Even, one participant commendted "I had downloaded all the papers I needed during..."



### Count of reformulated query types


ResN8316003849035045794 
no. of new queries was the highest during final paper for the LO group
they were frantically searching for new information
on an average, each participant in the LO group was making 4 more search queries than those in the HI group during the final paper phase.

ResN3604624796642370520 generalizations
ResP5443351906342031714 specializations
ResN4290201084087196149 word substitutions
ResN9056495301645634830 repeat
Repeat must be taken with a grain of salt as repeat behaviour was diffcult to tease out from hub and spoke behaviour, if the user pressed the back button of the browser

### Number of clicks per query

ResP878771263343515297
HI group was more efficient. Average number of clicks per query was lowe (less than 4) as they moved forward in the semester, with highest in the Outline phase.


ResN2805268620307804410

$(U = 1069.5, n_{HI}=56, n_{LO}=30, p=.0378)$


ResN5118140679486679749
SD also says something



### Entropy of Query Qeformulation

#### Transition Entropy of Query Qeformulation

Transition entropy remained lower for HI group at beginning, middle and end


For repeated task (finance) HI group stopped querying from the middle of the task.
Hi group felt more certain (TODO: get quote from response?)
While LO group continued till the end to query - they were more uncertain.


TODO: add network figure?

#### Stationary Entropy of Query Qeformulation

ResN2968292861275564559 - whole
stationary entropy

Stationary entropy:
for HI goup
high in the beginning of task
then low at the end of task
their chaosness decreased as they progressed through the tasks, while for the LO group, their uncertainty increased?



ResN634943316862196398 - beginning
ResN6777243644016606483 - middle
ResN6573661794052961083 - end




<!-- ### Query Location

where did participants query?
Google, Library, etc -->










## Webpages Visits

As the final project task involved searching for 20 references, we structure the discussion around visits to "normal" webpages, and visits to "academic" webpages containing publications.

### All

HI group visited more number of pages in total and on average

Count

Dwell time


### L: Lists / Search Results

#### LWEB

Count

Dwell Time


Web Search Results

ResN7422811346952663922

#### LPUB

Count

Dwell Time

Publication / Academic search results




### I: Information Objects / Content Pages

#### IWEB - Normal web pages

Count

Dwell Time


#### IPUB - Publications

Count

Dwell Time






## Entropy of Search Tactics

### Transition Entropy of Search Tactics

ResN7289045485214067875 
Both groups had highest Ht during 2C (and also 3 Finance?)

ResP5638176378435107439 - beginning
ResN3435548740759333098 - middle
ResP6845508037293675246 - end

No sig diff for whole task, and at the beginning of task
Sig diff appeared at the middle and increased at the end of task (similar to as observed by Gwizdka).


### Stationary Entropy of Search Tactics

ResP8685879301269737254
LO group had hghest Hs in 2C
No sig diff in overall task

No diff at beginning.
Diff is most prominent at middle (ResN1140102300744288543)
and also somewhat at end (ResN663758335140886910)







R!esN7928923989685423424 is a VERY good result!
Model ID = 10




More plots are in Appendix \@ref(app-plots), such as Figure \@ref(fig:ResP5183239600404564866)













<!-- ================= OLD CONTENTS =============== -->

## COMMENT OUT: Old Content - Remove everything from here


As the sample sizes were often small, and / or did not match the normality assumptions of parametric tests, we have employed the non-parametric Mann-Whitney U test for all the null-hypothesis testing. This allows easy comparison between different categories of results.



What about learning??
What are the measures of learning?

Transition analysis and entropy helps to cover differences in disparate tasks and activities (get text from Qvardford paper)


- Also see Yung Sheng's Dissertation
- think hard about which data component has not been touched / analysed


Hypotheses from @he2016beyond:

The second set (H2) compares two different user groups, experts and novices, using one of the search systems in two different conditions. The H2 hypotheses illustrate how a focus on search tactics provides a different lens to view search logs.

- H2.1: Search experts are likely to be more predictable in their choice of search tactics compared to novices
- H2.2: Search experts have developed a set of search tactics they prefer over others, while novices use search tactics more uniformly.
- H2.3: While working with a search system novices will find a preferred method of transitioning from one search tactic to another. In other words, their search tactics transitions will become more predictable over time.
- H2.4: While working with a search systems novices will find preferred search tactics to use. In other words, their distribution of search tactics will become less uniform over time.


<!-- ## Discussion - Research Questions -->

## RQ1: - how do search behaviours change over time?

- Phase 1
  - Query reform measures
  - search measures
  - learning outcomes
  - SHEG tasks

- Phase 2
- Phase 3

- Indicators predictive of struggling [@hassan2014struggling]:
  - low amount of similarity between consecutive queries
  - more clicks per query
  - differences in the nature of the reformulation patterns: less query term substitution and more addition/removal with exploring

## RQ2: repeated vs new tasks

Phase 1 and Phase 3 only

- search task
- SHEG tasks



## RQ3: mention here


## RQ4: mention here



## Learning Outcomes

### Correlation of SPL with Grades

- self-perceived learning (SPL)
- grades
- paired t-tests for SPL with obtained grade
- CONCL: obtained grades are not good indicators of learning


## Qualitative / Free Text Results

Note-taking strategies

- how do you organize your notes
- how long do you store your notes
- how do you search for a bit of info in the notes

Other surveys




## Quick Check Results (Descriptive)

- which results have the strongest effect sizes and RBC scores?
- which users went beyond page 1 in a SERP?
- possible to track revisits across SES2a - SES2d?
- 




## Descriptive statistics

### profile transitions

- how the following changed over time 
  - motivation, metacognition, self regulation
  - perceived learning, perceived search outcome




<!-- 

Difference between timepoints (across all profiles?)
Difference between profiles (across all times)
Mix (?)

For everything below: change in mean and sig diff over time / task type
    - repeated task
        - A / B
    - non-repeated task
        - A / B
    - longitudinal task
        - A / B

    ✅ Number of terms per query: 
      - query_len_{sum, avg, sd}

    ✅ Number (count) of queries per search task: 
      - query_n_{sum, avg, sd}

    ✅ Number (count) of reformulated query types:
      - query_reform_gen_{sum, avg, sd}
      - query_reform_spec_{sum, avg, sd}
      - query_reform_wsub_{sum, avg, sd}
      - query_reform_rpt_{sum, avg, sd}
      - query_reform_new_{sum, avg, sd}

    ✅ Entropy of query reformulation types
      - {overall, beginning, middle, end} of tasks
      - for different task types
      - query_reform_{Hs, Ht}
      - 🧠 always use closed state space for better entropy

    📌 characterizing words in query according to difficulty
      - freq in vocabulary / specialized terms
        - whether this changes over time

    ✅ No. of clicks per query
    - query_click_n_{sum, avg, sd}
  
    📌 Abandoned queries (Percentage of queries with no clicks) ==> per task
    - query_abandoned_pct_{sum, avg, sd}
    - only 56 queries were without clicks
    
    ✅ generate report containing each variable (cols?), and each task name (rows)
    like CHI SRC paper

    📃 create graphs

    Query Location: where did participants query?
    - Google, Library, etc

    

  TODO: across time, somehow
-->





## L - source selection / Item Selection

- dwell times
- "Item" selection as in IWSS

## I - interacting with sources

- dwell times

> 
> *If the dwell time is long, i.e. ≥ 5 seconds, it is more likely that a user is reading the search results summary (ER) rather than only skimming it (EI). The 5 second threshold was determined based on reading research using eye tracking (Rayner, 1998) and the size of the summaries in Querium. A time span of less than 5 seconds is a too short period for being able to read a summary and extract information.*
>
> `r nbQuoteAuthor('--- @he2016beyond')`
>


## Overall search behaviour

- sig diffs at (beginning, middle and end of tasks) x (LPA profiles)
- 

<!--

Sequences

✅ Transition and Stationary Entropies
- Change in entropy with time (Qvardfort paper)
- Transition matrix density (ET bibile Sec 10.7.2)

- Patterns from Russell Paper
  - Short Navigation: [BREAK]* -> S -> X -> BREAK
  - Topic Exploration: S –> X –> X –> X –> X –> …
  - Methodical Results Exploration: S –> X –> S –> X –> S …
  - Query Refinement: S –> S –> S –> S …

- query without clicks
  
- [Overall / Beginning / Middle / End of task] x [LPA Profiles]
  ✅ Entropies
  - Patterns
  - Sequence Lengths ==> no. of actions performed.


✅ Number (Count) of unique URLs (webpages) visited [visit_n_{sum,avg,sd}]
- url_n
    _{lweb, lpub, lother, iweb, ipub, iother, misc, all}
    _{sum, avg, sd}
    _{whole, beg, mid, end}

⏳ Dwell Times on webpages [dwell_{category}_{sum,avg,sd}] - seconds
  - url_dur
    _{lweb, lpub, lother, iweb, ipub, iother, misc, all}
    _{sum, avg, sd}
    _{whole, beg, mid, end}

^^ Already Too much

📃 Viz: LPA Profile Mean + Transition parallel categories (like CHI paper)
- generate for each profile (60)
- extract HTML for each plot, and put all of them together in one HTML file

🚨🚨 LEARNING OUTCOME!!! 🚨🚨 
- grades

Task Time
  - task_dur_{sum, avg, sd}
  - maybe too much, because need to exclude IDLE time from task
  - or, aka, `url_dur_all_sum` is an indicator of `task_dur`?


Other Time
  - time between queries [later]



Unique domains
  - Percentage and number of unique  clicked domains



- 

📃 Viz: Distributions of all tactics - tables / bars / etc.

-->

### search tactics



## SHEG tasks - information evaluation capabilities

> *We've confused young people’s ability to operate digital devices with the sophistication they need to discern whether the information those devices yield is something that can be relied upon*
>
> https://twitter.com/suzettelohmeyer/status/1617909351766757376
> https://www.grid.news/story/misinformation/2023/01/23/will-information-literacy-in-schools-fix-our-misinformation-problem/


